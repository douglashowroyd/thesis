\chapter{Parallel method for generating pairs}
\label{chap:pairs}

A congruence is a binary relation, and therefore is formally described as a set
of pairs.  In a computational setting, it is rarely practical to keep track of
every pair in a congruence; a congruence on a semigroup of size $n$ contains
$n^2$ pairs in the worst case, and on an infinite semigroup contains an infinite
number of pairs.  A congruence can be described in more concise ways:
for example, taking advantage of it being an equivalence relation
and recording only its equivalence classes; or in the case of a Rees congruence,
storing a generating set for the ideal which defines it.  A variety of different
ways to describe a congruence are explained in Chapter \ref{chap:converting},
along with ways to convert from one to another.  However, a congruence is still just
a set of pairs, and by reducing the number of pairs we store, we can often describe a
congruence very concisely using them.

\begin{definition}
  Let $S$ be a semigroup and let $R$ be a subset of $S \times S$.
  \begin{itemize}
  \item 
    The \textbf{congruence generated by} $R$ is the least congruence
    (with respect to containment) that contains $R$ as a subset.
  \item
    The \textbf{left congruence generated by} $R$ is the least left congruence
    (with respect to containment) that contains $R$ as a subset.
  \item
    The \textbf{right congruence generated by} $R$ is the least right congruence
    (with respect to containment) that contains $R$ as a subset.
  \end{itemize}
\end{definition}

This definition is based on a simple intuitive idea:
a congruence $\rho$ is generated by a set of pairs $R$ if it consists
of only the pairs in $R$ along with the pairs required by the axioms of a
congruence (reflexivity, symmetry, transitivity and compatibility).  Thus a
congruence can be described completely by storing only a few pairs.
Indeed, experiments on a range of interesting semigroups in Chapter
\ref{chap:lattice} show that many congruences can be generated by an extremely
small number of generating pairs, and indeed that most of the congruences
studied are principal (generated by a single pair). %TODO: actually do this

Another justification for the use of generating pairs is that it is a completely
generic representation.  Some special types of semigroup have their own abstract
representations of congruences---for inverse semigroups, one can study
kernel-trace pairs \cite[\S 5.3]{howie}; for groups, normal subgroups
\cite[Theorem 11.5]{warner}; for completely simple or completely 0-simple
semigroups, linked triples \cite[\S 3.5]{howie}---but generating pairs can
represent a congruence on any semigroup whatsoever.  Furthermore, one
might be interested in what pairs are implied by a given pair or set of pairs in
a congruence, and this representation can answer such questions.

Left congruences and right congruences can also be described using generating
pairs, and some algorithms designed for two-sided congruences can be used with
minor modifications to compute information about left and right congruences.

This chapter describes a parallelised approach for computing a congruence from a
set of generating pairs, as implemented in \texttt{libsemigroups}
\cite{libsemigroups}.  First we will give a general outline of the system
and what questions it hopes to answer, then we will describe in detail each
algorithm used, its advantages and disadvantages, and when it can be applied.
Finally we will explain how the different algorithms are executed together, and
consider their implementation in \cite{libsemigroups}.

\section{Reasons for parallelisation}

Parallel processing has seen major advances in the last ten years, with
multi-core processors becoming the norm in many types of computers, and
processors with 4, 8, or even 16 cores becoming common on a desktop PC.  This
being the case, it is desirable to parallelise mathematical algorithms wherever
possible, and take advantage of the ability to execute multiple
threads of instructions concurrently.  Some algorithms
are ``embarrassingly parallel''---that is, they can be split into
independent threads which require almost no communication with each other.
Examples of these algorithms would be brute force searches, or rendering of
computer graphics.
These are suited so well to parallelisation that splitting the operation into
$n$ parallel threads reduces the expected run-time to barely more than
$\frac{1}{n}$ what it would be when run in a single thread.  Other
% TODO: don't say "1/n what it would be"
algorithms do not parallelise so well: sometimes threads have to communicate, or
use shared resources, causing significant slowdown and severely limiting the
improvements that can be made by parallelising.

When it comes to computing information about a congruence from generating pairs,
there are various different approaches which can be taken: in Sections
\ref{sec:p}, \ref{sec:tc} and \ref{sec:kb}, we describe three possible
algorithms: pair orbit enumeration, Todd-Coxeter, and Knuth-Bendix/Froidure-Pin.
Depending on what sort of semigroup is given as an input (see Section
\ref{sec:program-outputs}), several
or all of these might be appropriate.  However, depending on certain properties
of the congruence, one might perform far better than another.  For example, the
pair orbit algorithm works well on congruences that contain few non-reflexive
pairs, while the Todd-Coxeter algorithm tends to work well on congruences with
few classes (i.e.~very many pairs).  For a detailed analysis of which algorithms
perform well on which inputs, see Section \ref{sec:benchmarking}.  Given
only a set of generating pairs, these properties are likely to be unknown in
advance, which makes it difficult to choose a good algorithm.

The natural answer to this problem is the core concept of this chapter: a
parallel approach which does not attempt to parallelise individual algorithms,
but which runs several known algorithms at the same time, each in a different
thread, and simply halts all threads as soon as any one completes.  Since these
algorithms do not interact with each other in any way, the total run-time will
be close to the minimum run-time of all the different algorithms.  This is
particularly important, since for certain semigroups and congruences some
algorithms will never terminate, while others may terminate in a very short
time.

\section{Applicable types of semigroup}
\label{sec:applicable-types-of-semigroup}

The type of element in a semigroup affects which methods will be most effective,
or even which methods will be applicable.  For this purpose, we divide
semigroups into two categories: finitely presented semigroups, and
\textit{concrete} semigroups.  By \textit{concrete}, we mean a finite semigroup
whose elements can be multiplied and compared quickly, without reference to the
semigroup as a whole; these could be semigroups of transformations, partial
permutations, bipartitions, matrices, or other finite objects.  Their elements
will be known in advance.  Finitely presented semigroups are a different case,
in that their elements are not known in advance, and indeed it may not be known
whether or not a given semigroup is finite.  Finitely presented monoids are treated as
equivalent to finitely presented semigroups, since a semigroup presentation can
be attained by simply adding a generator for the identity and relations to make
it multiplicatively neutral.

\section{Program inputs}

Our algorithm determines the properties of a single left, right, or two-sided
congruence defined by generating pairs, over a semigroup $S$.
For the remainder of this chapter, the word ``congruence'' will be used to refer
to left, right, and two-sided congruences equally, without having the default
meaning of ``two-sided congruence''. % TODO: move this warning?

If $S$ is a concrete semigroup (as described in Section
\ref{sec:applicable-types-of-semigroup}) then it will almost certainly be
specified by a generating set; in this case, it is quick to use these generators
to calculate a list of elements in $S$, along with left and right Cayley graphs
for $S$, using the Froidure-Pin algorithm (see Section \ref{sec:fp}).  If on the
other hand $S$ is a finitely presented semigroup, then the elements will not be
known in advance.  In either case, a finite presentation $\pres X R$ can be
given---a technique for efficiently finding a presentation for a concrete
semigroup is given in Section \ref{sec:find-pres}.
The exact parameters supplied to the algorithm are therefore as follows:
\begin{itemize}
\item A set of generators $X$;
\item A finite set of relations $R \subseteq X^* \times X^*$;
\item A finite set of generating pairs $W \subseteq X^* \times X^*$;
\item A record of whether we are computing a left, right, or two-sided
  congruence.
\end{itemize}
The following are also supplied if and only if $S$ is concrete:
\begin{itemize}
\item A list of elements of $S$;
\item Left and right Cayley graphs for $S$;
\item A factorisation function $f : S \to X^*$.
\end{itemize}

We shall now make clear the meanings of these different parameters, by giving a
complete description of the system, starting with the commutative diagram in
Figure \ref{fig:pairs-cd-1}.

Here $X$ is our alphabet, a subset of the free semigroup $X^+$.  We have a set
of relations $R \subseteq X^+ \times X^+$, which generates the congruence
$R^\sharp$ on $X^+$.  This congruence gives a quotient semigroup
$X^+ / R^\sharp$; this is isomorphic to the semigroup $S$,
which is described by the presentation $\pres X R$.  The
congruence also gives us its natural homomorphism $\pi: X^+ \to S$ (see
Definition \ref{def:natural-homomorphism}).  We also have a set of generating pairs
$\mathbf{P} \subseteq S \times S$, which defines a congruence
$\mathbf{P}^\sharp$.  The aim of the algorithm described in this chapter is to
obtain a data structure for $\mathbf{P}^\sharp$, where the precise meaning of
``data structure'' is defined in Section \ref{sec:program-outputs}.  If we are
calculating a two-sided congruence, then it gives rise to the
quotient semigroup $S / \mathbf{P}^\sharp$.

\begin{figure}[h]
  \centering
  $\begin{tikzcd}
    X \arrow[dr] \arrow[d, hook] & \\
    X^+ \arrow[r, two heads, "\pi"] & \frac{X^+}{R^\sharp} \cong S \arrow[r, two heads] & \frac{S}{\mathbf{P}^\sharp}
  \end{tikzcd}$
  \caption{The relationships between different input objects}
  \label{fig:pairs-cd-1}
\end{figure}

The generating pairs $\mathbf{P}$ are not given by the user.  Since the elements
of $S$ might be unknown (for example if $S$ was specified by a finite
presentation), it would be impractical for the user to specify them precisely.
Instead, the user specifies a set $W$ consisting of pairs of words from
$X^+ \times X^+$, which can be evaluated to pairs of elements in $S \times S$,
giving the set of generating pairs $\mathbf{P}$.
More formally, let
$\Pi: X^+ \times X^+ \to S \times S$ be defined by
$\Pi: (w_1, w_2) \mapsto (w_1\pi, w_2\pi)$, where $\pi$ is the natural
homomorphism from $X^+$ to $S$ mentioned above.  The generating pairs
$\mathbf{P}$ of the congruence are given by $\mathbf{P} = W \Pi$.
This relationship is summarised in Figure \ref{fig:pairs-cd-2}.

\begin{figure}[h]
  \centering
  $\begin{tikzcd}
    W \arrow[r, hook, two heads, "\Pi|_W"] \arrow[d, hook] & \mathbf{P} \arrow[d, hook] \\
    X^+ \times X^+ \arrow[r, two heads, "\Pi"] & S \times S
  \end{tikzcd}$
  \caption{The generating pairs of the congruence}
  \label{fig:pairs-cd-2}
\end{figure}

\section{Program outputs}
\label{sec:program-outputs}

Each method we are about to explain can provide a variety of different pieces of
information, but it is important to consider which questions we aim to answer.
Our system should be able to return the following information about a given
congruence when requested:

\begin{itemize}
\item Presence of a given pair $(x,y)$ in the congruence
\item Number of congruence classes
\item The elements in each non-trivial congruence class
\item Class number of a given element
\end{itemize}

Note that for finitely presented semigroups, these questions may be
undecidable.  In the case that a question is decidable, our approach should
return an answer in finite time.

\section{Finding a presentation}
\label{sec:find-pres}

A concrete semigroup $S$ will not be specified by a presentation, but a finite
presentation is still required for the Todd-Coxeter and
Knuth-Bendix/Froidure-Pin algorithms to be used, and so a presentation
$\pres X R$ must be calculated.  For the purposes of the algorithm described in
this chapter, it is not important how this presentation is obtained, only that
it is correct and that elements from $S$ can be factorised with respect to $X$.
However, in order to implement the ideas of this chapter, it may be useful to
see a description of the method used in \texttt{libsemigroups}
\cite{libsemigroups}.

Description goes here. % TODO

\section{The methods}

\subsection{Pair orbit enumeration}
\label{sec:p}

Background

The P algorithm

Using Knuth-Bendix: KBP

Semigroups/congs it works/works best on

Complexity

\subsection{Todd-Coxeter}
\label{sec:tc}

The Todd-Coxeter algorithm was originally described in 1936 in
\cite{todd_coxeter_1936}.  It was an algorithm to enumerate the cosets of a
finitely generated subgroup of a finitely presented group.  Arriving before the
advent of electronic computers, the algorithm was originally intended to be
carried out by hand.  Perhaps the earliest automatic implementation was on the
EDSAC II computer in Cambridge \cite{leech_1963}.  Since then, a wide variety of
efficient, optimised versions have been implemented, for example \cite{ace}.

A variation of Todd-Coxeter for semigroups was described in 1967
\cite{neumann_1967}.  The algorithm takes a presentation $\pres X R$
for a semigroup $S$ and computes the right regular representation of $S^1$ with
respect to the generators $X$---that is, it computes all the elements of $S$ and
the result of right-multiplying each element by each generator.  Since the
original algorithm makes very little use of those properties unique to groups,
the method applied to semigroups is essentially the same.  Other descriptions of
Todd-Coxeter for semigroups can be found in \cite[Chapter 12]{ruskuc_thesis} and
\cite[Chapter 1.2]{walker_thesis}, and a variation specific to inverse
semigroups can be found in \cite{cutting_thesis}.  Our version of the algorithm
is based closely on an implementation by G\"otz Pfeiffer, found in
\cite[\texttt{lib/tcsemi.gi}]{gap}, and based on \cite{walker_thesis}.

We will now describe the Todd-Coxeter method as used in the context of this
chapter.  Though the method itself does not represent new work, it is an
important part of the overall parallel approach, and in order to understand its
uses and limitations, it is described here in full.

\subsubsection{Setup}

The Todd-Coxeter algorithm is based on a table, where each row corresponds to a
single congruence class (or equivalently, a single element of the quotient
semigroup).  The columns of the table correspond to the generators of the
semigroup, and the entry in row $i$, column $j$ represents the element found by
taking element $i$ and right-multiplying it by generator $j$.  These entries may
be blank, and two different rows may be found to describe the same element.
Mathematically, we can view this table as a triple $(n, \mathbf{N}, \tau)$
consisting of:
\begin{itemize}
\item an integer $n \in \mathbb{N}$ representing the number of rows in the table;
\item a set $\mathbf{N} \subseteq \{1, \ldots, n\}$ containing the indices of the
  \textit{undeleted} rows; and
\item a function $\tau: \mathbf{N} \times X \to \mathbf{N} \cup \{0\}$, where
  $(i, x)\tau$ is equal to the entry in row $i$ and the column corresponding to
  generator $x$---with $0$ representing a blank entry.
\end{itemize}

Suppose that we have a semigroup presentation $\pres X R$ for a semigroup
$S$.  The table is initialised with a single row, numbered $1$.  This row
corresponds to the empty word $\varepsilon$, or the adjoined identity of the
monoid $S^1$.  The row is empty, containing a blank entry in all $|X|$ columns.
In our mathematical notation, we define $n=1$ and
$(1,x)\tau = 0$ for all $x \in X$.

We can naturally extend the function
$\tau: \mathbf{N} \times X \to \mathbf{N} \cup \{0\}$
to a function 
$\bar{\tau}: \mathbf{N} \times X^* \to \mathbf{N} \cup \{0\}$
which is described as follows.
If $w \in X^*$ and $w=w_1 \ldots w_n$, where $w_1, \ldots, w_n \in X$,
then we can define $\bar\tau$ recursively by
$$
(i, w)\bar\tau = \left\{
\begin{matrix*}[l]
  i & \textnormal{if~} w=\varepsilon,\\
  0 & \textnormal{if~} (i, w_1)\tau=0,\\
  ((i, w_1)\tau, w_2 \ldots w_n)\bar\tau & \textnormal{otherwise}.
\end{matrix*} \right.
$$

The effect of $\bar\tau$ is to trace an entire word through the table, starting
at a given row.

\subsubsection{Elementary operations}

We now describe 3 operations which may be applied to the table.  These
operations will be described loosely to give an intuition behind what they are
designed to do, and then each one will be described formally in pseudo-code
(Algorithms \ref{alg:add}, \ref{alg:trace} and \ref{alg:coinc}).  We will then
describe the overall Todd-Coxeter procedure which uses these operations
(Algorithm \ref{alg:tc}).

\begin{itemize}
\item \textsc{Add}: Fill in a blank entry and add a row to the table;
\item \textsc{Trace}: Trace a relation from a row;
\item \textsc{Coinc}: Process a coincidence.
\end{itemize}

The first operation, \textsc{Add}, is simple.  A new row is added at the bottom
of the table, and its number is written into the blank cell in the table
specified by the arguments given to \textsc{Add}.  Pseudo-code for this
operation is given in Algorithm \ref{alg:add}.

\begin{algorithm}
\caption{The \textsc{Add} algorithm}
\label{alg:add}
\begin{algorithmic}[1]
\Procedure{Add}{$i, x$}
\State $n \gets n + 1$
\State $\mathbf{N} \gets \mathbf{N} \cup \{n\}$
\For{$x \in X$}
  \State $(n, x)\tau := 0$
\EndFor
\State $(i, x)\tau \gets n$  
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textsc{Trace} takes two arguments: a relation $v=w$ from $R$, and a row $e$
in the table.  We now give an informal description of this procedure---see
Algorithm \ref{alg:trace} for pseudo-code.

We start from row $e$, and find $(e, v)\bar\tau$ by processing $v$ one letter at a time: we
find the column corresponding to the letter, and look at that position in the
table; this gives us the number of the new row which we move to.  If we
encounter a blank entry before the final letter, we apply \textsc{Add} to that
cell, and follow the new entry.  At the end of the process we have an entry in
the table, blank or filled, called $(e, v)\bar\tau$.  We repeat the
process with the other word to find $(e, w)\bar\tau$.

To satisfy $v=w$, we need to set these two entries such that
$(e, v)\bar\tau = (e, w)\bar\tau$.
\begin{itemize}
\item If $(e, v)\bar\tau$ and $(e, w)\bar\tau$ are both blank, then we apply
  \textsc{Add} to $(e, v)\bar\tau$ and copy the entry into $(e, w)\bar\tau$.
\item If just one of the entries is filled, then the filled entry is copied into
  the blank one.
\item If both entries are filled and equal, we need do nothing.
\item If both entries are filled and are distinct, we apply \textsc{Coinc} to
  the two entries.
\end{itemize}

\begin{algorithm}
\caption{The \textsc{Trace} algorithm}
\label{alg:trace}
\begin{algorithmic}[1]
\Procedure{Trace}{$e, v = w$}
\State Write $v = v_1 \ldots v_m$ \Comment $(v_i \in X \text{~for~} 1 \leq i \leq m)$
\State Write $w = w_1 \ldots w_n$ \Comment $(w_i \in X \text{~for~} 1 \leq i \leq n)$
\State $s \gets e$
\For{$i \in \{1, \ldots, m-1\}$}
  \If{$(s, v_i)\tau = \varepsilon$}
    \State \Call{Add}{$s, v_i$}
  \EndIf
  \State $s \gets (s, v_i)\tau$
\EndFor
\State $t \gets e$
\For{$i \in \{1, \ldots, n-1\}$}
  \If{$(t, w_i)\tau = \varepsilon$}
    \State \Call{Add}{$t, w_i$}
  \EndIf
  \State $t \gets (t, w_i)\tau$
\EndFor

\If{$(s, v_m)\tau = (t, w_n)\tau = \varepsilon$}
  \State \Call{Add}{$s, v_m$}
  \State $(t, w_n)\tau \gets (s, v_m)\tau$
\ElsIf{$(s, v_m)\tau = \varepsilon$}
  \State $(s, v_m)\tau \gets (t, w_n)\tau$
\ElsIf{$(t, w_n)\tau = \varepsilon$}
  \State $(t, w_n)\tau \gets (s, v_m)\tau$
\ElsIf{$(s, v_m)\tau \neq (t, w_n)\tau$}
  \State \Call{Coinc}{$(s, v_m)\tau, (t, w_n)\tau$}
\EndIf

\EndProcedure
\end{algorithmic}
\end{algorithm}

\textsc{Coinc} is used when two rows in the table are found to refer to the same
element of $S$.  The higher-numbered row is deleted, and all occurrences of the
higher number in the table are replaced by the lower number.  The two rows are
combined into one, with all known information being preserved---this may imply
that another pair of rows are equal, another coincidence that should be
processed by a recursive call to \textsc{Coinc} (or should be stored in a list
for processing after this call is finished).

\begin{algorithm}
\caption{The \textsc{Coinc} algorithm}
\label{alg:coinc}
\begin{algorithmic}[1]
\Require $r < s$
\Procedure{Coinc}{$r, s$}
\State $\mathbf{N} \gets \mathbf{N} \setminus \{s\}$
\For{$e \in \mathbf{N}$}
  \For{$x \in X$}
    \If{$(e, x)\tau = s$}
      \State $(e, x)\tau \gets r$
    \EndIf
  \EndFor
\EndFor
\For{$x \in X$}
  \If{$(r, x)\tau = 0$}
    \State $(r, x)\tau \gets (s, x)\tau$
  \ElsIf{$(r, x)\tau \neq (s, x)\tau \textbf{~and~} (s, x)\tau \neq 0$}
    \State \Call{Coinc}{$(r, x)\tau, (s, x)\tau$}
  \EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Now that we have these three operations, it is simple to describe the overall
procedure, as shown in Algorithm \ref{alg:tc}.  We go through the list of used
rows $\mathbf{N}$, starting with row $1$.  To each
of these rows we apply each relation from $R$, using \textsc{Trace}.  Each call
to \textsc{Trace} may, of course, invoke calls to \textsc{Add} and
\textsc{Coinc}, so rows will be appended to the list as the algorithm
progresses.  When the end of $\mathbf{N}$ is reached, the table should
completely describe the multiplication for the finitely presented semigroup:
each row in $\mathbf{N} \setminus \{1\}$ represents one element of $S$, and
$(i, x)\tau$ represents the element denoted by $i$ right-multiplied by the
generator $x$.

\begin{algorithm}
\caption{The \textsc{Todd-Coxeter} algorithm for semigroups}
\label{alg:tc}
\begin{algorithmic}[1]
\Procedure{Todd-Coxeter}{$\pres X R$}
\State $n := 1$
\State $\mathbf{N} := \{1\}$
\For{$x \in X$}
  \State $(1, x)\tau := 0$
\EndFor
\For{$e \in \mathbf{N}$}
  \For{$(u=v) \in R$}
    \State \Call{Trace}{$e, u=v$}
  \EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Note that there is no guarantee that the end of $\mathbf{N}$ will ever be
reached: if the given presentation defines an infinite semigroup, the table will
grow forever and the procedure will never terminate.  On the other hand, if the
presentation defines a finite semigroup, the procedure is guaranteed to
terminate in a finite number of steps (see
\cite[Theorem 5.5]{cgt} and \cite[Theorem 3]{beetham_campbell_1976}).
This number of steps is, however, unbounded; and since a user may
not know in advance whether a presentation defines a finite or infinite
semigroup, it is impossible to
know, while the procedure is running, whether it will end.

\subsubsection{An example}
We now give an example of the Todd-Coxeter algorithm running on the semigroup
presentation
$$\pres{a, b}{ba=ab,\ b^2=b,\ a^3=ab,\ a^2b=a^2}.$$
We initialise the table to look like Table \ref{tab:tc1}.
\tctableAB{tab:tc1}
{Initial position}
{ 1 & & \\ }

The list $\mathbf{N}$ of undeleted rows contains only a single entry, $1$.  We
begin by tracing each relation on the row $1$, starting with $ba=ab$.
The left-hand side of this
relation makes us call \textsc{Add} on the cell $(1, b)$, creating a new row,
$2$, which is added to $\mathbf{N}$.
For the right-hand side, we must call \textsc{Add} on
the cell $(1, a)$, creating a row $3$.  At the end of the \textsc{Trace}, we
must set $(1, ba)\bar\tau$ equal to $(1, ab)\bar\tau$, so we set both
$(2, a)\tau$ and $(3, b)\tau$ to $4$ (as in Table \ref{tab:tc2}).
\tctableAB{tab:tc2}
{Position after \textsc{Trace}($1, ba=ab$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & \\
  \cline{2-3}
  3 & & 4 \\
  \cline{2-3}
  4 & & \\
}
Next, we apply \textsc{Trace}($1, b^2=b$).  Since $(1, b)\bar\tau$ is already
set, we just set $(1, b^2)\bar\tau$ equal to it: $(2, b)\tau \gets 2$.  See
Table \ref{tab:tc3}.
\tctableAB{tab:tc3}
{Position after \textsc{Trace}($1, b^2=b$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & & 4 \\
  \cline{2-3}
  4 & & \\
}
Still on row $1$, we apply \textsc{Trace} to the third relation, $a^3=ab$.  This
creates a new row for $(1, a^2)\bar\tau = (3, a)\tau = 5$.  The new row's $a$
entry is set to be the same as $(1, ab)\bar\tau$, which is $4$
(see Table \ref{tab:tc4}).
\tctableAB{tab:tc4}
{Position after \textsc{Trace}($1, a^3=ab$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & & \\
  \cline{2-3}
  5 & 4 & \\
}

The final relation for row $1$ is $a^2b=a^2$.
$(1, a^2b)\bar\tau$ is currently blank, and is set to the current value of
$(1, a^2)\bar\tau$, which is $5$.  Hence $(5, b)\tau \gets 5$
(as in Table \ref{tab:tc5}).
\tctableAB{tab:tc5}
{Position after \textsc{Trace}($1, a^2b=a^2$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & & \\
  \cline{2-3}
  5 & 4 & 5 \\
}
We have now finished with row $1$, and we proceed to the next row in
$\mathbf{N}$, which is $2$.  Accordingly, we apply the first relation,
\textsc{Trace}($2, ba=ab$).  The value of $(2, ba)\bar\tau$ is $4$, whereas the
value of $(2, ab)\bar\tau$ has not yet been set.
We set it by applying $(4, b)\tau \gets 4$.
See Table \ref{tab:tc6}.
\tctableAB{tab:tc6}
{Position after \textsc{Trace}($2, ba=ab$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & & 4 \\
  \cline{2-3}
  5 & 4 & 5 \\
}
Proceeding with \textsc{Trace}($2, b^2=b$), we find that
$(2, b^2)\bar\tau = (2, b)\bar\tau$ already, so we make no modifications to the
table.  Next, \textsc{Trace}($2, a^3=ab$) discovers that $(2, a^2)\bar\tau$ is
not set, and so we call \textsc{Add}($4, a$), creating a new row $6$.
Now $(6, a)\tau$ is set to $(2, ab)\bar\tau$ which is equal to $4$.
See Table \ref{tab:tc7}.
\tctableAB{tab:tc7}
{Position after \textsc{Trace}($2, a^3=ab$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & 6 & 4 \\
  \cline{2-3}
  5 & 4 & 5 \\
  \cline{2-3}
  6 & 4 & \\
}
The final relation for row $2$ is $a^2b=a^2$, setting $(6, b)\tau \gets 6$ (see
Table \ref{tab:tc8}).
\tctableAB{tab:tc8}
{Position after all relations on row $2$}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & 6 & 4 \\
  \cline{2-3}
  5 & 4 & 5 \\
  \cline{2-3}
  6 & 4 & 6 \\
}
Next we move onto row $3$, and we apply \textsc{Trace}($3, ba=ab$).  Inspecting
the table shows $(3, ba)\bar\tau = 6$ but $(3, ab)\bar\tau = 5$, giving rise to
a coincidence.  We apply \textsc{Coinc}($5, 6$), which deletes row $6$, rewrites
any occurrences of $6$ in the table to $5$, and copies row $6$ into row $5$
(yielding no new information).  The result is shown in Table \ref{tab:tc9}.  The
rest of the relations are applied to row $3$, and to the remaining rows in the
table, but no changes are made to the table, so Table \ref{tab:tc9} is the final
state.
\tctableAB{tab:tc9}
{Final position}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & \cancel{\textcolor{gray}{6}}5\!\!\! & 4 \\
  \cline{2-3}
  5 & 4 & 5 \\
  \cline{2-3}
  \textcolor{gray}{6} & \textcolor{gray}{4} & \textcolor{gray}{6} \\[-1.6ex]
  \hline\noalign{\vspace{\dimexpr 1.4ex}} \cline{2-3}
}

We can now delete row $1$, which acts as an appended identity, and we find a
description of the semigroup's multiplication, with relation to its generators.
This description can be represented as a Cayley graph,
as shown in Figure \ref{fig:tc-cayley-graph}.
\begin{figure}[H]
  \centering
  \begin{dot2tex}
    //dot
    digraph {
      rankdir=LR
      node [shape=circle]
      2
      3
      4
      5
      2 -> 4 [label=a]
      2 -> 2 [label=b]
      3 -> 5 [label=a]
      3 -> 4 [label=b, dir=both]
      4 -> 5 [label=a, dir=both]
      4 -> 4 [label=b]
      5 -> 5 [label=b]
    }
  \end{dot2tex}
  \caption{Right Cayley graph of $\pres{a, b}{ba=ab,\ b^2=b,\ a^3=ab,\ a^2b=a^2}$}
  \label{fig:tc-cayley-graph}
\end{figure}
It is worth noting that the columns of the table now give a right representation
of $S$.  That is, $S$ is isomorphic to the semigroup generated by the
transformations $\transV 34554$ and $\transV 22445$.

\subsubsection{Improvements}
Left/right congruences

Pre-filling the table

Semigroups/congs it works/works best on

Complexity

\subsubsection{Implementation}

Rows will be added to the table, and deleted from it.  A list must be kept of
rows which are in use; when a row is added, its position in the table should be
appended to this list at the end, and when a row is deleted it should be removed
from its position in the list and added to a list of ``free rows'' which can be
reused later.  The ``rows in use'' list is best implemented as a doubly-linked
list, so that single entries can be added and removed with as little processor
work as possible.

\clearpage

\subsection{Rewriting systems}
\label{sec:kb}

Another approach for solving the word problem in a finitely presented semigroup
is using rewriting systems.  Hence, given a semigroup $S$ with presentation
$\pres X R$ and a congruence $\rho$ over $S$ with generating pairs $\mathbf{G}$,
% TODO: use $W$ instead, and make this bit clearer
we may be able to find a rewriting system which converts any word $w \in X^+$ to
a canonical word representing the same element of $\pres X {R, \mathbf{G}}$;
that is, a word representing a semigroup element in the same $\rho$-class as
the semigroup element of $S$ represented by $w$.

For ease of notation and understanding, this section will describe an algorithm
for the word problem on a finitely presented semigroup.  We understand that this
is the same as the problem of whether a given pair of words represent semigroup
elements related to each other by a two-sided congruence $\rho$.  The ideas do
not extend easily to left and right congruences, which will therefore not have
this method as an option.

In order
to describe the process, we must first explain some background theory.  A full
description of these ideas can be found in \cite[Section 12.2]{cgt}.  Note that
we shall again consider monoid presentations instead of semigroup presentations,
since it is easy to change between the two by appending or removing an identity
(the empty string $\varepsilon$).

Let $X$ be an alphabet.  A \textit{rewriting system} $\rws$ on $X^*$ is a
set of ordered pairs $(u,v)$ where $u, v \in X^*$.
A pair $(u,v) \in \rws$ is called a \textit{rule}, and can be viewed as
an operation which transforms an occurrence of $u$ in a word into an occurrence
of $v$.
For this section, we will assume that $\rws$ is finite.
A rewriting system $\rws$ extends to relations
$\to_\rws$, $\tostar_\rws$, and $\lrstar_\rws$
which describe how words are rewritten, and which are defined as follows.

Let $u, v \in X^*$ and let $\rws$ be a rewriting system.
We write $u \to_\rws v$ if there exist $(w_1, w_2) \in \rws$ and
$s, t \in X^*$ such that $u=sw_1t$ and $v=sw_2t$.
That is, $u \to_\rws v$ if a rule rewrites a contiguous subword of $u$ to turn
$u$ into $v$.  The relation $\tostar_\rws$ is simply the reflexive transitive
closure of $\to_\rws$; that is, $u \tostar_\rws v$ if and only if $u = v$ or
$$u = u_0 \to_\rws u_1 \to_\rws \ldots \to_\rws u_n = v,$$
for some $u_0, \ldots, u_n \in X^*$.
Finally, $\lrstar_\rws$ is the symmetric closure of
$\tostar_\rws$.  It is easy to see that $\lrstar_\rws$ is an equivalence
relation whose classes we may write as $[w]_\rws$.
Where there is no chance of ambiguity, we omit the subscript in these
operations, just writing $\to$, $\tostar$ and $\lrstar$.

This definition of a rewriting system does not guarantee that a word can be
rewritten in a useful way.  A rewriting system could allow an endless loop of
rewriting; for example, a system over the alphabet $\{a,b\}$ could contain rules
$(aa,b)$ and $(b,aa)$ which would allow the rewrite sequence
$$aa \to b \to aa \to b \to aa \to b \to aa \to \ldots$$
to go on forever.  Alternatively, a it could be possible to rewrite one word in
two different ways; for example, a system over the alphabet $\{a,b,c\}$ could
contain rules $(aa,b)$ and $(aa,c)$.
In order to solve the word problem for a semigroup, we require a
rewriting system with certain properties.  We will describe these properties,
and then explain how to produce a rewriting system which satisfies them.

\begin{definition}
  A string $u \in X^*$ is $\rws$-\textbf{irreducible} if there is no
  string $v \in X^*$ such that $u \to v$; that is, $u$ cannot be rewritten by
  any rule in $\rws$.  \cite[Def~12.13]{cgt}
\end{definition}

\begin{definition}
  A rewriting system is \textbf{terminating} if there is no infinite chain of
  words $u_1, u_2, \ldots \in X^*$ such that $u_i \to u_{i+1}$ for all $i > 0$.
\end{definition}

If a rewriting system is terminating, this is good news computationally.  It
means that any word can be transformed by rules only a finite number of times
before it reaches an irreducible state, so the task of finding an irreducible
form of a word is guaranteed to be achievable in finite time.  But note that we
can still only talk about \textit{an} irreducible word, not \textit{the}
irreducible word.  We could still have a word $u \in X^*$ and irreducible words
$v, w \in X^*$ such that $u \tostar v$ and $u \tostar w$ but $v \neq w$.
To avoid this, we must ensure that the system is \textit{confluent}, as follows.

\begin{definition}
  A rewriting system is \textbf{confluent} if, for any words $u,v_1,v_2 \in X^*$
  such that $u \tostar v_1$ and $u \tostar v_2$, there exists a word $w \in X^*$
  such that $v_1 \tostar w$ and $v_2 \tostar w$.
\end{definition}

The intuition behind this definition is that, as the name suggests, different
paths ``flow together''.  The result is that, in a confluent terminating
rewriting system, rules can be applied to a word in any order, and a canonical
irreducible word will be found in a finite number of steps.
% If $\rws$ is
% a confluent terminating rewriting system, then let us write $(w)f_\rws$
% or $(w)f$ for the unique irreducible word which $w$ can be written to.  Let
% $[w]_\rws$ or $[w]$ be the set of all words $u \in X^*$ such that
% $u \tostar (w)f$.

Another definition will help us to determine whether a rewriting system is
confluent: \textit{local confluence}.  This is a weaker condition than
confluence, but the two are strongly linked by Lemma \ref{lem:newman}.


\begin{definition}
  A rewriting system is \textbf{locally confluent} if, for any words
  $u,v_1,v_2 \in X^*$ such that $u \to v_1$ and $u \to v_2$, there exists a word
  $w \in X^*$ such that $v_1 \tostar w$ and $v_2 \tostar w$.
\end{definition}

\begin{lemma}[Newman's diamond lemma] % TODO: reference
  \label{lem:newman}
  A terminating rewriting system is confluent if and only if it is locally
  confluent.
\end{lemma}

Lemma \ref{lem:newman} gives us an idea of how to check computationally whether
a system is confluent: rather than checking every possible transitive rewriting
of a word (its neighbours under $\tostar$), it suffices to check a word's
immediate children (its neighbours under $\to$).  This lemma will help us later,
with Theorem \ref{thm:knuth-bendix} and the Knuth-Bendix procedure.

We can now see an application of rewriting systems to the word problem in a
finitely presented monoid.  Indeed, given an alphabet $X$ and a rewriting system
$\rws$, the quotient monoid $X^* / \lrstar_\rws$ is described by the
monoid presentation $\pres{X}{\rws}$.  Hence, given a monoid $M$ with a
presentation $\pres X R$, if there is a confluent terminating rewriting system
$\rws$ such that the word equality relation $=_M$ is the same as the
relation $\lrstar_\rws$, then the word problem can be solved
simply by rewriting two words using $\rws$ until their irreducible
representatives are found, and then comparing them.  The only difficulty is in
finding a rewriting system which is confluent and terminating---but we can find
one, by starting with the set of relations $R$, and then using the Knuth-Bendix
completion algorithm.

Let $M$ be a monoid with finite presentation $\pres X R$.
We start with no rules in our rewriting system, $\rws = \varnothing$, and
we begin to add relations from $R$.  However, in order to ensure our system is
\textit{terminating}, we must reorder each relation to ensure we do not create
any loops.  For this purpose, we define a total ordering on $X^*$ and reorder
each relation so that a rule $(u, v)$ has the property that $u < v$.  Criteria
for this ordering are discussed in \cite[p.~420]{cgt}, but it will suffice to
use the \textit{shortlex ordering}: $u < v$ if and only if $u$ is shorter than
$v$ or they have equal length and $u$ is less than $v$ lexicographically.  Hence the
first few words over the alphabet $\{a, b\}$ are
$$\varepsilon < a < b < aa < ab < ba < bb < aaa < aab < aba < \ldots$$
Note that this requires a well-understood total order on the alphabet $X$
itself.  This ordering justifies the use of the words ``reducible'' and
``irreducible''---words are always replaced with lesser words.

Once each relation from $R$ is added---possibly reordered---to $\rws$, we
will have a terminating rewriting system such that
$X^* / \lrstar_\rws\ = \pres X R$, as required.  It only remains to add
rules to $\rws$ to make it confluent, without altering the relation
$\lrstar$.  This is where we use the Knuth-Bendix completion process.

First described by Knuth and Bendix in \cite{knuth_bendix}, the completion
process adds rules to $\rws$ based on finding and resolving
\textit{critical pairs}.  Roughly speaking, a critical pair is a pair of rules $(u_1, v_1)$ and
$(u_2, v_2)$ from $\rws$ such that $u_1$ and $u_2$ overlap and therefore a word could be
rewritten as either of two different irreducible words.  The formal definition
is as follows.

\begin{definition}
  \label{def:critical-pair}
  Let $\rws$ be a terminating rewriting system over an alphabet $X^*$.  A
  \textbf{critical pair} is a pair of distinct words
  $(w_1, w_2) \in X^* \times X^*$ such that $w_1$ and $w_2$ can be produced from
  the same word by two different rules.
  Critical pairs arise in two different ways.
  Let $(u_1, v_1)$ and $(u_2, v_2)$ be rules in $\rws$.
  \begin{enumerate}[(i)]
  \item If $u_1 = rs$ and $u_2 = st$ with $r,s,t \in X^*$ and
    $s \neq \varepsilon$, then $rst \to v_1t$ and $rst \to rv_2$, so
    $(v_1t, rv_2)$ is a critical pair.
  \item If $u_1 = ru_2t$ for some $r,t \in X^*$ and $u_2 \neq \varepsilon$, then
    $u_1 \to v_1$ and $u_1 \to rv_2t$, so $(v_1, rv_2t)$ is a critical
    pair.
  \end{enumerate}
\end{definition}

The importance of critical pairs is shown in the following theorem, which is key
to the Knuth-Bendix completion process.

\begin{theorem}[Lemma 12.17 in \cite{cgt}]
  \label{thm:knuth-bendix}
  A terminating rewriting system $\rws$ over $X$ is confluent if and only if,
  for each critical pair $(u, v)$, there exists some $w \in X^*$ such that
  $u \tostar w$ and $v \tostar w$.
\end{theorem}

Now we have all the concepts required to describe the Knuth-Bendix completion
process.  The process searches through rules in $\rws$ looking for critical
pairs; when a critical pair $(u, v)$ is found which does not satisfy the
condition stated in Theorem \ref{thm:knuth-bendix}, $(u, v)$ or $(v, u)$ is
added to $\rws$ in order to ensure confluence.  The choice of pair depends on
whether $u < v$, since all pairs in $\rws$ must reduce words with respect to the
chosen ordering on $X^*$ to make sure the system is terminating.
% TODO: the whole u<v v<u thing should be mentioned explicitly earlier, not as
% an afterthought.

Let us consider an example of each of the two types of critical pair in
Definition \ref{def:critical-pair}.

\begin{example}
  Let $X=\{a,b,c\}$, and let $\rws$ be a rewriting system on $X$, containing
  two rules $(ab, c)$ and $(bb, a)$.  Here the word $abb$ could be
  rewritten by either rule: $abb = (ab)b \to cb$, but also
  $abb = a(bb) \to aa$.  Hence $(cb, aa)$ is a critical pair of type (i).

  If there exists some $w \in X^*$ such that $cb \tostar w$ and $aa \tostar w$,
  then confluence is not violated; otherwise, we must add a new rule to $\rws$
  to make sure confluence holds.  The Knuth-Bendix process adds the rule
  $(cb, aa)$, since $aa < cb$ in our shortlex order.  Now Theorem
  \ref{thm:knuth-bendix} is satisfied.
\end{example}

\begin{example}
  Let $X=\{a,b,c\}$ and let $\rws$ be a rewriting system on $X$ with rules
  $(abc, c)$ and $(b, a)$.  The word $abc$ can now be written by either rule,
  $abc \to c$ or $abc \to aac$.  Hence $(c, aac)$ is a critical pair of type
  (ii).

  Again, if there are other rules allowing both words to be rewritten to a word
  $w \in X^*$, then no rules need to be added; if however there are no such
  rules, we must add $(aac, c)$ to ensure the theorem is satisfied, and
  confluence can be guaranteed.
\end{example}

The Knuth-Bendix completion process consists of searching for these critical
pairs and adding rules where necessary.  It should be noted that, like the
Todd-Coxeter procedure, there is no guarantee that the algorithm will complete
in a finite time unless the presentation $\pres X R$ is known to be finite.
More rules may be added continually, which themselves need to
be checked for critical pairs, without a complete set ever being found.  It may
also be that a confluent terminating rewriting system will be found, but after a
very long time, and it is never known whether the process will finish until it
does.  However, this process has one clear advantage over Todd-Coxeter, which is
that it may complete even when the monoid $\pres X R$ is infinite, so long as
the set of rules in the rewriting system is finite
(see Example \ref{ex:bicyclic}).  This is a strong argument
in favour of trying the Knuth-Bendix process along with other methods.  However,
the Knuth-Bendix process cannot be used for left or right congruences, an area
where Todd-Coxeter has a clear advantage.

We give an example of a rewriting system which solves the word problem for an
infinite monoid:

\begin{example}
  \label{ex:bicyclic}
  The \textit{bicyclic monoid} $B = \pres{b,c}{bc=\varepsilon}$ trivially admits
  a rewriting system $\{(bc, \varepsilon)\}$.  Using this, any element can be
  rewritten in a finite number of steps to an irreducible word of the form
  $c^ib^j$.  There is only one rule in the rewriting system.  Since it reduces
  words by the \textit{short-lex} ordering, the system is terminating; and since
  there are no critical pairs, the system is confluent by Theorem
  \ref{thm:knuth-bendix}.
\end{example}

Pseudo-code (Sims p.77)

Use with Froidure-Pin

\clearpage

\subsection{Froidure-Pin}
\label{sec:fp}

Background

The KBFP algorithm

Semigroups/congs it works/works best on

Complexity

\section{Running in parallel}

How do we tie together all the different algorithms?

\section{Implementation}

Practical considerations in libsemigroups

\section{Benchmarking}
\label{sec:benchmarking}

Comparing the three algorithms: good and bad examples for each.

Showing off speed.

Drawbacks.

\section{Future work}

For example:
\begin{itemize}
\item Linking Knuth-Bendix and Todd-Coxeter so they feed into each other;
\item Multiplying concrete elements directly in some places instead of using
  the presentation (beyond prefilling TC);
\item In the GAP library, anything interesting they might do;
\end{itemize}
