\chapter{Parallel method for generating pairs}
\label{chap:pairs}

A congruence is a binary relation, and therefore is formally described as a set
of pairs.  In a computational setting, it is rarely practical to keep track of
every pair in a congruence; a congruence on a semigroup of size $n$ contains
$n^2$ pairs in the worst case, and on an infinite semigroup contains an infinite
number of pairs.  A congruence can be described in more concise ways:
for example, taking advantage of it being an equivalence relation
and recording only its equivalence classes; or in the case of a Rees congruence,
storing a generating set for the ideal which defines it.  A variety of different
ways to describe a congruence are explained in Chapter \ref{chap:converting},
along with ways to convert from one to another.  However, a congruence is still just
a set of pairs, and by reducing the number of pairs we store, we can often describe a
congruence very concisely using them.

\begin{definition}
  Let $S$ be a semigroup and let $R$ be a subset of $S \times S$.
  \begin{itemize}
  \item 
    The \textbf{congruence generated by} $R$ is the least congruence
    (with respect to containment) that contains $R$ as a subset.
  \item
    The \textbf{left congruence generated by} $R$ is the least left congruence
    (with respect to containment) that contains $R$ as a subset.
  \item
    The \textbf{right congruence generated by} $R$ is the least right congruence
    (with respect to containment) that contains $R$ as a subset.
  \end{itemize}
\end{definition}

This definition is based on a simple intuitive idea:
a congruence $\rho$ is generated by a set of pairs $R$ if it consists
of only the pairs in $R$ along with the pairs required by the axioms of a
congruence (reflexivity, symmetry, transitivity and compatibility).  Thus a
congruence can be described completely by storing only a few pairs.
Indeed, experiments on a range of interesting semigroups in Chapter
\ref{chap:lattice} show that many congruences can be generated by an extremely
small number of generating pairs, and indeed that most of the congruences
studied are principal (generated by a single pair). %TODO: actually do this

Another justification for the use of generating pairs is that it is a completely
generic representation.  Some special types of semigroup have their own abstract
representations of congruences---for inverse semigroups, one can study
kernel-trace pairs \cite[\S 5.3]{howie}; for groups, normal subgroups
\cite[Theorem 11.5]{warner}; for completely simple or completely 0-simple
semigroups, linked triples \cite[\S 3.5]{howie}---but generating pairs can
represent a congruence on any semigroup whatsoever.  Furthermore, one
might be interested in what pairs are implied by a given pair or set of pairs in
a congruence, and this representation can answer such questions.

Left congruences and right congruences can also be described using generating
pairs, and some algorithms designed for two-sided congruences can be used with
minor modifications to compute information about left and right congruences.

This chapter describes a parallelised approach for computing a congruence from a
set of generating pairs, as implemented in \texttt{libsemigroups}
\cite{libsemigroups}.  First we will give a general outline of the system
and what questions it hopes to answer, then we will describe in detail each
algorithm used, its advantages and disadvantages, and when it can be applied.
Finally we will explain how the different algorithms are executed together, and
consider their implementation in \cite{libsemigroups}.

\section{Reasons for parallelisation}

Parallel processing has seen major advances in the last ten years, with
multi-core processors becoming the norm in many types of computers, and
processors with 4, 8, or even 16 cores becoming common on a desktop PC.  This
being the case, it is desirable to parallelise mathematical algorithms wherever
possible, and take advantage of the ability to execute multiple
threads of instructions concurrently.  Some algorithms
are ``embarrassingly parallel''---that is, they can be split into
independent threads which require almost no communication with each other.
Examples of these algorithms would be brute force searches, or rendering of
computer graphics.
These are suited so well to parallelisation that splitting the operation into
$n$ parallel threads reduces the expected run-time to barely more than
$\frac{1}{n}$ what it would be when run in a single thread.  Other
% TODO: don't say "1/n what it would be"
algorithms do not parallelise so well: sometimes threads have to communicate, or
use shared resources, causing significant slowdown and severely limiting the
improvements that can be made by parallelising.

When it comes to computing information about a congruence from generating pairs,
there are various different approaches which can be taken: in Sections
\ref{sec:p}, \ref{sec:tc} and \ref{sec:kb}, we describe three possible
algorithms: pair orbit enumeration, Todd-Coxeter, and Knuth-Bendix/Froidure-Pin.
Depending on what sort of semigroup is given as an input (see Section
\ref{sec:program-outputs}), several
or all of these might be appropriate.  However, depending on certain properties
of the congruence, one might perform far better than another.  For example, the
pair orbit algorithm works well on congruences that contain few non-reflexive
pairs, while the Todd-Coxeter algorithm tends to work well on congruences with
few classes (i.e.~very many pairs).  For a detailed analysis of which algorithms
perform well on which inputs, see Section \ref{sec:benchmarking}.  Given
only a set of generating pairs, these properties are likely to be unknown in
advance, which makes it difficult to choose a good algorithm.

The natural answer to this problem is the core concept of this chapter: a
parallel approach which does not attempt to parallelise individual algorithms,
but which runs several known algorithms at the same time, each in a different
thread, and simply halts all threads as soon as any one completes.  Since these
algorithms do not interact with each other in any way, the total run-time will
be close to the minimum run-time of all the different algorithms.  This is
particularly important, since for certain semigroups and congruences some
algorithms will never terminate, while others may terminate in a very short
time.

\section{Applicable types of semigroup}
\label{sec:applicable-types-of-semigroup}

The type of element in a semigroup affects which methods will be most effective,
or even which methods will be applicable.  For this purpose, we divide
semigroups into two categories: finitely presented semigroups, and
\textit{concrete} semigroups.  By \textit{concrete}, we mean a finite semigroup
whose elements can be multiplied and compared quickly, without reference to the
semigroup as a whole; these could be semigroups of transformations, partial
permutations, bipartitions, matrices, or other finite objects.  Their elements
will be known in advance.  Finitely presented semigroups are a different case,
in that their elements are not known in advance, and indeed it may not be known
whether or not a given semigroup is finite.  Finitely presented monoids are treated as
equivalent to finitely presented semigroups, since a semigroup presentation can
be attained by simply adding a generator for the identity and relations to make
it multiplicatively neutral.

\section{Program inputs}
\label{sec:program-inputs}

Our algorithm determines the properties of a single left, right, or two-sided
congruence defined by generating pairs, over a semigroup $S$.
For the remainder of this chapter, the word ``congruence'' will be used to refer
to left, right, and two-sided congruences equally, without having the default
meaning of ``two-sided congruence''. % TODO: move this warning?

If $S$ is a concrete semigroup (as described in Section
\ref{sec:applicable-types-of-semigroup}) then it will almost certainly be
specified by a generating set; in this case, it is quick to use these generators
to calculate a list of elements in $S$, along with left and right Cayley graphs
for $S$, using the Froidure-Pin algorithm (see Section \ref{sec:fp}).  If on the
other hand $S$ is a finitely presented semigroup, then the elements will not be
known in advance.  In either case, a finite presentation $\pres X R$ can be
given---a technique for efficiently finding a presentation for a concrete
semigroup is given in Section \ref{sec:find-pres}.
The exact parameters supplied to the algorithm are therefore as follows:
\begin{itemize}
\item A set of generators $X$;
\item A finite set of relations $R \subseteq X^* \times X^*$;
\item A finite set of generating pairs $W \subseteq X^* \times X^*$;
\item A record of whether we are computing a left, right, or two-sided
  congruence.
\end{itemize}
The following are also supplied if and only if $S$ is concrete:
\begin{itemize}
\item A list of elements of $S$;
\item Left and right Cayley graphs for $S$;
\item A factorisation function $f : S \to X^*$.
\end{itemize}

We shall now make clear the meanings of these different parameters, by giving a
complete description of the system, starting with the commutative diagram in
Figure \ref{fig:pairs-cd-1}.

Here $X$ is our alphabet, a subset of the free semigroup $X^+$.  We have a set
of relations $R \subseteq X^+ \times X^+$, which generates the congruence
$R^\sharp$ on $X^+$.  This congruence gives a quotient semigroup
$X^+ / R^\sharp$; this is isomorphic to the semigroup $S$,
which is described by the presentation $\pres X R$.  The
congruence also gives us its natural homomorphism $\pi: X^+ \to S$ (see
Definition \ref{def:natural-homomorphism}).  We also have a set of generating pairs
$\mathbf{P} \subseteq S \times S$, which defines a congruence
$\mathbf{P}^\sharp$.  The aim of the algorithm described in this chapter is to
obtain a data structure for $\mathbf{P}^\sharp$, where the precise meaning of
``data structure'' is defined in Section \ref{sec:program-outputs}.  If we are
calculating a two-sided congruence, then it gives rise to the
quotient semigroup $S / \mathbf{P}^\sharp$.

\begin{figure}[h]
  \centering
  $\begin{tikzcd}
    X \arrow[dr] \arrow[d, hook] & \\
    X^+ \arrow[r, two heads, "\pi"] & \frac{X^+}{R^\sharp} \cong S \arrow[r, two heads] & \frac{S}{\mathbf{P}^\sharp}
  \end{tikzcd}$
  \caption{The relationships between different input objects}
  \label{fig:pairs-cd-1}
\end{figure}

The generating pairs $\mathbf{P}$ are not given by the user.  Since the elements
of $S$ might be unknown (for example if $S$ was specified by a finite
presentation), it would be impractical for the user to specify them precisely.
Instead, the user specifies a set $W$ consisting of pairs of words from
$X^+ \times X^+$, which can be evaluated to pairs of elements in $S \times S$,
giving the set of generating pairs $\mathbf{P}$.
More formally, let
$\Pi: X^+ \times X^+ \to S \times S$ be defined by
$\Pi: (w_1, w_2) \mapsto (w_1\pi, w_2\pi)$, where $\pi$ is the natural
homomorphism from $X^+$ to $S$ mentioned above.  The generating pairs
$\mathbf{P}$ of the congruence are given by $\mathbf{P} = W \Pi$.
This relationship is summarised in Figure \ref{fig:pairs-cd-2}.

\begin{figure}[h]
  \centering
  $\begin{tikzcd}
    W \arrow[r, hook, two heads, "\Pi|_W"] \arrow[d, hook] & \mathbf{P} \arrow[d, hook] \\
    X^+ \times X^+ \arrow[r, two heads, "\Pi"] & S \times S
  \end{tikzcd}$
  \caption{The generating pairs of the congruence}
  \label{fig:pairs-cd-2}
\end{figure}

\section{Program outputs}
\label{sec:program-outputs}

Each method we are about to explain can provide a variety of different pieces of
information, but it is important to consider which questions we aim to answer.
Our system should be able to return the following information about a given
congruence when requested:

\begin{itemize}
\item Presence of a given pair $(x,y)$ in the congruence
\item Number of congruence classes
\item The elements in each non-trivial congruence class
\item Class number of a given element
\end{itemize}

\clearpage

\section{Decidability}
\label{sec:decidability}

\begin{definition}
  \label{def:decidable}
  A problem is \textbf{decidable} if there exists an algorithm which is
  guaranteed to return a correct answer to the problem in a finite amount of
  time.
\end{definition}

Decidability is always something that should be considered when designing an
algorithm that may act on infinite objects.  In the context of this chapter, if
the semigroup $S$ is specified as a concrete semigroup, then all of the
questions defined in Section \ref{sec:program-outputs} have a definite answer
which can be calculated.  However, if $S$ is specified using a presentation,
then certain problems asked about it may be undecidable.

Firstly, we might consider the presence of a given pair $(x,y)$ in a given
congruence.  This is equivalent to solving the word problem for the quotient
semigroup of the congruence, and has long been known to be undecidable in
general.  Indeed, consider the following example from Gennadi\'{i} Makanin.

\begin{example}[Makanin, 1966]
  The presentation
  $$\pres{a,b,c}{c^2b^2 = b^2c^2,\ bc^3b^2 = cb^3c^2,\ ac^2b^2 = b^2a,\ abc^3b^2 = cb^2a,\ b^2c^2b^4c^2 = b^2c^2b^4c^2a}$$
  defines a semigroup which has undecidable word problem.  \cite{makanin_1966}
\end{example}
% TODO: read this properly, and maybe give a proof.

Since the general semigroup word problem is undecidable, the algorithm in this
chapter cannot hope to guarantee to return an answer.  Similarly, the number of
classes of a given congruence and the elements of each non-trivial class may not
be computable, since there may be infinitely many.

The only guarantee that can be given by the algorithm is that an answer will be
returned if $S$ happens to be finite.  In the concrete case, semigroups are
always finite and so an answer is guaranteed; but in the case of a finitely
presented semigroup, it is unknown in advance whether a semigroup is finite or
not.  Hence, the user of this algorithm might not know whether a particular run
is guaranteed to terminate, since a run that is about to finish is
indistinguishable from one that will run forever.

\section{Finding a presentation}
\label{sec:find-pres}

A concrete semigroup $S$ will not be specified by a presentation, but a finite
presentation is still required for the Todd-Coxeter and
Knuth-Bendix/Froidure-Pin algorithms to be used, and so a presentation
$\pres X R$ must be calculated.  For the purposes of the algorithm described in
this chapter, it is not important how this presentation is obtained, only that
it is correct and that elements from $S$ can be factorised with respect to $X$.
However, in order to implement the ideas of this chapter, it may be useful to
see a description of the method used in \texttt{libsemigroups}
\cite{libsemigroups}.

Description goes here. % TODO

\section{The methods}

\subsection{Pair orbit enumeration}
\label{sec:p}

Background

The P algorithm

Using Knuth-Bendix: KBP

Semigroups/congs it works/works best on

Complexity

\subsection{Todd-Coxeter}
\label{sec:tc}

The Todd-Coxeter algorithm was originally described in 1936 in
\cite{todd_coxeter_1936}.  It was an algorithm to enumerate the cosets of a
finitely generated subgroup of a finitely presented group.  Arriving before the
advent of electronic computers, the algorithm was originally intended to be
carried out by hand.  Perhaps the earliest automatic implementation was on the
EDSAC II computer in Cambridge \cite{leech_1963}.  Since then, a wide variety of
efficient, optimised versions have been implemented, for example \cite{ace}.

A variation of Todd-Coxeter for semigroups was described in 1967
\cite{neumann_1967}.  The algorithm takes a presentation $\pres X R$
for a semigroup $S$ and computes the right regular representation of $S^1$ with
respect to the generators $X$---that is, it computes all the elements of $S$ and
the result of right-multiplying each element by each generator.  Since the
original algorithm makes very little use of those properties unique to groups,
the method applied to semigroups is essentially the same.  Other descriptions of
Todd-Coxeter for semigroups can be found in \cite[Chapter 12]{ruskuc_thesis} and
\cite[Chapter 1.2]{walker_thesis}, and a variation specific to inverse
semigroups can be found in \cite{cutting_thesis}.  Our version of the algorithm
is based closely on an implementation by G\"otz Pfeiffer, found in
\cite[\texttt{lib/tcsemi.gi}]{gap}, and based on \cite{walker_thesis}.

We will now describe the Todd-Coxeter method as used in the context of this
chapter.  Though the method itself does not represent new work, it is an
important part of the overall parallel approach, and in order to understand its
uses and limitations, it is described here in full.

\subsubsection{Setup}

The Todd-Coxeter algorithm is based on a table, where each row corresponds to a
single congruence class (or equivalently, a single element of the quotient
semigroup).  The columns of the table correspond to the generators of the
semigroup, and the entry in row $i$, column $j$ represents the element found by
taking element $i$ and right-multiplying it by generator $j$.  These entries may
be blank, and two different rows may be found to describe the same element.
Mathematically, we can view this table as a triple $(n, \mathbf{N}, \tau)$
consisting of:
\begin{itemize}
\item an integer $n \in \mathbb{N}$ representing the number of rows in the table;
\item a set $\mathbf{N} \subseteq \{1, \ldots, n\}$ containing the indices of the
  \textit{undeleted} rows; and
\item a function $\tau: \mathbf{N} \times X \to \mathbf{N} \cup \{0\}$, where
  $(i, x)\tau$ is equal to the entry in row $i$ and the column corresponding to
  generator $x$---with $0$ representing a blank entry.
\end{itemize}

Suppose that we have a semigroup presentation $\pres X R$ for a semigroup
$S$.  The table is initialised with a single row, numbered $1$.  This row
corresponds to the empty word $\varepsilon$, or the adjoined identity of the
monoid $S^1$.  The row is empty, containing a blank entry in all $|X|$ columns.
In our mathematical notation, we define $n=1$ and
$(1,x)\tau = 0$ for all $x \in X$.

We can naturally extend the function
$\tau: \mathbf{N} \times X \to \mathbf{N} \cup \{0\}$
to a function 
$\bar{\tau}: \mathbf{N} \times X^* \to \mathbf{N} \cup \{0\}$
which is described as follows.
If $w \in X^*$ and $w=w_1 \ldots w_n$, where $w_1, \ldots, w_n \in X$,
then we can define $\bar\tau$ recursively by
$$
(i, w)\bar\tau = \left\{
\begin{matrix*}[l]
  i & \textnormal{if~} w=\varepsilon,\\
  0 & \textnormal{if~} (i, w_1)\tau=0,\\
  ((i, w_1)\tau, w_2 \ldots w_n)\bar\tau & \textnormal{otherwise}.
\end{matrix*} \right.
$$

The effect of $\bar\tau$ is to trace an entire word through the table, starting
at a given row.

\subsubsection{Elementary operations}

We now describe 3 operations which may be applied to the table.  These
operations will be described loosely to give an intuition behind what they are
designed to do, and then each one will be described formally in pseudo-code
(Algorithms \ref{alg:add}, \ref{alg:trace} and \ref{alg:coinc}).  We will then
describe the overall Todd-Coxeter procedure which uses these operations
(Algorithm \ref{alg:tc}).

\begin{itemize}
\item \textsc{Add}: Fill in a blank entry and add a row to the table;
\item \textsc{Trace}: Trace a relation from a row;
\item \textsc{Coinc}: Process a coincidence.
\end{itemize}

The first operation, \textsc{Add}, is simple.  A new row is added at the bottom
of the table, and its number is written into the blank cell in the table
specified by the arguments given to \textsc{Add}.  Pseudo-code for this
operation is given in Algorithm \ref{alg:add}.

\begin{algorithm}
\caption{The \textsc{Add} algorithm}
\label{alg:add}
\begin{algorithmic}[1]
\Procedure{Add}{$i, x$}
\State $n \gets n + 1$
\State $\mathbf{N} \gets \mathbf{N} \cup \{n\}$
\For{$x \in X$}
  \State $(n, x)\tau := 0$
\EndFor
\State $(i, x)\tau \gets n$  
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textsc{Trace} takes two arguments: a relation $v=w$ from $R$, and a row $e$
in the table.  We now give an informal description of this procedure---see
Algorithm \ref{alg:trace} for pseudo-code.

We start from row $e$, and find $(e, v)\bar\tau$ by processing $v$ one letter at a time: we
find the column corresponding to the letter, and look at that position in the
table; this gives us the number of the new row which we move to.  If we
encounter a blank entry before the final letter, we apply \textsc{Add} to that
cell, and follow the new entry.  At the end of the process we have an entry in
the table, blank or filled, called $(e, v)\bar\tau$.  We repeat the
process with the other word to find $(e, w)\bar\tau$.

To satisfy $v=w$, we need to set these two entries such that
$(e, v)\bar\tau = (e, w)\bar\tau$.
\begin{itemize}
\item If $(e, v)\bar\tau$ and $(e, w)\bar\tau$ are both blank, then we apply
  \textsc{Add} to $(e, v)\bar\tau$ and copy the entry into $(e, w)\bar\tau$.
\item If just one of the entries is filled, then the filled entry is copied into
  the blank one.
\item If both entries are filled and equal, we need do nothing.
\item If both entries are filled and are distinct, we apply \textsc{Coinc} to
  the two entries.
\end{itemize}

\begin{algorithm}
\caption{The \textsc{Trace} algorithm}
\label{alg:trace}
\begin{algorithmic}[1]
\Procedure{Trace}{$e, v = w$}
\State Write $v = v_1 \ldots v_m$ \Comment $(v_i \in X \text{~for~} 1 \leq i \leq m)$
\State Write $w = w_1 \ldots w_n$ \Comment $(w_i \in X \text{~for~} 1 \leq i \leq n)$
\State $s \gets e$
\For{$i \in \{1, \ldots, m-1\}$}
  \If{$(s, v_i)\tau = \varepsilon$}
    \State \Call{Add}{$s, v_i$}
  \EndIf
  \State $s \gets (s, v_i)\tau$
\EndFor
\State $t \gets e$
\For{$i \in \{1, \ldots, n-1\}$}
  \If{$(t, w_i)\tau = \varepsilon$}
    \State \Call{Add}{$t, w_i$}
  \EndIf
  \State $t \gets (t, w_i)\tau$
\EndFor

\If{$(s, v_m)\tau = (t, w_n)\tau = \varepsilon$}
  \State \Call{Add}{$s, v_m$}
  \State $(t, w_n)\tau \gets (s, v_m)\tau$
\ElsIf{$(s, v_m)\tau = \varepsilon$}
  \State $(s, v_m)\tau \gets (t, w_n)\tau$
\ElsIf{$(t, w_n)\tau = \varepsilon$}
  \State $(t, w_n)\tau \gets (s, v_m)\tau$
\ElsIf{$(s, v_m)\tau \neq (t, w_n)\tau$}
  \State \Call{Coinc}{$(s, v_m)\tau, (t, w_n)\tau$}
\EndIf

\EndProcedure
\end{algorithmic}
\end{algorithm}

\textsc{Coinc} is used when two rows in the table are found to refer to the same
element of $S$.  The higher-numbered row is deleted, and all occurrences of the
higher number in the table are replaced by the lower number.  The two rows are
combined into one, with all known information being preserved---this may imply
that another pair of rows are equal, another coincidence that should be
processed by a recursive call to \textsc{Coinc} (or should be stored in a list
for processing after this call is finished).

\begin{algorithm}
\caption{The \textsc{Coinc} algorithm}
\label{alg:coinc}
\begin{algorithmic}[1]
\Require $r < s$
\Procedure{Coinc}{$r, s$}
\State $\mathbf{N} \gets \mathbf{N} \setminus \{s\}$
\For{$e \in \mathbf{N}$}
  \For{$x \in X$}
    \If{$(e, x)\tau = s$}
      \State $(e, x)\tau \gets r$
    \EndIf
  \EndFor
\EndFor
\For{$x \in X$}
  \If{$(r, x)\tau = 0$}
    \State $(r, x)\tau \gets (s, x)\tau$
  \ElsIf{$(r, x)\tau \neq (s, x)\tau \textbf{~and~} (s, x)\tau \neq 0$}
    \State \Call{Coinc}{$(r, x)\tau, (s, x)\tau$}
  \EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Now that we have these three operations, it is simple to describe the overall
procedure, as shown in Algorithm \ref{alg:tc}.  We go through the list of used
rows $\mathbf{N}$, starting with row $1$.  To each
of these rows we apply each relation from $R$, using \textsc{Trace}.  Each call
to \textsc{Trace} may, of course, invoke calls to \textsc{Add} and
\textsc{Coinc}, so rows will be appended to the list as the algorithm
progresses.  When the end of $\mathbf{N}$ is reached, the table should
completely describe the multiplication for the finitely presented semigroup:
each row in $\mathbf{N} \setminus \{1\}$ represents one element of $S$, and
$(i, x)\tau$ represents the element denoted by $i$ right-multiplied by the
generator $x$.

\begin{algorithm}
\caption{The \textsc{Todd-Coxeter} algorithm for semigroups}
\label{alg:tc}
\begin{algorithmic}[1]
\Procedure{Todd-Coxeter}{$\pres X R$}
\State $n := 1$
\State $\mathbf{N} := \{1\}$
\For{$x \in X$}
  \State $(1, x)\tau := 0$
\EndFor
\For{$e \in \mathbf{N}$}
  \For{$(u=v) \in R$}
    \State \Call{Trace}{$e, u=v$}
  \EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Note that there is no guarantee that the end of $\mathbf{N}$ will ever be
reached: if the given presentation defines an infinite semigroup, the table will
grow forever and the procedure will never terminate.  On the other hand, if the
presentation defines a finite semigroup, the procedure is guaranteed to
terminate in a finite number of steps (see
\cite[Theorem 5.5]{cgt} and \cite[Theorem 3]{beetham_campbell_1976}).
This number of steps is, however, unbounded; and since a user may
not know in advance whether a presentation defines a finite or infinite
semigroup, it is impossible to
know, while the procedure is running, whether it will end.

\subsubsection{An example}
We now give an example of the Todd-Coxeter algorithm running on the semigroup
presentation
$$\pres{a, b}{ba=ab,\ b^2=b,\ a^3=ab,\ a^2b=a^2}.$$
We initialise the table to look like Table \ref{tab:tc1}.
\tctableAB{tab:tc1}
{Initial position}
{ 1 & & \\ }

The list $\mathbf{N}$ of undeleted rows contains only a single entry, $1$.  We
begin by tracing each relation on the row $1$, starting with $ba=ab$.
The left-hand side of this
relation makes us call \textsc{Add} on the cell $(1, b)$, creating a new row,
$2$, which is added to $\mathbf{N}$.
For the right-hand side, we must call \textsc{Add} on
the cell $(1, a)$, creating a row $3$.  At the end of the \textsc{Trace}, we
must set $(1, ba)\bar\tau$ equal to $(1, ab)\bar\tau$, so we set both
$(2, a)\tau$ and $(3, b)\tau$ to $4$ (as in Table \ref{tab:tc2}).
\tctableAB{tab:tc2}
{Position after \textsc{Trace}($1, ba=ab$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & \\
  \cline{2-3}
  3 & & 4 \\
  \cline{2-3}
  4 & & \\
}
Next, we apply \textsc{Trace}($1, b^2=b$).  Since $(1, b)\bar\tau$ is already
set, we just set $(1, b^2)\bar\tau$ equal to it: $(2, b)\tau \gets 2$.  See
Table \ref{tab:tc3}.
\tctableAB{tab:tc3}
{Position after \textsc{Trace}($1, b^2=b$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & & 4 \\
  \cline{2-3}
  4 & & \\
}
Still on row $1$, we apply \textsc{Trace} to the third relation, $a^3=ab$.  This
creates a new row for $(1, a^2)\bar\tau = (3, a)\tau = 5$.  The new row's $a$
entry is set to be the same as $(1, ab)\bar\tau$, which is $4$
(see Table \ref{tab:tc4}).
\tctableAB{tab:tc4}
{Position after \textsc{Trace}($1, a^3=ab$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & & \\
  \cline{2-3}
  5 & 4 & \\
}

The final relation for row $1$ is $a^2b=a^2$.
$(1, a^2b)\bar\tau$ is currently blank, and is set to the current value of
$(1, a^2)\bar\tau$, which is $5$.  Hence $(5, b)\tau \gets 5$
(as in Table \ref{tab:tc5}).
\tctableAB{tab:tc5}
{Position after \textsc{Trace}($1, a^2b=a^2$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & & \\
  \cline{2-3}
  5 & 4 & 5 \\
}
We have now finished with row $1$, and we proceed to the next row in
$\mathbf{N}$, which is $2$.  Accordingly, we apply the first relation,
\textsc{Trace}($2, ba=ab$).  The value of $(2, ba)\bar\tau$ is $4$, whereas the
value of $(2, ab)\bar\tau$ has not yet been set.
We set it by applying $(4, b)\tau \gets 4$.
See Table \ref{tab:tc6}.
\tctableAB{tab:tc6}
{Position after \textsc{Trace}($2, ba=ab$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & & 4 \\
  \cline{2-3}
  5 & 4 & 5 \\
}
Proceeding with \textsc{Trace}($2, b^2=b$), we find that
$(2, b^2)\bar\tau = (2, b)\bar\tau$ already, so we make no modifications to the
table.  Next, \textsc{Trace}($2, a^3=ab$) discovers that $(2, a^2)\bar\tau$ is
not set, and so we call \textsc{Add}($4, a$), creating a new row $6$.
Now $(6, a)\tau$ is set to $(2, ab)\bar\tau$ which is equal to $4$.
See Table \ref{tab:tc7}.
\tctableAB{tab:tc7}
{Position after \textsc{Trace}($2, a^3=ab$)}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & 6 & 4 \\
  \cline{2-3}
  5 & 4 & 5 \\
  \cline{2-3}
  6 & 4 & \\
}
The final relation for row $2$ is $a^2b=a^2$, setting $(6, b)\tau \gets 6$ (see
Table \ref{tab:tc8}).
\tctableAB{tab:tc8}
{Position after all relations on row $2$}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & 6 & 4 \\
  \cline{2-3}
  5 & 4 & 5 \\
  \cline{2-3}
  6 & 4 & 6 \\
}
Next we move onto row $3$, and we apply \textsc{Trace}($3, ba=ab$).  Inspecting
the table shows $(3, ba)\bar\tau = 6$ but $(3, ab)\bar\tau = 5$, giving rise to
a coincidence.  We apply \textsc{Coinc}($5, 6$), which deletes row $6$, rewrites
any occurrences of $6$ in the table to $5$, and copies row $6$ into row $5$
(yielding no new information).  The result is shown in Table \ref{tab:tc9}.  The
rest of the relations are applied to row $3$, and to the remaining rows in the
table, but no changes are made to the table, so Table \ref{tab:tc9} is the final
state.
\tctableAB{tab:tc9}
{Final position}
{
  1 & 3 & 2 \\
  \cline{2-3}
  2 & 4 & 2 \\
  \cline{2-3}
  3 & 5 & 4 \\
  \cline{2-3}
  4 & \cancel{\textcolor{gray}{6}}5\!\!\! & 4 \\
  \cline{2-3}
  5 & 4 & 5 \\
  \cline{2-3}
  \textcolor{gray}{6} & \textcolor{gray}{4} & \textcolor{gray}{6} \\[-1.6ex]
  \hline\noalign{\vspace{\dimexpr 1.4ex}} \cline{2-3}
}

We can now delete row $1$, which acts as an appended identity, and we find a
description of the semigroup's multiplication, with relation to its generators.
This description can be represented as a Cayley graph,
as shown in Figure \ref{fig:tc-cayley-graph}.
\begin{figure}[H]
  \centering
  \begin{dot2tex}
    //dot
    digraph {
      rankdir=LR
      node [shape=circle]
      2
      3
      4
      5
      2 -> 4 [label=a]
      2 -> 2 [label=b]
      3 -> 5 [label=a]
      3 -> 4 [label=b, dir=both]
      4 -> 5 [label=a, dir=both]
      4 -> 4 [label=b]
      5 -> 5 [label=b]
    }
  \end{dot2tex}
  \caption{Right Cayley graph of $\pres{a, b}{ba=ab,\ b^2=b,\ a^3=ab,\ a^2b=a^2}$}
  \label{fig:tc-cayley-graph}
\end{figure}
It is worth noting that the columns of the table now give a right representation
of $S$.  That is, $S$ is isomorphic to the semigroup generated by the
transformations $\transV 34554$ and $\transV 22445$.

\subsubsection{Improvements}
Left/right congruences

Pre-filling the table

Semigroups/congs it works/works best on

Complexity

\subsubsection{Implementation}

Rows will be added to the table, and deleted from it.  A list must be kept of
rows which are in use; when a row is added, its position in the table should be
appended to this list at the end, and when a row is deleted it should be removed
from its position in the list and added to a list of ``free rows'' which can be
reused later.  The ``rows in use'' list is best implemented as a doubly-linked
list, so that single entries can be added and removed with as little processor
work as possible.

\clearpage

\subsection{Rewriting systems}
\label{sec:kb}

Another approach for solving the word problem in a finitely presented semigroup
is using rewriting systems.  Hence, given a semigroup $S$ with presentation
$\pres X R$ and a congruence $\rho$ over $S$ with generating pairs given by $W$
(see Section \ref{sec:program-inputs})
we may be able to find a rewriting system which converts any word $w \in X^+$ to
a canonical word representing the same element of $\pres X {R,\ W}$;
that is, a word representing a semigroup element in the same $\rho$-class as
the semigroup element of $S$ represented by $w$.

For ease of notation and understanding, this section will describe an algorithm
for the word problem on a finitely presented semigroup.  We understand that this
is the same as the problem of whether a given pair of words represent semigroup
elements related to each other by a two-sided congruence $\rho$.  The ideas do
not extend easily to left and right congruences, which will therefore not have
this method as an option.

In order
to describe the process, we must first explain some background theory.  A full
description of these ideas can be found in \cite[Section 12.2]{cgt}.  Note that
we shall again consider monoid presentations instead of semigroup presentations,
since it is easy to change between the two by appending or removing an identity
(the empty string $\varepsilon$).

Let $X$ be an alphabet.  A \textit{rewriting system} $\rws$ on $X^*$ is a
set of ordered pairs $(u,v)$ where $u, v \in X^*$.
A pair $(u,v) \in \rws$ is called a \textit{rule}, and can be viewed as
an operation which transforms an occurrence of $u$ in a word into an occurrence
of $v$.
For this section, we will assume that $\rws$ is finite.
A rewriting system $\rws$ extends to relations
$\to_\rws$, $\tostar_\rws$, and $\lrstar_\rws$
which describe how words are rewritten, and which are defined as follows.

Let $u, v \in X^*$ and let $\rws$ be a rewriting system.
We write $u \to_\rws v$ if there exist $(w_1, w_2) \in \rws$ and
$s, t \in X^*$ such that $u=sw_1t$ and $v=sw_2t$.
That is, $u \to_\rws v$ if a rule rewrites a contiguous subword of $u$ to turn
$u$ into $v$.  The relation $\tostar_\rws$ is simply the reflexive transitive
closure of $\to_\rws$; that is, $u \tostar_\rws v$ if and only if $u = v$ or
$$u = u_0 \to_\rws u_1 \to_\rws \ldots \to_\rws u_n = v,$$
for some $u_0, \ldots, u_n \in X^*$.
Finally, $\lrstar_\rws$ is the symmetric closure of
$\tostar_\rws$.  It is easy to see that $\lrstar_\rws$ is an equivalence
relation whose classes we may write as $[w]_\rws$.
Where there is no chance of ambiguity, we omit the subscript in these
operations, just writing $\to$, $\tostar$ and $\lrstar$.

This definition of a rewriting system does not guarantee that a word can be
rewritten in a useful way.  A rewriting system could allow an endless loop of
rewriting; for example, a system over the alphabet $\{a,b\}$ could contain rules
$(aa,b)$ and $(b,aa)$ which would allow the rewrite sequence
$$aa \to b \to aa \to b \to aa \to b \to aa \to \ldots$$
to go on forever.  Alternatively, a it could be possible to rewrite one word in
two different ways; for example, a system over the alphabet $\{a,b,c\}$ could
contain rules $(aa,b)$ and $(aa,c)$.
In order to solve the word problem for a semigroup, we require a
rewriting system with certain properties.  We will describe these properties,
and then explain how to produce a rewriting system which satisfies them.

\begin{definition}
  A string $u \in X^*$ is $\rws$-\textbf{irreducible} if there is no
  string $v \in X^*$ such that $u \to v$; that is, $u$ cannot be rewritten by
  any rule in $\rws$.  \cite[Def~12.13]{cgt}
\end{definition}

\begin{definition}
  A rewriting system is \textbf{terminating} if there is no infinite chain of
  words $u_1, u_2, \ldots \in X^*$ such that $u_i \to u_{i+1}$ for all $i > 0$.
\end{definition}

If a rewriting system is terminating, this is good news computationally.  It
means that any word can be transformed by rules only a finite number of times
before it reaches an irreducible state, so the task of finding an irreducible
form of a word is guaranteed to be achievable in finite time.  But note that we
can still only talk about \textit{an} irreducible word, not \textit{the}
irreducible word.  We could still have a word $u \in X^*$ and irreducible words
$v, w \in X^*$ such that $u \tostar v$ and $u \tostar w$ but $v \neq w$.
To avoid this, we must ensure that the system is \textit{confluent}, as follows.

\begin{definition}
  A rewriting system is \textbf{confluent} if, for any words $u,v_1,v_2 \in X^*$
  such that $u \tostar v_1$ and $u \tostar v_2$, there exists a word $w \in X^*$
  such that $v_1 \tostar w$ and $v_2 \tostar w$.
\end{definition}

The intuition behind this definition is that, as the name suggests, different
paths ``flow together''.  The result is that, in a confluent terminating
rewriting system, rules can be applied to a word in any order, and a canonical
irreducible word will be found in a finite number of steps.
% If $\rws$ is
% a confluent terminating rewriting system, then let us write $(w)f_\rws$
% or $(w)f$ for the unique irreducible word which $w$ can be written to.  Let
% $[w]_\rws$ or $[w]$ be the set of all words $u \in X^*$ such that
% $u \tostar (w)f$.

Another definition will help us to determine whether a rewriting system is
confluent: \textit{local confluence}.  This is a weaker condition than
confluence, but the two are strongly linked by Lemma \ref{lem:newman}.


\begin{definition}
  A rewriting system is \textbf{locally confluent} if, for any words
  $u,v_1,v_2 \in X^*$ such that $u \to v_1$ and $u \to v_2$, there exists a word
  $w \in X^*$ such that $v_1 \tostar w$ and $v_2 \tostar w$.
\end{definition}

\begin{lemma}[Newman's diamond lemma] % TODO: reference
  \label{lem:newman}
  A terminating rewriting system is confluent if and only if it is locally
  confluent.
\end{lemma}

Lemma \ref{lem:newman} gives us an idea of how to check computationally whether
a system is confluent: rather than checking every possible transitive rewriting
of a word (its neighbours under $\tostar$), it suffices to check a word's
immediate children (its neighbours under $\to$).  This lemma will help us later,
with Theorem \ref{thm:knuth-bendix} and the Knuth-Bendix procedure.

We can now see an application of rewriting systems to the word problem in a
finitely presented monoid.  Indeed, given an alphabet $X$ and a rewriting system
$\rws$, the quotient monoid $X^* / \lrstar_\rws$ is described by the
monoid presentation $\pres{X}{\rws}$.  Hence, given a monoid $M$ with a
presentation $\pres X R$, if there is a confluent terminating rewriting system
$\rws$ such that the word equality relation $=_M$ is the same as the
relation $\lrstar_\rws$, then the word problem can be solved
simply by rewriting two words using $\rws$ until their irreducible
representatives are found, and then comparing them.  The only difficulty is in
finding a rewriting system which is confluent and terminating---but we can find
one, by starting with the set of relations $R$, and then using the Knuth-Bendix
completion algorithm.

Let $M$ be a monoid with finite presentation $\pres X R$.
We start with no rules in our rewriting system, $\rws = \varnothing$, and
we begin to add relations from $R$.  However, in order to ensure our system is
\textit{terminating}, we must reorder each relation to ensure we do not create
any loops.  For this purpose, we define a total ordering on $X^*$ and reorder
each relation so that a rule $(u, v)$ has the property that $u < v$.  Criteria
for this ordering are discussed in \cite[p.~420]{cgt}, but it will suffice to
use the \textit{shortlex ordering}: $u < v$ if and only if $u$ is shorter than
$v$ or they have equal length and $u$ is less than $v$ lexicographically.  Hence the
first few words over the alphabet $\{a, b\}$ are
$$\varepsilon < a < b < aa < ab < ba < bb < aaa < aab < aba < \ldots$$
Note that this requires a well-understood total order on the alphabet $X$
itself.  This ordering justifies the use of the words ``reducible'' and
``irreducible''---words are always replaced with lesser words.

Once each relation from $R$ is added---possibly reordered---to $\rws$, we
will have a terminating rewriting system such that
$X^* / \lrstar_\rws\ = \pres X R$, as required.  It only remains to add
rules to $\rws$ to make it confluent, without altering the relation
$\lrstar$.  This is where we use the Knuth-Bendix completion process.

First described by Knuth and Bendix in \cite{knuth_bendix}, the completion
process adds rules to $\rws$ based on finding and resolving
\textit{critical pairs}.  Roughly speaking, a critical pair is a pair of rules $(u_1, v_1)$ and
$(u_2, v_2)$ from $\rws$ such that $u_1$ and $u_2$ overlap and therefore a word could be
rewritten as either of two different irreducible words.  The formal definition
is as follows.

\begin{definition}
  \label{def:critical-pair}
  Let $\rws$ be a terminating rewriting system over an alphabet $X^*$.  A
  \textbf{critical pair} is a pair of distinct words
  $(w_1, w_2) \in X^* \times X^*$ such that $w_1$ and $w_2$ can be produced from
  the same word by two different rules.
  Critical pairs arise in two different ways.
  Let $(u_1, v_1)$ and $(u_2, v_2)$ be rules in $\rws$.
  \begin{enumerate}[(i)]
  \item If $u_1 = rs$ and $u_2 = st$ with $r,s,t \in X^*$ and
    $s \neq \varepsilon$, then $rst \to v_1t$ and $rst \to rv_2$, so
    $(v_1t, rv_2)$ is a critical pair.
  \item If $u_1 = ru_2t$ for some $r,t \in X^*$ and $u_2 \neq \varepsilon$, then
    $u_1 \to v_1$ and $u_1 \to rv_2t$, so $(v_1, rv_2t)$ is a critical
    pair.
  \end{enumerate}
\end{definition}

The importance of critical pairs is shown in the following theorem, which is key
to the Knuth-Bendix completion process.

\begin{theorem}[Lemma 12.17 in \cite{cgt}]
  \label{thm:knuth-bendix}
  A terminating rewriting system $\rws$ over $X$ is confluent if and only if,
  for each critical pair $(u, v)$, there exists some $w \in X^*$ such that
  $u \tostar w$ and $v \tostar w$.
\end{theorem}

Now we have all the concepts required to describe the Knuth-Bendix completion
process.  The process searches through rules in $\rws$ looking for critical
pairs; when a critical pair $(u, v)$ is found which does not satisfy the
condition stated in Theorem \ref{thm:knuth-bendix}, $(u, v)$ or $(v, u)$ is
added to $\rws$ in order to ensure confluence.  The choice of pair depends on
whether $u < v$, since all pairs in $\rws$ must reduce words with respect to the
chosen ordering on $X^*$ to make sure the system is terminating.
% TODO: the whole u<v v<u thing should be mentioned explicitly earlier, not as
% an afterthought.

Let us consider an example of each of the two types of critical pair in
Definition \ref{def:critical-pair}.

\begin{example}
  Let $X=\{a,b,c\}$, and let $\rws$ be a rewriting system on $X$, containing
  two rules $(ab, c)$ and $(bb, a)$.  Here the word $abb$ could be
  rewritten by either rule: $abb = (ab)b \to cb$, but also
  $abb = a(bb) \to aa$.  Hence $(cb, aa)$ is a critical pair of type (i).

  If there exists some $w \in X^*$ such that $cb \tostar w$ and $aa \tostar w$,
  then confluence is not violated; otherwise, we must add a new rule to $\rws$
  to make sure confluence holds.  The Knuth-Bendix process adds the rule
  $(cb, aa)$, since $aa < cb$ in our shortlex order.  Now Theorem
  \ref{thm:knuth-bendix} is satisfied.
\end{example}

\begin{example}
  Let $X=\{a,b,c\}$ and let $\rws$ be a rewriting system on $X$ with rules
  $(abc, c)$ and $(b, a)$.  The word $abc$ can now be written by either rule,
  $abc \to c$ or $abc \to aac$.  Hence $(c, aac)$ is a critical pair of type
  (ii).

  Again, if there are other rules allowing both words to be rewritten to a word
  $w \in X^*$, then no rules need to be added; if however there are no such
  rules, we must add $(aac, c)$ to ensure the theorem is satisfied, and
  confluence can be guaranteed.
\end{example}

The Knuth-Bendix completion process consists of searching for these critical
pairs and adding rules where necessary.  For a more detailed description of
precisely how these tasks are done, see \cite[\S 2.6]{sims}.

A large part of the computational work involved in the Knuth-Bendix process
consists of rewriting words using rules from $\rws$.  If the rewriting system is
confluent and terminating, then the rules can be applied in any order and an
irreducible word will eventually be reached.  However, the order in which the
rules are applied can have a great effect on the overall runtime of the
algorithm.  In Algorithm \ref{alg:rewrite} we present one possible strategy
known as \textit{rewriting from the left}.  This is the strategy used in
\texttt{libsemigroups} \cite{libsemigroups} and is adapted from the
\textsc{Rewrite\_From\_Left} procedure described in \cite[\S 2.4]{sims}.  It
takes two arguments---a word $u$ and a confluent terminating rewriting system
$\rws$---and returns the irreducible word produced from $u$ by applying rules in
$\rws$.

\begin{algorithm}
\caption{The \textsc{Rewrite} algorithm}
\label{alg:rewrite}
\begin{algorithmic}[1]
\Procedure{Rewrite}{$u, \rws$}
\State $v := \varepsilon$
\While{$u \neq \varepsilon$}
  \State Let $u_1$ be the first character of $u$
  \State $v \gets v u_1$
  \State Remove the first character of $u$
  \For{$(p,q) \in \rws$}
    \If{$v = rp$ for some $r \in X^*$}
      \State $u \gets qu$
      \State $v \gets r$
      \State \Break
    \EndIf
  \EndFor
\EndWhile
\State \Return $v$
\EndProcedure
\end{algorithmic}
\end{algorithm}

It should be noted that, like the
Todd-Coxeter procedure, there is no guarantee that the algorithm will complete
in a finite time unless the presentation $\pres X R$ is known to be finite.
More rules may be added continually, which themselves need to
be checked for critical pairs, without a complete set ever being found.  It may
also be that a confluent terminating rewriting system will be found, but after a
very long time, and it is never known whether the process will finish until it
does.  However, this process has one clear advantage over Todd-Coxeter, which is
that it may complete even when the monoid $\pres X R$ is infinite, so long as
the set of rules in the rewriting system is finite
(see Example \ref{ex:bicyclic}).  This is a strong argument
in favour of trying the Knuth-Bendix process along with other methods.  However,
the Knuth-Bendix process cannot be used for left or right congruences, an area
where Todd-Coxeter has a clear advantage.

We give an example of a rewriting system which solves the word problem for an
infinite monoid:

\begin{example}
  \label{ex:bicyclic}
  The \textit{bicyclic monoid} $B = \pres{b,c}{bc=\varepsilon}$ trivially admits
  a rewriting system $\{(bc, \varepsilon)\}$.  Using this, any element can be
  rewritten in a finite number of steps to an irreducible word of the form
  $c^ib^j$.  There is only one rule in the rewriting system.  Since it reduces
  words by the \textit{short-lex} ordering, the system is terminating; and since
  there are no critical pairs, the system is confluent by Theorem
  \ref{thm:knuth-bendix}.
\end{example}

\clearpage

\subsection{Froidure-Pin}
\label{sec:fp}

Background

The KBFP algorithm

Semigroups/congs it works/works best on

Complexity

\section{Running in parallel}

How do we tie together all the different algorithms?

\section{Implementation}

Practical considerations in libsemigroups

\clearpage

\section{Benchmarking}
\label{sec:benchmarking}

The approach described in this chapter was implemented in the C++ programming
language in \texttt{libsemigroups} \cite{libsemigroups}, and this allows us to
run benchmarking tests to analyse the performance of the different algorithms,
compared to each other and compared to existing programs.  In this section we
provide some examples of such tests.

\subsection{Examples}
Firstly, it will be helpful to examine a few examples of problems which can be
solved by the different algorithms, and to compare the types of problems in
which each algorithm is most effective.  These examples were implemented in the
benchmarking section of \texttt{libsemigroups}, and their performance was tested
with various algorithms.

\begin{example}
  \label{ex:good-tc}
  Consider $\PBR_2$, the full PBR monoid of degree $2$.  It has $65536$ elements
  and is therefore large enough to require significant processing time when
  considering its congruences.   We take a set of pairs which we know will
  generate the universal congruence, and we ask \texttt{libsemigroups} for the
  number of congruence classes, using simple Todd-Coxeter in one case and
  pre-filled Todd-Coxeter in the other.

  On average, simple Todd-Coxeter returns an answer in 724 milliseconds, while
  pre-filled Todd-Coxeter requires an average 3057 milliseconds, more than 4
  times as long.  This difference can be explained by the structure
  of the table at completion: a single row representing the one congruence
  class.  In the pre-fill case, a coset table must be constructed with 65536
  rows, only to have all but one row deleted.  In the simple Todd-Coxeter case,
  the number of rows may stay very small throughout the procedure, drastically
  reducing the amount of work which needs to be done.
\end{example}

\begin{example}
  \label{ex:good-tc-prefill}
  The virtue of pre-filled Todd-Coxeter is shown by a different congruence on
  $\PBR_2$.  A much smaller set of generating pairs gives rise to a congruence
  $\rho$ with 19009 congruence classes, in contrast to the universal congruence
  which only has one.  On average, simple Todd-Coxeter returns in around 128
  seconds, while the pre-filled version completes in under 12 seconds, a tenfold
  increase in speed.  This disparity can be explained by the large number of
  rows which must be in the table on completion: simple Todd-Coxeter has to
  compute a large amount of information from relations, and add rows one by one,
  while pre-filled Todd-Coxeter begins the process with much of the table's
  information already known, and only has to combine rows which coincide.
\end{example}

The two examples above show the difference in speed between two algorithms.  We
now consider an example where certain algorithms will never terminate, but
others complete quickly.  This is an even greater justification for the parallel
use of a variety of algorithms.

\begin{example}
  \label{ex:good-kbfp}
  Consider the semigroup $S$ defined by the semigroup presentation
  $\pres{a,b}{a^3=a,\ ab=ba}$.  We define $\rho$ to be the congruence on $S$
  generated by the single pair $(a, a^2)$.  Since each word of the form $b^i$
  represents a different element of $S$, and a different class of $\rho$, we can
  conclude that $S$ is infinite and $\rho$ has an infinite number of congruence
  classes.  A test asks whether $(ab^{20}, a^2b^{20})$ is a pair in $\rho$.  The
  Todd-Coxeter methods would never complete, since they require a separate row
  for each congruence class and therefore an infinite amount of memory.
  However, the Knuth-Bendix method is able to answer the question very quickly,
  since it is not restricted to finite objects.
\end{example}

% A specific utility of Knuth-Bendix with pair orbit enumeration (\texttt{kbp}) is
% the ability to return a congruence's \textit{non-trivial congruence classes},
% which in some cases cannot be determined by any of the other algorithms.
% Consider the following example.

% \begin{example}
%   \label{ex:good-kbp}
%   TODO: find a simpler example.

%   Let $S$ be the semigroup defined by the presentation
%   $$\pres{a,b,c}{b^7=b,\ c^5=c,\ bc^2ba=bc^2b,\ bc^2bc=bc^2b,\ b^2cbca=b^2cbc,\ b^2cbcb=b^2cbc}.$$
%   Certainly this semigroup is infinite.  Next let $\rho$ be the right congruence
%   defined by the pair $(ab^2a, a^2bab)$.  It turns out that $\rho$ has only one
%   non-trivial congruence class, which is simply equal to $\{ab^2a, a^2bab\}$,
%   and all other classes are singletons.  Since the total number of congruence
%   classes is infinite, Todd-Coxeter cannot complete.  The \texttt{kbfp} method
%   can certainly rewrite any pair of words to identify whether a pair lies in
%   $\rho$; but only \texttt{kbp} has a way of looking up which elements are not
%   in singletons, and thus returning an answer to the question: \textit{what are
%     the non-trivial congruence classes of $\rho$?}
% \end{example}

Taken together, these examples give a justification for the use of several
different algorithms to solve one question.  Since it may be unknown in advance
which algorithms may perform well or which may return at all, it is useful to be
able to execute all at once and return in whichever run-time is quickest.

\subsection{Comparison on random examples}
A large number of example congruences were generated randomly and run using each
different algorithm, along with the default implementation in GAP \cite{gap}, to
analyse performance.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{pics/ch-pairs/bench-trans-all}
  \caption{Performance of the algorithms on 5000 transformation semigroups}
  \label{fig:bench-trans-all}
\end{figure}

Figure \ref{fig:bench-trans-1p-times} shows the results of a set of 5000 tests
on transformation semigroups.  In each test, $3$ transformations of degree $6$
were chosen at random, and used to generate a semigroup $S$ (any semigroup of
size over $10000$ was rejected).  The elements of $S$ were computed, and a pair
of elements was chosen at random to generate a congruence $\rho$.  Three more
pairs were chosen at random, and each of the different algorithms described in
this chapter was used to determine whether each pair was contained in $\rho$.
Similar tests were also conducted using $2$ and $3$ generating pairs for $\rho$,
as shown in \ref{fig:bench-trans-2p-times} and \ref{fig:bench-trans-3p-times}.
The time taken to return an answer was recorded in each case, and these figures
were compared to one another.  The algorithms used were:
\begin{itemize}
\item Todd-Coxeter (\texttt{tc}),
\item Todd-Coxeter with prefilled table (\texttt{tc\_prefill}),
\item Knuth-Bendix (\texttt{kb}),
\item pair orbit enumeration (\texttt{p}),
\item the parallel method described in this chapter (\texttt{default}),
\item and the method implemented in the library of GAP (\texttt{GAP}).
\end{itemize}

As can be seen in Figure \ref{fig:bench-trans-1p-times}, the prefilled Todd-Coxeter
method is the most likely to complete fastest, with regular Todd-Coxeter winning
in a sizeable minority of cases.  This backs up the anecdotal observations group
theorists have made \cite{havascomparing} that Todd-Coxeter tends to perform
faster than Knuth-Bendix.  We may also observe that pair orbit enumeration
sometimes completes almost instantly, which makes some sense when we consider
how little work the algorithm does when there are very few non-reflexive pairs
in the congruence.  The Knuth-Bendix procedure lags behind badly on these
examples, taking even longer than the built-in methods in GAP.  These results
are a justification for the decision to run only the Todd-Coxeter algorithms in
the case of a finite, concrete semigroup.

Figures \ref{fig:bench-trans-2p-times} and \ref{fig:bench-trans-3p-times} show
that with a higher number of generating pairs, the pair orbit enumeration
algorithm suffers badly---this can be understood, since it generally has to
enumerate more pairs when the generating set is larger.  However, with more
generating pairs \texttt{tc} tends to perform relatively better, since there are
likely to be fewer congruence classes and therefore fewer rows in the coset
table.

This tendency is illustrated further in Figures \ref{fig:bench-trans-1p-tccomp},
\ref{fig:bench-trans-2p-tccomp} and \ref{fig:bench-trans-3p-tccomp}, which
compare the standard Todd-Coxeter algorithm with the prefilled version, arranged
according to whether the congruence in question has many or few classes relative
to its size.  The $x$ axis in these figures is
$$(\text{Number of congruence classes} - 1) / (\text{Size of} $S$ - 1),$$
a scale from $0$ to $1$, where $0$ represents a universal congruence and $1$
represents a trivial congruence.  Since the size of $S$ is a major factor in how
long any algorithm takes to run, only the ratio of \texttt{tc} to
\texttt{tc\_prefill} is shown: a black line is drawn on the graphs to indicate
the length of time taken by \texttt{tc\_prefill}, and a data point is then
plotted for each test, showing how many times as long \texttt{tc} took to
complete.  As can be seen, \texttt{tc} often wins when there are relatively few
large classes, and \texttt{tc_prefill} is much more likely to win when there are
many small classes.  This reinforces the idea given in Examples \ref{ex:good-tc}
and \ref{ex:good-tc-prefill}, that the winning algorithm may depend on number of
classes, and it is therefore not clear in advance which algorithm may be better.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{pics/ch-pairs/bench-trans-1p-gap}
  \caption{Comparison between \texttt{libsemigroups} and the GAP library, for
    transformation semigroups}
  \label{fig:bench-trans-1p-gap}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{pics/ch-pairs/bench-trans-2p-gap}
  \caption{Comparison between \texttt{libsemigroups} and the GAP library, for
    transformation semigroups}
  \label{fig:bench-trans-2p-gap}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{pics/ch-pairs/bench-trans-3p-gap}
  \caption{Comparison between \texttt{libsemigroups} and the GAP library, for
    transformation semigroups}
  \label{fig:bench-trans-3p-gap}
\end{figure}

Figures \ref{fig:bench-trans-1p-gap}, \ref{fig:bench-trans-2p-gap} and
\ref{fig:bench-trans-3p-gap} show data from the same tests, plotted not by
the size of the semigroup, but simply in order of the ratio between GAP's
runtime and that of the default method in \texttt{libsemigroups}.  As can be
seen, the \texttt{libsemigroups} methods run much faster than the GAP methods in
almost all cases, with the GAP methods taking as much as $2800$ times as long
for one generating pair.  Out of $5000$ tests, GAP performed better in only $12$
cases, and these were all tests in which both methods completed in less than
$0.3$ milliseconds.  On tests with two and three generating pairs, GAP was
slower every time.

Further tests were carried out in a similar way, but using finitely presented
semigroups instead of concrete semigroups.  The main difference between these
tests and the ones described previously is that for each transformation
semigroup $S$ which was generated, a finite presentation $\pres X R$ was found
for $S$, and that presentation was used in tests instead of the concrete
semigroup $S$ itself.  This is intended to test which algorithms are effective
when the elements of the semigroup are not known in advance.

In order to produce a further comparison, the tests for finitely presented
semigroups were also run with the \textit{kbmag} package for GAP \cite{kbmag}.
The results are shown on the graphs with the name \texttt{kbmag}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{pics/ch-pairs/bench-fp-all}
  \caption{Performance of the algorithms on $1000$ finitely presented semigroups}
  \label{fig:bench-fp-all}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{pics/ch-pairs/bench-fp-gap}
  \caption{Comparison between \texttt{libsemigroups} and the GAP library, for
    finitely presented semigroups}
  \label{fig:bench-fp-gap}
\end{figure}

As can be seen in Figures \ref{fig:bench-fp-all} and \ref{fig:bench-fp-gap}, the
performance of the GAP library over finitely presented semigroups is far worse
than it was for concrete semigroups, taking as much as $400,000$ times as long
as \texttt{libsemigroups} to complete.  Due to the excessive times GAP took to
complete some tests, the size of semigroups was restricted to $1000$, and only
$1000$ tests were carried out.  In these tests, unlike for concrete semigroups,
Knuth-Bendix tended to outperform GAP, but in
general Todd-Coxeter methods were still faster.  It should be noted, however,
that these were all congruences that were guaranteed in advance to have a finite
number of classes.  An arbitrary congruence on a finitely presented semigroup
may have an infinite number of classes, and there are many examples in which
Knuth-Bendix can return an answer but Todd-Coxeter cannot (see Example
\ref{ex:good-kbfp}).

\clearpage

\section{Future work}

For example:
\begin{itemize}
\item Linking Knuth-Bendix and Todd-Coxeter so they feed into each other;
\item Multiplying concrete elements directly in some places instead of using
  the presentation (beyond prefilling TC);
\item In the GAP library, anything interesting they might do;
\item Adding Knuth-Bendix support for left and right congruences.
\end{itemize}
