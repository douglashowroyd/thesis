\chapter{Introduction}
\label{chap:intro}

% Notation without page references
\nomenclature[N]{$\mathbb{N}$}{Natural numbers $\{1, 2, 3, \ldots\}$\nomnorefpage}
\nomenclature[R]{$\mathbb{R}$}{Real numbers\nomnorefpage}

% TODO: write this properly
Notation:
maps on the right
compose maps left-to-right
$\subseteq$ for subsets possibly including equality
$\subset$ for strict containment
\nomenclature[<=]{$\subseteq$}{Subset containment (including equality)}
\nomenclature[<]{$\subset$}{Strict subset containment (excluding equality)}

$\mathbf{n} = \{1, \ldots, n\}$ for $n \in \mathbb{N}$
\nomenclature[n]{$\mathbf{n}$}{$\{1, 2, \ldots, n\}$ for some $n \in \mathbb{N}$}

$\Delta_S$ and $\nabla_S$
\nomenclature{$\Delta$}{Diagonal relation}
\nomenclature{$\nabla$}{Universal relation}

\section{Basic definitions}
\label{sec:intro-basic}

\begin{definition}
  \label{def:semigroup}
  \index{semigroup}
  \nomenclature[S]{$S$}{Semigroup}
  \nomenclature[.]{$\cdot$}{Binary operation in a semigroup}
  A \textbf{semigroup} is a non-empty set $S$ together with
  a binary operation $\cdot: S \times S \to S$ such that
  $$(x \cdot y) \cdot z = x \cdot (y \cdot z)$$
  for all $x, y, z \in S$.
\end{definition}
Note that some writers leave out the requirement that $S$ must be non-empty,
resulting in the empty semigroup $\varnothing$.  This extra semigroup is of
little theoretical interest, but requires awkward caveats to be added to various
statements that only apply to a non-empty semigroup, adding unnecessary
complication.  Hence, we exclude the empty semigroup from our considerations,
and we will require that all semigroups contain at least one element.  Note also
that the operation symbol $\cdot$ is often omitted where there is no risk of
ambiguity, in the manner of multiplication.

\begin{definition}
  \label{def:monoid}
  \index{monoid}
  \nomenclature[M]{$M$}{Monoid}
  \nomenclature[e]{$e$}{Identity of a monoid}
  A \textbf{monoid} is a semigroup $M$ containing a distinguished element $e$
  such that
  $$ex = xe = x$$
  for all $x \in M$.  The element $e$ is called the \textit{identity} of $M$.
\end{definition}

\begin{definition}
  \label{def:cayley-graph}
  \index{Cayley graph}
  \index{Cayley graph!left}
  \index{Cayley graph!right}
  Let $S$ be a semigroup, with a generating set $X$.  The \textbf{right Cayley
    graph} of $S$ with respect to $X$ is the digraph-with-edge-labels $\Gamma$,
  which is described as follows:
  \begin{itemize}
  \item The vertices of $\Gamma$ are the elements of $S$;
  \item For each pair $(s, x) \in S \times X$ there exists an edge from $s$ to
    $s \cdot x$ labelled by $x$:
    $$s \overset{x}{\longrightarrow} s \cdot x$$
  \end{itemize}
  The \textbf{left Cayley graph} of $S$ with respect to $X$ is defined
  analogously, replacing $s \cdot x$ with $x \cdot s$.
\end{definition}

\section{Generators}
\label{sec:generators}
% TODO: semigroup generators, group generators, inverse semigroup generators,
% normal subgroup generators...

\section{Homomorphisms}
\label{sec:homomorphisms}

\begin{definition}
  \label{def:homomorphism}
  \index{homomorphism}
  \nomenclature{$\phi$}{Homomorphism}
  Let $S$ and $T$ be semigroups.  A semigroup \textbf{homomorphism} is a
  function $\phi: S \to T$ such that
  $$(x)\phi \cdot (y)\phi = (xy)\phi,$$
  for all $x, y \in S$.
\end{definition}

\begin{definition}
  \label{def:monomorphism}
  \index{monomorphism}
  \nomenclature[->]{$\hookrightarrow$}{Monomorphism}
  A semigroup \textbf{monomorphism} is a semigroup homomorphism which is
  injective (one-to-one).  It is indicated on diagrams by a hooked arrow:
  $$S \hookrightarrow T$$
\end{definition}

\begin{definition}
  \label{def:epimorphism}
  \index{epimorphism}
  \nomenclature[->]{$\twoheadrightarrow$}{Epimorphism}
  A semigroup \textbf{epimorphism} is a semigroup homomorphism which is
  surjective (onto).  It is indicated on diagrams by a double-headed arrow:
  $$S \twoheadrightarrow T$$
\end{definition}

Monoid homomorphisms, monomorphisms and epimorphisms are defined analogously,
replacing the word ``semigroup'' with ``monoid''.  If not specified, it is
assumed that ``homomorphism'' refers to a semigroup homomorphism.
% TODO: monoid homos map id to id

\begin{definition}
  \label{def:kernel}
  \index{kernel!of a homomorphism}
  \nomenclature[ker]{$\ker$}{Kernel of a homomorphism}
  The \textbf{kernel} $\ker\phi$ of a homomorphism $\phi:S \to T$ is the
  equivalence relation on $S$ defined by the rule that $(a,b) \in \ker\phi$ if
  and only if
  $$(a)\phi = (b)\phi,$$
  for $a, b \in S$.
\end{definition}

\begin{definition}
  \label{def:image}
  \index{image}
  \nomenclature[im]{$\im$}{Image of a homomorphism}
  The \textbf{image} $\im\phi$ of a homomorphism $\phi:S \to T$ is the set of
  elements $t \in T$ such that
  $$(s)\phi = t$$
  for some $s \in S$.
\end{definition}

\section{Congruences}
\label{sec:intro-congs}

\begin{definition}
  \label{def:compatible}
  \index{compatible}
  \index{compatible!left-}
  \index{compatible!right-}
  \nomenclature[R]{$\R$}{Relation on a semigroup}
  Let $X$ be a semigroup, and let $\mathbf{R}$ be a relation on $S$.  The
  relation $\mathbf{R}$ is:
  \begin{itemize}
  \item \textbf{left-compatible} if $(x, y) \in \mathbf{R}$ implies that
    $(ax, ay) \in \mathbf{R}$ for all $a \in S$;
  \item \textbf{right-compatible} if $(x, y) \in \mathbf{R}$ implies that
    $(xa, ya) \in \mathbf{R}$ for all $a \in S$;
  \item \textbf{compatible} if it is both left-compatible and
    right-compatible.
  \end{itemize}
\end{definition}

\begin{definition}
  \label{def:congruence}
  \index{congruence}
  \index{congruence!left}
  \index{congruence!right}
  \index{congruence!two-sided}
  Let $S$ be a semigroup, and let $\rho$ be an equivalence relation on $S$.  The
  relation $\rho$ is:
  \begin{itemize}
  \item a \textbf{left congruence} if it is left-compatible;
  \item a \textbf{right congruence} if it is right-compatible;
  \item a \textbf{two-sided congruence} if it is compatible.
  \end{itemize}
\end{definition}

When we talk about a \textit{congruence} without specifying that it is left or
right, it is understood to be a two-sided congruence.

\begin{proposition}
  \label{prop:cong-def}
  Let $\rho$ be a congruence on a semigroup $S$.  If $(x, y), (s, t) \in \rho$,
  then $(xs, yt) \in \rho$.
  \begin{proof}
    Since $\rho$ is a left congruence, $xs ~\rho~ xt$, and since it is a right
    congruence, $xt ~\rho~ yt$.  Hence, by transitivity, $xs ~\rho~ yt$, as
    required.
  \end{proof}
\end{proposition}

Congruences have an important property that allows new semigroups, known as
\textit{quotient semigroups}, to be made from old ones.  We will state the
definition of a quotient semigroup, and then show that the definition is well
defined.
% TODO: "well-defined" or "well defined"?

\begin{definition}
  \label{def:quotient}
  \index{quotient semigroup}
  \nomenclature[/]{$/$}{Quotient semigroup}
  Let $S$ be a semigroup, and let $\rho$ be a congruence on $S$.  The
  \textbf{quotient semigroup} $S / \rho$ is the semigroup whose elements are the
  congruence classes of $\rho$, and whose operation $*$ is defined by
  $$[a]_\rho * [b]_\rho = [ab]_\rho,$$ % TODO: define [a]_\rho
  for $a, b \in S$.
\end{definition}

\begin{proposition}
  \label{thm:quotient-well-defined}
  A quotient semigroup is well-defined by Definition \ref{def:quotient}.  That
  is to say, for two $\rho$-classes $A$ and $B$, the value of $A * B$ is the
  same regardless of which representatives are chosen from the two classes.
  \begin{proof}
    Let $S$ and $\rho$ be as in Definition \ref{def:quotient}, and let $A$ and
    $B$ be classes of $\rho$, with elements $a_1,a_2 \in A$ and $b_1,b_2 \in B$.
    The product $A*B$ is defined to be the class which contains the element
    $a_1b_1$, but it is also defined as the class which contains the element
    $a_2b_2$.  For this definition to be consistent, we need to prove that
    $a_1b_1$ and $a_2b_2$ are in the same $\rho$-class.

    To prove this, we observe that $(a_1,a_2) \in \rho$ and
    $(b_1,b_2) \in \rho$.  Hence, by Proposition \ref{prop:cong-def},
    $(a_1b_1, a_2b_2) \in \rho$, so the two representatives are in the same
    class, as required.  Hence $A * B$ is well-defined.
  \end{proof}
\end{proposition}

Note that Definition \ref{def:quotient} does not apply to left and right
congruences, which do not generally satisfy the condition stated in Proposition
\ref{prop:cong-def}.  A quotient semigroup can only be taken using a two-sided
congruence.

\begin{definition}
  \label{def:natural-homomorphism}
  \index{natural homomorphism}
  \nomenclature{$\pi_\rho$}{Natural homomorphism}
  Let $S$ be a semigroup, and let $\rho$ be a congruence on $S$.  The
  \textbf{natural homomorphism} $\pi_\rho: S \to S / \rho$ is the map which
  takes an element of $S$ to its $\rho$-class:
  $$\pi_\rho: x \mapsto [x]_\rho.$$
  It is denoted simply by $\pi$ where there is no risk of ambiguity.
\end{definition}

Congruences have long been an important area of study in semigroup theory.
Perhaps the most important feature of two-sided congruences is that they
determine the homomorphic images of a semigroup, and therefore describe an
important part of a semigroup's structure.  Consider the following theorem.

\begin{theorem}[First isomorphism theorem]
  \label{thm:first-isomorphism}
  Let $S$ and $T$ be semigroups, and let $\phi$ be a homomorphism from $S$ to
  $T$.  Then the kernel of $\phi$ is a congruence on $S$, and the image of
  $\phi$ is isomorphic to the quotient semigroup $S / \ker{\phi}$.
\end{theorem}
%TODO: define phi-bar, and maybe explain how these diagrams work?

\begin{figure}[h]
  \centering
  $
  \begin{tikzcd}
    S \ar[d, two heads, "\pi"'] \ar[r, "\phi"] & T \\
    S / \ker{\phi} \ar[ur, dashed, hook, "\bar\phi"']
  \end{tikzcd}
  $
  \caption{Illustration of Theorem \ref{thm:first-isomorphism}}
  \label{fig:first-isomorphism-theorem}
\end{figure}
These ideas fit closely with the concept of semigroup \textit{presentations},
which we will introduce in Section \ref{sec:intro-presentations}, after the
prerequisite concept of \textit{generating pairs}.

\section{Generating pairs}
\label{sec:intro-gen-pairs}

We now describe a concept key to Chapter \ref{chap:pairs} as well as to
semigroup presentations, that of \textit{generating pairs}.  Much of the
description in this section is adapted from a previous thesis,
\cite{mtorpey_msc}.

\begin{definition}
  \label{def:gen-pairs}
  \index{generating pairs}
  Let $S$ be a semigroup and let $\mathbf{R}$ be a subset of $S \times S$.
  \begin{itemize}
  \item The \textbf{equivalence generated by} $\mathbf{R}$ is the least
    equivalence relation (with respect to containment) which contains
    $\mathbf{R}$ as a subset.
  \item The \textbf{left congruence generated by} $\mathbf{R}$ is the least left
    congruence (with respect to containment) which contains $\mathbf{R}$ as a
    subset.
  \item The \textbf{right congruence generated by} $\mathbf{R}$ is the least
    right congruence (with respect to containment) which contains $\mathbf{R}$
    as a subset.
  \item The \textbf{congruence generated by} $\mathbf{R}$ is the least
    congruence (with respect to containment) which contains $\mathbf{R}$ as a
    subset.
  \end{itemize}
\end{definition}

Chapter \ref{chap:pairs} deals in detail with congruences specified by
generating pairs.  We now present some theory relating to generating pairs, in
order to inform discussions later.

We first need to establish a few definitions.  Let $S$ be a semigroup, and let
$\mathbf{R}$ be a relation on $S$.  We define
$$\mathbf{R}^{-1} = \{(x,y) \in S \times S ~|~ (y,x) \in \mathbf{R}\},$$
so that $\mathbf{R}^{-1}$ is a copy of $\mathbf{R}$ but with the entries in each
pair swapped. \nomenclature[-1]{$^{-1}$}{Relation with pairs swapped}
Next, let $\circ$ be the operation of concatenation, so that for two relations
$\mathbf{R}_1$ and $\mathbf{R}_2$ on $S$,
$$\mathbf{R}_1 \circ \mathbf{R}_2 = \left\{(x,y) \in S \times S ~\middle|~
  \exists z \in S: (x,z) \in \mathbf{R}_1, (z,y) \in \mathbf{R}_2\right\},$$
and for $n \in \mathbb{N}$ let
$$\mathbf{R}^n = \underbrace{\mathbf{R} \circ \dots \circ \mathbf{R}}_{n~\text{times}}.$$

\begin{definition}
  \label{def:transitiveclosure}
  \index{transitive closure}
  \nomenclature[\infty]{$^\infty$}{Transitive closure of a relation}
  The \textbf{transitive closure} $\mathbf{R}^\infty$ of a relation $\mathbf{R}$
  is the relation given by
  $$\mathbf{R}^\infty=\bigcup_{n \in \mathbb{N}}\mathbf{R}^n$$
\end{definition}

The transitive closure $\mathbf{R}^\infty$ of $\mathbf{R}$ is the least
transitive relation on $S$ containing $\mathbf{R}$ \cite[Lemma
2.3]{mtorpey_msc}.  This allows us to give a useful description of the
equivalence relation generated by $\mathbf{R}$.

\begin{definition}
  \label{def:re}
  \nomenclature[e]{$^e$}{Least equivalence containing a relation}
  For a relation $\mathbf{R}$ on a semigroup $S$, we define $\mathbf{R}^e$ as
  the relation $\left(\mathbf{R} \cup \mathbf{R}^{-1} \cup \Delta_S\right)^\infty$.
\end{definition}

\begin{lemma}
  \label{lem:re}
  The relation $\mathbf{R}^e$ is the smallest equivalence on $S$ that contains
  $\mathbf{R}$ as a subset.
  \begin{proof}
    Clearly $\mathbf{R} \subseteq \mathbf{R}^e$.

    We will first prove that $\mathbf{R}^e$ is an equivalence relation, and then
    go on to prove that there is no smaller equivalence relation containing
    $\mathbf{R}$.

    Let $\mathbf{Q} = \mathbf{R} \cup \mathbf{R}^{-1} \cup \Delta_S$, so that
    $\mathbf{R}^e = \mathbf{Q}^\infty$.  Since $\Delta_S$ contains all the
    pairs necessary for reflexivity, we know that $\mathbf{Q}$ is reflexive,
    and therefore $\mathbf{Q}^\infty$ is reflexive and transitive.

    To show symmetry, observe that $(x,y) \in \mathbf{R}$ if and only if $(y,x)
    \in \mathbf{R}^{-1}$, and that $(x,y) \in \Delta_S$ if and only if $x=y$.
    $\mathbf{Q}$ is therefore certainly symmetric, and
    $$\mathbf{Q}^n = (\mathbf{Q}^{-1})^n = (\mathbf{Q}^n)^{-1},$$
    and so $\mathbf{Q}^n$ is symmetric.

    Now let $(x,y) \in \mathbf{R}^e$.  For some $n \in \mathbb{N}$, we have
    $(x,y) \in \mathbf{Q}^n$.  By the symmetry of $\mathbf{Q}^n$,
    $$(y,x) \in \mathbf{Q}^n \subseteq \mathbf{Q}^\infty = \mathbf{R}^e,$$
    and so $\mathbf{R}^e$ is symmetric.  Hence $\mathbf{R}^e$ is an equivalence.

    Now to show that $\mathbf{R}^e$ is the \textit{least} such equivalence,
    consider any equivalence $\mathbf{E}$ on $S$ such that $\mathbf{R} \subseteq
    \mathbf{E}$.  Since $\mathbf{E}$ is reflexive, we know that $\Delta_S
    \subseteq \mathbf{E}$, and since $\mathbf{E}$ is symmetric and contains
    $\mathbf{R}$, we know that $\mathbf{R}^{-1} \subseteq \mathbf{E}$.  Hence
    $$\mathbf{Q}=\mathbf{R}\cup\mathbf{R}^{-1}\cup\Delta_S \subseteq \mathbf{E}.$$
    Finally, since $\mathbf{E}$ is transitive and contains $\mathbf{Q}$, we know
    that $\mathbf{Q}^\infty \subseteq \mathbf{E}$.  Hence $\mathbf{R}^e$ is
    contained in $\mathbf{E}$, and so is no larger than any equivalence on $S$.
  \end{proof}
\end{lemma}

\begin{definition}
  \label{def:rc}
  For a relation $\mathbf{R}$ on a semigroup $S$, we define three relations:
  \begin{enumerate}[\rm(1)]
  \item
    \nomenclature[c]{$^c$}{Least compatible relation containing a relation}
    $\mathbf{R}^c = \left\{(xay, xby) ~\middle|~ (a,b) \in \mathbf{R},~x,y \in
      S^1\right\}$.
  \item
    \nomenclature[l]{$^l$}{Least left-compatible relation containing a relation}
    $\mathbf{R}^l = \left\{(xa, xb) ~\middle|~ (a,b) \in \mathbf{R},~x \in
      S^1\right\}$.
  \item
    \nomenclature[r]{$^r$}{Least right-compatible relation containing a relation}
    $\mathbf{R}^r = \left\{(ay, by) ~\middle|~ (a,b) \in \mathbf{R},~y \in
      S^1\right\}$.
  \end{enumerate}
\end{definition}

\begin{lemma}
  \label{lem:rc}
  Let $\mathbf{R}$ be a relation on a semigroup $S$.
  The following hold:
  \begin{enumerate}[\rm(1)]
  \item $\mathbf{R}^c$ is the smallest compatible relation on $S$ containing
    $\mathbf{R}$.
  \item $\mathbf{R}^l$ is the smallest left-compatible relation on $S$
    containing $\mathbf{R}$.
  \item $\mathbf{R}^r$ is the smallest right-compatible relation on $S$
    containing $\mathbf{R}$.
  \end{enumerate}
  \begin{proof}
    We prove the statement for $\mathbf{R}^c$, and note that the proofs for
    $\mathbf{R}^l$ and $\mathbf{R}^r$ are very similar.
    $\mathbf{R}^c$ certainly contains $\mathbf{R}$ -- all the elements of
    $\mathbf{R}$ are encountered in the case that $x=y=1$.

    Let us show first that $\mathbf{R}^c$ is compatible.
    Let $(u,v) \in \mathbf{R}^c$ and let $w \in S$.  Now there must exist $a,b
    \in S$ and $x,y \in S^1$ such that $u=xay$, $v=xby$, and $(a,b) \in
    \mathbf{R}$.  Hence
    $wu = wx \cdot a \cdot y$ and
    $wv = wx \cdot b \cdot y$, and $wx \in S^1$,
    so $(wu,wv) \in \mathbf{R}^c$ and $\mathbf{R}^c$ is left-compatible.
    Similarly,
    $uw = x \cdot a \cdot yw$ and
    $vw = x \cdot b \cdot yw$, and $yw \in S^1$,
    so $(uw,vw) \in \mathbf{R}^c$ and $\mathbf{R}^c$ is right-compatible.

    Finally we need to show that there is no compatible relation smaller than
    $\mathbf{R}^c$ which contains $\mathbf{R}$.  For this purpose, let
    $\mathbf{C}$ be a compatible relation on $S$ such that $\mathbf{R} \subseteq
    \mathbf{C}$.  Now for any $(a,b) \in \mathbf{R}$ and $x,y \in S^1$, we must
    have $(xay,xby) \in \mathbf{C}$ by the definition of compatibility.  Every
    element of $\mathbf{R}^c$ has this form, hence $\mathbf{R}^c \subseteq
    \mathbf{C}$. \cite[p.26]{howie}
  \end{proof}
\end{lemma}

These three relations $\mathbf{R}^c$, $\mathbf{R}^l$ and $\mathbf{R}^r$ have
some properties which will be useful later.  These properties make up the
following lemmas.

\begin{lemma}
  \label{lem:cinverse}
  Let $\mathbf{R}$ be a relation on a semigroup $S$.
  The following hold:
  \begin{enumerate}[\rm(1)]
  \item $(\mathbf{R}^{-1})^c = (\mathbf{R}^c)^{-1}$
  \item $(\mathbf{R}^{-1})^l = (\mathbf{R}^l)^{-1}$
  \item $(\mathbf{R}^{-1})^r = (\mathbf{R}^r)^{-1}$
  \end{enumerate}
  \begin{proof}
    Let $\mathbf{R}$ be a relation on a semigroup $S$.
    $\mathbf{R}^{-1} = \{(a,b)~|~(b,a)\in\mathbf{R}\}$, so
    $$(\mathbf{R}^{-1})^c = \{(xay,xby)~|~x,y \in S^1, (b,a)\in\mathbf{R}\}.$$
    The inverse of this last expression is
    $$\{(xay,xby)~|~x,y \in S^1, (a,b)\in\mathbf{R}\},$$
    which is equal to $\mathbf{R}^c$.
    Now $\big((\mathbf{R}^{-1})^c\big)^{-1} = \mathbf{R}^c$, which is equivalent
    to what we wanted.

    A similar argument holds for both $\mathbf{R}^l$ and $\mathbf{R}^r$
  \end{proof}
\end{lemma}

\begin{lemma}
  \label{lem:csubset}
  Let $\mathbf{A}$ and $\mathbf{B}$ be relations on a semigroup $S$.  If
  $\mathbf{A} \subseteq \mathbf{B}$, then $\mathbf{A}^c \subseteq \mathbf{B}^c$,
  $\mathbf{A}^l \subseteq \mathbf{B}^l$, and
  $\mathbf{A}^r \subseteq \mathbf{B}^r$.
  \begin{proof}
    Let $\mathbf{A} \subseteq \mathbf{B}$, and let $(xay,xby)$ be an arbitrary
    element of $\mathbf{A}^c$ where $(a,b) \in \mathbf{A}$ and $x,y \in S^1$.
    Since $\mathbf{A} \subseteq \mathbf{B}$, we have that $(a,b) \in
    \mathbf{B}$, and hence also that $(xay,xby) \in \mathbf{B}^c$.

    A similar statement holds for $\mathbf{A}^l \subseteq \mathbf{B}^l$ and
    $\mathbf{A}^r \subseteq \mathbf{B}^r$.
  \end{proof}
\end{lemma}

\begin{lemma}
  \label{lem:compatn}
  If $\mathbf{R}$ is a (left/right) compatible relation, then $\mathbf{R}^n$ is
  also (left/right) compatible, for all $n \in \mathbb{N}$.
  \begin{proof}
    Let $\mathbf{R}$ be a compatible relation on a semigroup $S$,
    and let $n \in \mathbb{N}$.
    Now let $(a,b) \in \mathbf{R}^n$, and $x \in S$.  Hence there exist
    $c_1, c_2, \dots, c_n, c_{n+1} \in S$ such that $a=c_1$, $b=c_{n+1}$,
    and $$(c_1, c_2), (c_2, c_3), \dots, (c_n, c_{n+1}) \in \mathbf{R}.$$
    Since $\mathbf{R}$ is left-compatible,
    $$(xc_1, xc_2), (xc_2, xc_3), \dots, (xc_n, xc_{n+1}) \in \mathbf{R},$$
    and since $\mathbf{R}$ is right-compatible,
    $$(c_1x, c_2x), (c_2x, c_3x), \dots, (c_nx, c_{n+1}x) \in \mathbf{R},$$
    and therefore
    $(xa,xb) \in \mathbf{R}^n$ and $(ax, bx) \in \mathbf{R}^n,$
    so $\mathbf{R}^n$ is compatible. \cite[p.26]{howie}
    A similar argument holds for left and right compatibility.
  \end{proof}
\end{lemma}

These lemmas now enable us to give a theorem characterising the congruence, left
congruence, and right congruence generated by $\mathbf{R}$.

\begin{theorem}
  \label{thm:rsharp}
  Let $\mathbf{R}$ be a relation on a semigroup $S$.
  The following hold:
  \begin{enumerate}[\rm(1)]
  \item $\mathbf{R}^\sharp$, the least congruence on $S$ which contains
    $\mathbf{R}$, is equal to $(\mathbf{R}^c)^e$;
  \item $\mathbf{R}^\triangleleft$, the least left congruence on $S$ which
    contains $\mathbf{R}$, is equal to $(\mathbf{R}^l)^e$;
  \item $\mathbf{R}^\triangleright$, the least right congruence on $S$ which
    contains $\mathbf{R}$, is equal to $(\mathbf{R}^r)^e$.
  \end{enumerate}
  \begin{proof}
    Since $\mathbf{R}^c$ is a relation, it follows from Lemma \ref{lem:re} that
    $(\mathbf{R}^c)^e$ is an equivalence, and it certainly contains
    $\mathbf{R}$.  To show that it is a congruence, we now only need to show
    that it is compatible:

    By Definition \ref{def:re}, $(\mathbf{R}^c)^e = \mathbf{Q}^\infty$, where
    $$\mathbf{Q} =
    \mathbf{R}^c \cup
    (\mathbf{R}^c)^{-1} \cup
    \Delta_S.$$
    Lemma \ref{lem:cinverse} gives us that
    $(\mathbf{R}^c)^{-1} = (\mathbf{R}^{-1})^c$, and
    we know
    $\Delta_S = {\Delta_S}^c$, so
    $$\mathbf{Q} =
    \mathbf{R}^c \cup
    (\mathbf{R}^{-1})^c \cup
    {\Delta_S}^c,$$
    and finally applying Lemma \ref{lem:csubset} gives us
    $$\mathbf{Q} =
    (\mathbf{R} \cup \mathbf{R}^{-1} \cup \Delta_S)^c.$$
    Hence by Lemma \ref{lem:rc}, $\mathbf{Q}$ is a compatible relation.

    Let $a \in S$ and let $(x,y) \in (\mathbf{R}^c)^e = \mathbf{Q}^\infty$.
    By Definition \ref{def:transitiveclosure}, $(x,y) \in \mathbf{Q}^n$ for some
    $n \in \mathbb{N}$, and by Lemma \ref{lem:compatn} we know that
    $\mathbf{Q}^n$ is compatible.  Hence
    $$(ax,ay), (xa, ya) \in \mathbf{Q}^n \subseteq \mathbf{Q}^\infty =
    (\mathbf{R}^c)^e,$$ and so $(\mathbf{R}^c)^e$ is a congruence.

    All that remains is to show that there is no congruence containing
    $\mathbf{R}$ which is smaller than $(\mathbf{R}^c)^e$.
    Let $\rho$ be a congruence containing $\mathbf{R}$.  Since $\rho$ is
    compatible, $\rho^c = \rho$ by Lemma \ref{lem:rc}; and since $\mathbf{R}
    \subseteq \rho$, by Lemma \ref{lem:csubset}, $\mathbf{R}^c \subseteq
    \rho^c$.  So we have
    $$\mathbf{R}^c \subseteq \rho.$$
    Finally, since $\rho$ is an equivalence containing $\mathbf{R}^c$, we know
    from Lemma \ref{lem:re} that $(\mathbf{R}^c)^e \subseteq \rho$, so
    $(\mathbf{R}^c)^e$ is the smallest congruence on $S$ containing
    $\mathbf{R}$. \cite[p.26-27]{howie}

    A similar argument holds for $\mathbf{R}^\triangleleft$ and
    $\mathbf{R}^\triangleright$.
  \end{proof}
\end{theorem}

We will make one final definition, which can be seen as the opposite of the
$\sharp$ (sharp) operator, hence the musical notation $\flat$ (flat).

\begin{definition}
  \label{def:e-flat}
  Let $\mathbf{E}$ be an equivalence on a semigroup $S$.  We denote by
  $\mathbf{E}^\flat$ the greatest congruence contained in $\mathbf{E}$.
\end{definition}

Note that the above definition is not well defined for a generic relation
$\mathbf{R}$, but only for an equivalence $\mathbf{E}$.  This is because
$\mathbf{R}$ is not guaranteed to contain any congruence at all, whereas
$\mathbf{E}$ must always contain the trivial congruence $\Delta_S$.

\section{Presentations}
\label{sec:intro-presentations}

We have now encountered two important concepts for congruences---a congruence
can be defined by generating pairs, and a quotient semigroup is defined by a
congruence.  We can combine these two concepts to give a very general way of
describing semigroups: presentations.  First we require the definition of a
\textit{free semigroup}.

\begin{definition}
  \label{def:free}
  \index{free!semigroup}
  \index{free!monoid}
  Let $X$ be a set.  The \textbf{free monoid} over $X$ is denoted by $X^*$, and
  consists of all finite sequences of elements in $X$, with the operation of
  concatenation.  The \textbf{free semigroup} $X^+$ is the subsemigroup of $X^*$
  consisting of sequences of length at least $1$.
\end{definition}

When we consider free semigroups and monoids, the set $X$ is usually referred to
as an \textit{alphabet}, its elements as \textit{letters}, and sequences of
letters as \textit{words}.

The justification for the use of the name ``free'' comes from category theory.
We can see that the free semigroup $X^+$ has the following property, an
alternative formulation of ``free'' given in \cite[\S 1.6]{howie}:

\begin{proposition}
  \label{prop:free}
  Let $X$ be a set.  The following hold:
  \begin{enumerate}[\rm(1)]
  \item there is a map $\alpha: X \to X^+$;
  \item for every semigroup $S$ and every map $\phi: X \to S$ there exists a
    unique homomorphism $\psi: X^+ \to S$ such that $\phi = \alpha\psi$.
  \end{enumerate}
  \begin{figure}[h]
    \centering
    $
    \begin{tikzcd}
      X \ar[d, "\phi"'] \ar[r, "\alpha"]
      & X^+ \ar[ld, dashed, "\exists ! \psi"] \\ S
    \end{tikzcd}
    $
    \caption{Commutative diagram illustrating Proposition \ref{prop:free}}
    \label{fig:free}
  \end{figure}
  \begin{proof}
    We can choose $\alpha$ to be the obvious embedding which takes a letter $x$
    in $X$ to the corresponding word $x$ of length $1$ in $X^+$.  Then, for any
    $S$ and $\phi: X \to S$ we can define $\psi: X^+ \to S$ by
    $$(x_1 x_2 \ldots x_n)\psi = (x_1)\phi (x_2)\phi \ldots (x_n)\phi,$$
    for $x_1, x_2, \ldots, x_n \in X$.  This clearly satisfies
    $\phi = \alpha\psi$, and it is certainly a homomorphism, since
    \begin{align*}
      (x_1 \ldots x_n)\psi \cdot (y_1 \ldots y_m)\psi
      & = (x_1)\phi\ldots(x_n)\phi \cdot (y_1)\phi\ldots(y_m)\phi \\
      & = (x_1 \ldots x_ny_1 \ldots y_m)\psi
    \end{align*}
    for $x_1, \ldots, x_n, y_1, \ldots, y_m \in X$.  For uniqueness, let $\psi'$
    be any homomorphism $X^+ \to S$ such that $\phi = \alpha\psi'$.  By this
    condition, we have
    $$(x_1)\phi = (x_1)\alpha\psi' = (x_1)\psi',$$
    for any $x_1 \in X$.  And since $\psi'$ is a homomorphism, this gives us
    $$(x_1x_2\ldots x_n)\psi'
    = (x_1)\psi'(x_2)\psi'\ldots(x_n)\psi'
    = (x_1)\phi (x_2)\phi \ldots(x_n)\phi,$$
    for $x_1, x_2, \ldots, x_n \in X$.
    This shows that $\psi' = \psi$, and so $\psi$ is unique.
  \end{proof}
\end{proposition}

We can now define semigroup presentations, a useful method of describing a
semigroup which will be encountered many times in this thesis, particularly in
Chapter \ref{chap:pairs}.  We will give the definition, and then discuss how the
definition is used to describe a semigroup.

\begin{definition}
  \label{def:presentation}
  \index{presentation}
  A \textbf{semigroup presentation} is a pair $\mathfrak{P} = \pres X R$
  consisting of a set $X$ and a set of pairs $R \subseteq X^+ \times X^+$.  The
  semigroup \textbf{defined by} the presentation $\mathfrak{P}$ is
  $X^+ / R^\sharp$, i.e.~the quotient of the free semigroup $X^+$ by the least
  congruence containing all the pairs in $R$.
\end{definition}

\begin{definition}
  \label{def:finite-presentation}
  \index{presentation!finite}
  A semigroup presentation $\pres X R$ is \textbf{finite} if $X$ and $R$ are
  finite.  A semigroup is \textbf{finitely presented} if it is defined by a
  finite presentation.
\end{definition}

The exact wording used to talk about a semigroup $S$ defined by a presentation
$\pres X R$ varies.  Some sources view $\pres X R$ as a semigroup in its own
right: they might describe $S$ as ``isomorphic to'' $\pres X R$, or they might
even say $S$ ``equals'' $\pres X R$.  We will opt for the more careful language
which separates a semigroup from its presentation: $S$ is \textbf{defined by} \index{define} or
\textbf{presented by} \index{present} $\pres X R$, if and only if it is isomorphic to
$X^+ / R^\sharp$.

If $S$ is presented by $\pres X R$, there is an epimorphism from $X^+$ to $S$:
if $\pi$ is the natural homomorphism from $X^+$ to the quotient semigroup
$X^+ / R^\sharp$ (see Definition \ref{def:natural-homomorphism}) and $\iota$ is
an isomorphism from $X^+ / R^\sharp$ to $S$, then $\pi\iota$ is an epimorphism
from $X^+$ to $S$, which assigns each word in the generators to an element of
the semigroup $S$.  If $w \in X^+$ and $s \in S$ are such that
$(w)\pi\iota = s$, then we say that the word $w$ \textbf{represents} \index{represent} the element
$s$, or that the element $s$ can be \textbf{factorised} \index{factorise} to the word $w$.

\begin{figure}[h]
  \centering
  $
  \begin{tikzcd}
    X^+ \ar[r, two heads, "\pi"] &
    \frac{X^+}{R^\sharp} \ar[r, two heads, hook, "\iota"] &
    S
  \end{tikzcd}
  $
  \caption[How a word represents a finitely presented semigroup element]{How a
    word from $X^+$ represents an element in $S$}
  \label{fig:word-represents-element}
\end{figure}

Semigroups are not uniquely defined by presentations: two different
presentations may define the same semigroup, and those two presentations may not
look similar at all.  Consider the following example.

\begin{example}
  The semigroup presentation $\pres{a}{a=a^{10}}$ defines the cyclic group
  $C_9$: there are $9$ elements which can be represented by the words
  $$\{a, a^2, a^3, a^4, a^5, a^6, a^7, a^8, a^9\}.$$  However, $C_9$ is also
  presented by $\pres{b,c}{b=c^3,\ bc=cb,\ c=c^2b^2c^2}$, and an equivalence
  between the two presentations can be defined by identifying $a$ with $c$.
  This second presentation, though it is more complicated to describe, allows us
  to use shorter words to describe many elements: the elements of $C_9$ are
  represented by the set of words $\{c, c^2, b, bc, bc^2, b^2, b^2c, b^2c^2\}$.
\end{example}

\section{Element types}
\label{sec:element-types}

\subsection{Transformations}
\label{sec:transformations}

\begin{definition}
  \label{def:tn}
  \index{full transformation monoid}
  \nomenclature[Tn]{$\T_n$}{Full transformation monoid}
  Full transformation monoid $\T_n$.
\end{definition}

\index{symmetric inverse monoid}
\nomenclature[In]{$\I_n$}{Symmetric inverse monoid}

\index{symmetric group}
\nomenclature[Sn]{$\Sym_n$}{Symmetric group}
% include partial transformations.  Sn, Tn, In, PTn

\subsection{Bipartitions}
\label{sec:bipartitions}

Another important type of element discussed in this thesis is a
\textit{bipartition} (sometimes referred to just as a \textit{partition}).
Bipartitions are formally defined as a class of equivalence relations that form
semigroups under an interesting composition operation; but we will often
understand them in a graphical way, as in Figure \ref{fig:bipartition-example}.
Bipartition semigroups are thus included in the class of \textit{diagram
  semigroups} \cite{diagram_semigroups}.  The study of bipartition semigroups is
born out of the study of diagram algebras, for example Temperley-Lieb algebras
and the partition algebra \cite{partition_algebra}.  However, it is of
independent interest in semigroup theory, being an example of a regular
$\star$-semigroup (discussed later), and containing copies of important algebras
such as $\Sym_n$, $\T_n$ and $\I_n$, along with other interesting features
\cite[\S1]{deg_motzkin}.

We begin with the formal definition.

\begin{definition}
  \label{def:bipartition}
  \index{bipartition}
  A \textbf{bipartition} is an equivalence relation on the set
  $\mathbf{n} \cup \mathbf{n}'$, where $\mathbf{n} = \{1, \ldots, n\}$ and
  $\mathbf{n}' = \{1', \ldots, n'\}$ for some $n \in \mathbb{N}$.
  \nomenclature[n']{$\mathbf{n}'$}{$\{1', 2', \ldots, n'\}$ for some
    $n \in \mathbb{N}$}
\end{definition}

The equivalence classes of a bipartition are called \textbf{blocks}.  A block is
called an \textbf{upper block} if it only contains points from $\mathbf{n}$, a
\textbf{lower block} if it only contains points from $\mathbf{n}'$, or a
\textbf{transversal} if it contains points from both $\mathbf{n}$ and
$\mathbf{n}'$.
\index{block}\index{block!upper}\index{block!lower}\index{transversal}

The number $n$ is called the \textit{degree} of the
bipartition. \index{degree!of a bipartition}  Two
bipartitions $\alpha$ and $\beta$ of the same degree can be composed in the
following way to make another bipartition, $\alpha \beta$: let
$\mathbf{n}'' = \{1'', \ldots, n''\}$, let $\alpha^\vee$ be obtained from
$\alpha$ by changing every point $i \in \mathbf{n}$ to $i''$, and let
$\beta^\wedge$ be obtained from $\beta$ be changing every point
$i' \in \mathbf{n}'$ to $i''$.  Now let $\Pi$ be the equivalence on
$\mathbf{n} \cup \mathbf{n}' \cup \mathbf{n}''$ given by
$(\alpha^\vee \cup \beta^\wedge)^e$.  We define $\alpha \beta$ as the
bipartition
$\Pi \cap \big((\mathbf{n} \cup \mathbf{n}') \times (\mathbf{n} \cup
\mathbf{n}')\big)$.  This composition is more easily understood visually, as
will be seen in Example \ref{ex:bipartition}.  The operation can be seen to be
associative, and so we can use it to form semigroups of bipartitions.

A bipartition can be displayed visually by plotting the points $1$ to $n$ in
order in a horizontal line, with the corresponding points $1'$ to $n'$
underneath, and drawing edges between points to make a spanning skeleton of the
equivalence relation.  The product of two bipartitions can then be found by
concatenating the two diagrams top-to-bottom and deleting points in the middle
line.  Consider the following example.

\begin{example}
  \label{ex:bipartition}
  Let $\alpha$ be the bipartition of degree $5$ with blocks $\{1, 2', 3'\}$,
  $\{2\}$, $\{3, 4, 3', 4'\}$, $\{5\}$ and $\{5'\}$.  Let $\beta$ be the
  bipartition of degree $5$ with blocks $\{1, 2, 5, 1', 2'\}$, $\{3, 4', 5'\}$,
  $\{4\}$ and $\{3'\}$.  The diagrams of these two figures, along with their
  product $\alpha \beta$, are shown in Figure \ref{fig:bipartition-example}.
  Note that two different diagrams are shown for $\alpha \beta$.
  \begin{figure}[h]
    \centering
    $$\alpha = \bipartdiag{\tv11\bc12 \tc34 \tv33\tc34\bc34} \qquad
    \beta = \bipartdiag{\tc12\tc25\tv22\bc12 \tv34\bc45}$$
    $$\alpha \beta = \doublebipartdiag{\utv11\ubc12 \utc34 \utv33\utc34\ubc34
      \tc12\tc25\tv22\bc12 \tv34\bc45}
    = \bipartdiag{\tv12\bc12 \tc34\tv34\bc45}
    = \bipartdiag{\tv11\bc12 \tc34\tv44\bc45}$$
    \caption{Diagrams of the bipartitions in Example \ref{ex:bipartition}}
    \label{fig:bipartition-example}
  \end{figure}
\end{example}

Since a given equivalence may have several spanning skeletons, a bipartition may
be represented by several different diagrams; for example, note the two
different representations of $\alpha \beta$ in Figure
\ref{fig:bipartition-example}.  In general, it does not matter which diagram is
used, only that it represents the appropriate bipartition---we will usually
choose the diagram which illustrates the bipartition most clearly.  In
particular, for each block that contains points from both $\mathbf{n}$ and
$\mathbf{n}'$, we will usually draw only one line crossing the diagram from top
to bottom.

Next we define some attributes of bipartitions, which will be important when we
come to consider certain bipartition semigroups later.

\begin{definition}
  Let $\alpha$ be a bipartition.
  \begin{itemize}
  \item The \textbf{rank} of $\alpha$, denoted $\rank \alpha$, is the number of
    transversals in $\alpha$;
    \index{rank!of a bipartition}
  \item The \textbf{domain} of $\alpha$, denoted $\dom \alpha$, is the set of
    points $i \in \mathbf{n}$ such that $i$ lies in a transversal of $\alpha$;
    % $\{i \in \mathbf{n} : i \text{~lies in a transversal of~} \alpha\}$;
    \index{domain!of a bipartition}
    \nomenclature[dom]{$\dom$}{Domain of a bipartition}
  \item The \textbf{codomain} of $\alpha$, denoted $\codom \alpha$, is the set
    of points $i \in \mathbf{n}$ such that $i'$ lies in a transversal of
    $\alpha$;
    % $\{i \in \mathbf{n} : i' \text{~lies in a transversal of~} \alpha\}$;
    \index{codomain}
    \nomenclature[codom]{$\codom$}{Codomain of a bipartition}
  \item The \textbf{kernel} of $\alpha$, denoted $\ker \alpha$, is equal to
    $\alpha \cap (\mathbf{n} \times \mathbf{n})$;
    \index{kernel!of a bipartition}
    \nomenclature[ker]{$\ker$}{Kernel of a bipartition}
  \item The \textbf{cokernel} of $\alpha$, denoted $\coker \alpha$, is equal to
    $\alpha \cap (\mathbf{n}' \times \mathbf{n}')$.
    \index{cokernel}
    \nomenclature[coker]{$\coker$}{Cokernel of a bipartition}
  \end{itemize}
\end{definition}

We illustrate these attributes by continuing our example.

\begin{example}
  \label{ex:codomain}
  Let $\alpha$ and $\beta$ be the bipartitions described in Example
  \ref{ex:bipartition}.  The attributes of $\alpha$ are
  $$\dom \alpha = \{1,3,4\}, \qquad
  \ker \alpha = \big\{\{1\}, \{2\}, \{3,4\}, \{5\}\big\},$$
  $$\codom \alpha = \{1,2,3,4\}, \qquad
  \coker \alpha = \big\{\{1, 2\}, \{3,4\}, \{5\}\big\},$$
  and the attributes of $\beta$ are
  $$\dom \beta = \{1,2,3,5\}, \qquad
  \ker \beta = \big\{\{1,2,5\}, \{3\}, \{4\}\big\},$$
  $$\codom \beta = \{1,2,4,5\}, \qquad
  \coker \beta = \big\{\{1,2\}, \{3\}, \{4,5\}\big\},$$
  where a kernel or cokernel is identified with the set of its blocks.
  We also have $\rank \alpha = \rank \beta = 2$.
\end{example}

Since drawing a diagram for a bipartition consumes a lot of space, and since
writing out the blocks is unwieldy and difficult to read, we have another way of
representing a bipartition: a modified two-row notation.  In this notation, the
blocks of the bipartition's kernel are written across the top row, separated by
vertical lines, and the blocks of the cokernel are written across the bottom
row, omitting prime symbols.  Transversals are written first, with the
appropriate kernel block written above its corresponding cokernel block.
Non-transversal blocks are written afterwards, with upper and lower blocks
separated by horizontal lines.  A generic bipartition could thus be represented
by
$$\alpha = \bipart{c|c|c|c|c|c}{4-6}
{A_1 & \ldots & A_q & C_1 & \ldots & C_r}
{B_1 & \ldots & B_q & D_1 & \ldots & D_s},$$
where $\alpha$ has $q$ transversals of the form $A_i \cup B_i'$ with
$A_i \subseteq \mathbf{n}$ and $B_i' \subseteq \mathbf{n}'$, $r$ upper blocks
labelled $C_i$, and $s$ lower blocks labelled $D_i'$.

\begin{example}
  \label{ex:bipartition-two-row}
  The bipartitions from Example \ref{ex:bipartition} can be written in the form
  $$\alpha = \bipart{c|c|c|c}{3-4}
  {1 & 3,4 & 2 & 5} {1,2 & 3,4 & \mc{2}{c}{5}}, \qquad
  \beta = \bipart{c|c|c}{3-3}{1,2,5 & 3 & 4}{1,2 & 4,5 & 3}.$$
\end{example}

We should also mention the $^\star$ operation.
\nomenclature[*]{$^\star$}{Operation on bipartitions} To each bipartition
$\alpha$ is assigned another bipartition $\alpha^\star$, which is found by
swapping each point $i \in \bn$ with its opposite point $i'$.  Hence if
$\alpha = \bipart{c|c|c|c|c|c}{4-6}
{A_1 & \ldots & A_q & C_1 & \ldots & C_r}
{B_1 & \ldots & B_q & D_1 & \ldots & D_s},$ then we have
$\alpha^\star = \bipart{c|c|c|c|c|c}{4-6}
{B_1 & \ldots & B_q & D_1 & \ldots & D_s}
{A_1 & \ldots & A_q & C_1 & \ldots & C_r}$.
This operation has the property that $\alpha \alpha^\star \alpha = \alpha$, and
that $(\alpha^\star)^\star = \alpha$.  Hence $\alpha^\star$ is a semigroup
inverse for $\alpha$.

We now consider the semigroup of all bipartitions of a given degree.

\begin{definition}
  \label{def:pn}
  \index{bipartition!monoid}
  \nomenclature[Pn]{$\Prt_n$}{Bipartition monoid}
  The \textbf{bipartition monoid} $\Prt_n$ is the semigroup of all bipartitions
  of degree $n$ under composition, where $n \in \mathbb{N}$.
\end{definition}

The bipartition monoid has many interesting features.  First of all, a number of
other important semigroups embed into $\Prt_n$ as subsemigroups.  For example,
consider the following way of embedding the full transformation monoid $\T_n$,
the symmetric inverse monoid $\I_n$, and the symmetric group $\Sym_n$.

\begin{example}
  \label{ex:ptn-to-pn}
  Let $f : \PT_n \to \Prt_n$ be the map that sends a partial transformation
  $\tau$ to the bipartition
  $\left\{\big(i, (i\tau)'\big) : i \in \dom \tau\right\}^e$.  For example, the
  partial transformation $\transV3-43-$ would be mapped to the bipartition
  $\bipart{c|c|c|c|c}{3-5}{1,4 & 3 & \two{2}{5}}{3 & 4 & 1 & 2 & 5}$.  It would
  be easy to mistake $f$ for a homomorphism; however, consider the following
  minimal counter-example:
  $$a = \transII11, \quad b = \transII--,\quad ab = \transII--,$$
  $$(a)f = \bipart{c|c}{2-2}{1,2 &}{1 & 2},\quad
  (b)f = \bipart{c|c}{1-2}{1 & 2}{1 & 2},$$
  $$(a)f(b)f = \bipart{c|c}{1-2}{\mc2{c}{1,2}}{1 & 2} \neq
  \bipart{c|c}{1-2}{1 & 2}{1 & 2} = (ab)f,$$
  showing that $f$ does not respect composition.  Still, $f$ is useful as an
  embedding for several submonoids of $\PT_n$.  The restricted maps $f|_{\T_n}$,
  $f|_{\I_n}$ and $f|_{\Sym_n}$ are all monomorphisms, and thus $\Prt_n$
  contains copies of $\T_n$, $\I_n$ and $\Sym_n$.
\end{example}

Note that, since each bipartition $\alpha \in \Prt_n$ has an inverse
$\alpha^\star \in \Prt_n$, we have that $\Prt_n$ is a regular semigroup.
Furthermore, it is a regular $\star$-semigroup in the sense of
\cite{reg_star_smgp}.

As with any semigroup, it will be helpful to describe the Green's relations of
the bipartition monoid.  The following proposition describes the Green's
relations, the containment of $\JJ$-classes and the ideals of $\Prt_n$ very
simply in terms of domain, codomain, kernel, cokernel, and rank.

\begin{proposition}
  \label{prop:bipartition-greens}
  Let $\alpha$ and $\beta$ be bipartitions in $\Prt_n$.  The following hold:
  \begin{enumerate}[\rm(i)]
  \item $\alpha \RR \beta$ if and only if $\dom \alpha = \dom \beta$ and
    $\ker \alpha = \ker \beta$;
  \item $\alpha \LL \beta$ if and only if $\codom \alpha = \codom \beta$ and
    $\coker \alpha = \coker \beta$;
  \item $\alpha \JJ \beta$ if and only if $\rank \alpha = \rank \beta$;
  \item $J_\alpha \leq J_\beta$ if and only if $\rank \alpha \leq \rank \beta$.
  \item the ideals of $\Prt_n$ are precisely the sets
    $I_r=\{\alpha \in \Prt_n : \rank \alpha \leq r\}$ for
    $r \in \{0, \ldots, n\}$.
  \end{enumerate}
  \begin{proof}
    Parts (i) to (iv) are from \cite{fitzgerald_2011}, and (v) follows naturally
    from (iv).
    % TODO: just give an actual proof
  \end{proof}
\end{proposition}

The bipartition monoid $\Prt_n$ grows very quickly as $n$ increases: its size is
the Bell number $B_{2n}$ \citeoeis{A000110}.
It therefore grows much faster than the symmetric group $\Sym_n$ with $n!$
elements, the symmetric inverse monoid $\I_n$ with
$\sum_{k=0}^n \binom{n}{k} \frac{n!}{k!}$ elements, and even the full
transformation monoid $\T_n$ with $n^n$ elements.  The degree of this difference
is illustrated in Table \ref{tab:pn-size}.

\begin{table}[h]
  \centering
  \renewcommand\arraystretch{1.0}
  \begin{tabular}{| r | r | r | r | r |}
    \hline
    $n$ & $|\Sym_n|$ & $|\I_n|$ & $|\T_n|$ & $|\Prt_n|$ \\
    \hline
     1 &         1&           2&              1&                  2 \\
     2 &         2&           7&              4&                 15 \\
     3 &         6&          34&             27&                203 \\
     4 &        24&         209&            256&              4 140 \\
     5 &       120&       1 546&          3 125&            115 975 \\
     6 &       720&      13 327&         46 656&          4 213 597 \\
     7 &     5 040&     130 922&        823 543&        190 899 322 \\
     8 &    40 320&   1 441 729&     16 777 216&     10 480 142 147 \\
     9 &   362 880&  17 572 114&    387 420 489&    682 076 806 159 \\
    10 & 3 628 800& 234 662 231& 10 000 000 000& 51 724 158 235 372 \\
    \hline
  \end{tabular}
  \renewcommand\arraystretch{0.7}
  \caption{Sizes of $\Sym_n$, $\I_n$, $\T_n$ and $\Prt_n$ for small values of
    $n$}
  \label{tab:pn-size}
\end{table}

\section{Computation \& decidability}
\label{sec:computation-decidability}

Since this thesis will deal with many computational issues, it may be helpful to
make precise some computational terms.  We start with a definition of
``algorithm'', a term which is generally well understood, but whose precise
definition is debatable.  In this thesis, we opt for a definition in line with
the Church--Turing thesis, which has been favoured by a variety of authors since
it was established \cite{gurevich_2000, minsky_1967}.

The Church--Turing thesis evolved from work by G\"{o}del, Church and Turing in
the 1930s, in which they established three different models of computation:
\textit{general recursive functions} \cite{godel}, \textit{$\lambda$-calculus}
\cite{church}, and \textit{Turing machines} \cite{turing}.  These three models
were soon shown to be equivalent, with any method computable on one being
computable on both the others.  This led to the Church--Turing thesis: the
opinion that the informal notion of an algorithm is accurately characterised by
each of these three models, and therefore that they should be used as a
definition of ``algorithm''.  This gives rise to our chosen definition.

\begin{definition}
  \label{def:algorithm}
  \index{algorithm}
  An \textbf{algorithm} is a computational method which can be simulated by a
  Turing machine.
\end{definition}

A Turing machine is a conceptual machine based on a finite state automaton which
interacts with an infinite tape.  There are several different formulations of a
Turing machine, all of which are equivalent in terms of the set of computational
methods that they can run.  Though this definition is not central to this
thesis, we present one such formulation here for completeness.

\begin{definition}
  \label{def:turing-machine}
  \index{Turing machine}
  A \textbf{Turing machine} is a tuple $(Q, \Sigma, q_0, \delta, H)$ where
  \begin{itemize}
  \item $Q$ is a set, called the set of \textit{states};
  \item $\Sigma$ is a set, called the \textit{alphabet};
  \item $q_0 \in Q$ is a state known as the \textit{initial state};
  \item $\delta: Q \times \Sigma \to
    Q \times \Sigma \times \{\mathtt{L}, \mathtt{R}\}$
    is a function known as the \textit{transition function}, which uses the
    current state and a character from the tape to process a step in the
    computation;
  \item $H \subseteq Q$ is a subset of states known as the \textit{halting
      states}.
  \end{itemize}
  This machine is paired with a conceptual infinite \textit{tape}---a
  sequence of characters from $\Sigma$ which continues infinitely in both
  directions.  This tape is provided as an input to the algorithm.  The machine
  starts in state $q_0$ with its read/write head pointed to a given position on
  the tape.  On each step of computation, it starts in a state $q$, reads a
  symbol $\sigma$ from the read/write head's position on the tape, and uses
  $(q, \sigma)\delta$ to produce a triple $(q', \sigma', d)$ from
  $Q \times \Sigma \times \{\mathtt{L},\mathtt{R}\}$:
  it then changes state from $q$ to $q'$,
  replaces the character $\sigma$ on the tape with $\sigma'$, and moves the
  read/write head one character to the left or right according to $d$.  This process is
  repeated until the machine enters a state in $H$, at which time it halts,
  having completed its operation.  The program's output is the resulting tape.
\end{definition}

This definition encompasses every computation which can be run on today's
electronic computers, and is therefore certainly applicable to any practical
implementation of the algorithms described in this thesis.

Now that we have a definition of an algorithm, we can define decidability.
Decidability is not a core topic of this thesis, but it will be worth going into
at least some detail, particularly around ideas related to semigroup
presentations.  A deeper discussion of decidability can be found in, for
example, \cite{enderton_2001}.

\begin{definition}
  \label{def:decidable}
  \index{decidable}
  A class of problems is \textbf{decidable} if there exists a single algorithm
  which is guaranteed to return a correct answer to any instance of one of those
  problems in a finite amount of time.
\end{definition}

Note that although an algorithm may be guaranteed to complete in finite time,
the length of time might be unbounded; that is, an actual run of the algorithm,
though guraranteed to complete, might take an arbitrarily long time.

Decidability is always something that should be considered when designing an
algorithm that may act on infinite objects.  In this thesis, particularly in
Chapter \ref{chap:pairs}, we encounter semigroup presentations, which have an
interesting decidability feature.

\begin{definition}
  \label{def:word-problem}
  \index{word problem}
  Let $\pres X R$ be a semigroup presentation.  The \textbf{word problem} for
  $\pres X R$ is the following question: given two words $u, v \in X^+$, do $u$
  and $v$ represent the same semigroup element?
\end{definition}

For many individual semigroup presentations, the word problem is decidable.  For
example, consider the following presentation.

\begin{example}
  \label{ex:presentation-decidable}
  Let $S$ be given by the presentation
  $$\pres{a,b}{ab=ba}.$$
  The word problem for this presentation is decidable: two words represent the
  same element of $S$ if and only if they contain the same number of occurrences
  of $a$ and the same number of occurrences of $b$.
\end{example}

In Example \ref{ex:presentation-decidable}, there is an algorithm to answer the
word problem; hence, we
say that $S$ has decidable word problem.  However, we have not shown the word
problem in general to be decidable, since our algorithm does not apply to every
semigroup presentation, only to the one considered in the example.  It turns out
that there is no single algorithm which can be applied to every presentation to
solve its word problem.  In fact, we can make a stronger statement: there are
some presentations for which there is not even a specific algorithm to solve
the word problem.  Consider the
following example from Gennadi\'{i} Makanin, which has only three generators.

\begin{example}[Makanin, 1966]
  \label{ex:makanin}
  The presentation
  \begin{align*}
    \langle a,b,c ~|~ & c^2b^2 = b^2c^2,\ bc^3b^2 = cb^3c^2,\ ac^2b^2 = b^2a,\\
                      & abc^3b^2 = cb^2a,\ b^2c^2b^4c^2 = b^2c^2b^4c^2a \rangle
  \end{align*}
  defines a semigroup which has undecidable word problem.  \cite{makanin_1966}
\end{example}

Furthermore, consider the following example from G.S.~Cijtin, perhaps the
simplest undecidable presentation, with 5 generators but only 33 occurrences of
those generators in its relations.

\begin{example}[Cijtin, 1957]
  \label{ex:cijtin}
  The presentation
  \begin{align*}
    \langle a,b,c,d,e ~|~ & ac=ca,\ ad=da,\ bc=cb,\ bd=db,\\
                          & ce=eca,\ de=edb,\ c^2e=c^2ae \rangle
  \end{align*}
  defines a semigroup which has undecidable word problem.
  \cite{cijtin_1957, collins_1986}
\end{example}
% TODO: read these properly, and maybe give proofs.

These two examples show that even relatively simple presentations can give rise
to semigroups with undecidable word problem.  Hence, no algorithm we give for
solving the word problem can be guaranteed to finish in finite time.
Furthermore, since words can be arbitrarily long, and presentations can have an
arbitrarily large set of relations, solving the word problem can take an
unbounded finite length of time even if it is decidable.  This means that it may
be impossible to tell whether an algorithm for the word problem will complete or
not, since it is not clear in advance whether a given semigroup has decidable
word problem.  At a given stage while such an algorithm is running, it may be
that an answer is about to be returned, but it may instead be that it will run
forever, and it may be impossible to tell the difference between those two
possibilities.

Aside from the word problem, there are a great many other properties of finite
presentations which are undecidable in general.  For example, there is no
algorithm which can take an arbitrary finite presentation and decide whether the
semigroup it describes:
\begin{itemize}
\item is finite;
\item has an identity;
\item has a zero;
\item has idempotents;
\item is a group;
\item has a nontrivial subgroup;
\item is an inverse semigroup;
\item is regular; or
\item is simple.
\end{itemize}
References for these, and a survey of many other decidability problems for
finitely presented semigroups, can be found in \cite{cain_maltcev}.

\section{Union-find}
\label{sec:union-find}
This thesis deals with congruences, and a congruence is a particular type of
equivalence.  It will therefore be useful for us to discuss methods of computing
with equivalences, in order to help us describe other algorithms later,
particularly the pair orbit enumeration algorithm in Section \ref{sec:p}.  We
begin by recalling a few definitions, and stating a problem we wish to solve.

Let $X$ be a set.  Recall that an \textbf{equivalence} on $X$ is a relation (a
subset of $X \times X$) which is reflexive, symmetric, and transitive---that is,
a partition of $X$ into disjoint subsets.  Also recall from Definition
\ref{def:re} the relation $\mathbf{R}^e$, the least equivalence containing a
given relation $\mathbf{R}$.  It may be that, given a set of pairs $\mathbf{R}$,
we wish to compute the equivalence $\mathbf{R}^e$.  This is where the union-find
method can be useful.

A \textbf{union-find} table, also known as a disjoint-set data structure, is a
data structure that stores and modifies an equivalence relation by viewing it as
a partition and using trees to keep track of which elements lie in which class.
This approach was first described in 1964 in \cite{galler_1964}, and its time
complexity has since been improved in various ways.  A few of these ways will be
described after the main description of the algorithm, but see \cite{galil_1991}
for a detailed survey of different improvements and their possible advantages
and drawbacks.

Assume we wish to compute $\mathbf{R}^e$ for a relation $\mathbf{R}$ on a set
$X$.  A union-find table, for us, is a pair $(\tau, \Lambda)$ consisting of a
table $\tau$ of integers and a list $\Lambda$ of elements from $X$, which will
both begin as arrays of size $0$.  The list $\Lambda$ will contain all elements
not in singletons, and the table $\tau$ will be used to keep track of which
elements of $\Lambda$ are in which class.  We will denote by $\tau_n$ the $n$th
entry of $\tau$, and we will denote by $\tau_x$ the entry in $\tau$ in the same
position as $x$ in $\Lambda$.  We will then update $\Lambda$ and $\tau$ using
three operations \textsc{AddElement}, \textsc{Union} and \textsc{Find} in the
following way.

At the beginning of the algorithm, every $\mathbf{R}^e$-class is assumed to be a
singleton, so $\Lambda$ and $\tau$ are both empty.  As the algorithm progresses,
at various times we will find that two elements (say $x$ and $y$) are
$\mathbf{R}^e$-related, and we will wish to record this.  If either of $x$ or
$y$ is not already in $\Lambda$, we call \textsc{AddElement} to start tracking
it in the union-find system; \textsc{AddElement} simply adds the element to the
end of $\Lambda$ and adds a new entry to the end of $\tau$ with value equal to
the new length of $\Lambda$---a value not yet used in the table.  See Algorithm
\ref{alg:uf-addelement} for pseudo-code.  Then we call \textsc{Union($x,y$)} to
combine the two classes as described below.

\begin{algorithm}
\caption{The \textsc{AddElement} algorithm (union-find)}
\label{alg:uf-addelement}
\index{AddElement@\textsc{AddElement}}
  \begin{algorithmic}
    \Require{$x \notin \Lambda$}
    \Procedure{AddElement}{$x$}
      \State{Append $x$ to the end of $\Lambda$}
      \State{Append $|\Lambda|$ to the end of $\tau$}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

A simple way of tracking classes would be for each entry in $\tau$ to contain
the equivalence class number of the appropriate element: distinct elements $x$
and $y$ are in the same equivalence class if and only if $x,y \in \Lambda$ and
$\tau_x = \tau_y$.  By this method, \textsc{Union($x,y$)} would go through the
whole of $\tau$, finding every entry which equals $\tau_y$, and updating it to
point to $\tau_x$, thus making the two classes equal.  However, this operation
has high time complexity---in the worst case, $O(|X|)$---and would cause any
implementation of this algorithm to take a long time to complete.  Instead, the
union-find algorithm treats $\tau_x$ as a pointer to a parent element, as
follows.

Rather than treating $\tau$ as a simple table in which elements are
$\mathbf{R}^e$-related if and only if they have the same number in their $\tau$
entry, we treat an entry in the table as a pointer to another entry in the
table.  Only if an entry in the table points to itself do we treat it as the
actual number of the equivalence class.  Hence we have a function,
\textsc{Find}, which takes an integer between $1$ and $|\Lambda|$ (referring to
the position of an element $x \in \Lambda$) and returns the number of its
equivalence class, as shown in Algorithm \ref{alg:uf-find}.

\begin{algorithm}
\caption{The \textsc{Find} algorithm (union-find)}
\label{alg:uf-find}
\index{Find@\textsc{Find}}
  \begin{algorithmic}
    \Procedure{Find}{$x$}
      \State $n := $ position of $x$ in $\Lambda$
      \Repeat
        \State $n \gets \tau_n$
      \Until{$n = \tau_n$}
      \State \Return $n$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Now we may view the operation of finding an element's class as traversing a tree
from its leaf up to its root, and we can view the entire connected tree as the
class itself.  In order to combine two classes, therefore, we have the function
\textsc{Union}, which simply finds the roots of the two trees and changes the
higher one to point to the lower.  Its pseudo-code is shown in Algorithm
\ref{alg:uf-union}.

\begin{algorithm}
\caption{The \textsc{Union} algorithm (union-find)}
\label{alg:uf-union}
\index{Union@\textsc{Union}}
  \begin{algorithmic}
    \Procedure{Union}{$x,y$}
      \State $m \gets $ \Call{Find}{$x$}
      \State $n \gets $ \Call{Find}{$y$}
      \If{$m<n$}
        \State $\tau_n \gets m$
      \ElsIf{$n<m$}
        \State $\tau_m \gets n$
      \EndIf
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

These three functions allow us to use two simple arrays to describe any
equivalence relation on a semigroup.  Whenever a pair $(x,y)$ is found in
$\rho$, we call \textsc{AddElement($x$)} and \textsc{AddElement($y$)} if
necessary, and then call \textsc{Union($x,y$)}.

Note that this union-find method has automatically removed the problem of
transitivity, as well as those of reflexivity and symmetry: if we relate the
element $x$ to $y$, and then $y$ to $z$, we have combined all three elements
into a single class, and so we will see that
$\textsc{Find}(x) = \textsc{Find}(y)$, so we have added the pair $(x,z)$ with no
additional effort; similarly every element $x$ is related to itself from the
very beginning; and relating $x$ to $y$ is precisely the same as relating $y$ to
$x$.  In other words, if we perform \textsc{Union} on all the pairs of
$\mathbf{R}$ one by one, we produce $\Lambda$ and $\tau$ which describe the
equivalence $\mathbf{R}^e$.

Other descriptions of union-find do not always include \textsc{AddElement}.  The
union-find algorithm is generally used to calculate equivalences on finite sets,
but it is possible to use it with infinite sets as well.  The method described
here allows for $X$ to be an infinite set, and stores information only about
elements that are not in singletons, by calling \textsc{AddElement} on each
element only when it is found to be in the same class as another element.  If
there are infinitely many elements in non-singleton classes, or if there are
infinitely many pairs in $\mathbf{R}$, then of course $\mathbf{R}^e$ cannot be
computed with this method.

\begin{example}
  \label{ex:uf}
  Let $X = \{a,b,c,d,e,f,g,h\}$ and let $R$ be the set of pairs
  $$\{(f,b), (d,c), (e,b), (b,d)\}.$$
  We can calculate the classes of $\mathbf{R}^e$ by applying \textsc{Union} to
  each pair in $\mathbf{R}$ in turn, calling \textsc{AddElement} on any
  appropriate elements to add them to $\Lambda$ first.

  First we call \textsc{AddElement}$(f)$ and \textsc{AddElement}$(b)$, then
  \textsc{Union}$(f,b)$.  After these operations we have $\Lambda = (f,b)$ and
  $\tau = (1,1)$, representing just one non-singleton class containing both
  elements.  Next we call \textsc{AddElement} on $d$ and $c$, and then
  \textsc{Union}$(d,c)$, after which we have $\Lambda = (f,b,d,c)$ and
  $\tau = (1,1,3,3)$, shown in tree form in the first diagram of Figure
  \ref{fig:uf}.

  Next we \textsc{AddElement}$(e)$ and call \textsc{Union}$(e,b)$.
  \textsc{Union} follows $b$ to its parent $f$ in the tree, and sets
  $\tau_x \gets 1$, resulting in $\Lambda = (f,b,d,c,e)$ and
  $\tau = (1,1,3,3,1)$.  This is represented in tree form in the second diagram
  of Figure \ref{fig:uf}.

  Finally we process the last pair by calling \textsc{Union}$(b,d)$.  This finds
  the root of $b$, which is $f$, and the root of $d$, which is $d$ itself, and
  unites the two roots.  $f$ has a lower position in $\Lambda$ than $d$ does, so
  $\tau_d$ is set equal to $f$.  At this final stage we have
  $\Lambda = \{f,b,d,c,e\}$ as before, and $\tau = (1,1,1,3,1)$, representing
  the tree structure shown in the third diagram of Figure \ref{fig:uf}.

  \begin{figure}[h]\centering
    $$
    \framebox[1.1\width][r]{
      \begin{forest}
        uf [$f$ [$b$]]
      \end{forest}
      \begin{forest}
        uf [$d$ [$c$]]
      \end{forest}
    } \quad \framebox[1.1\width][r]{
      \begin{forest}
        uf [$f$ [$b$][$e$]]
      \end{forest}
      \begin{forest}
        uf [$d$ [$c$]]
      \end{forest}
    } \quad \framebox[1.1\width][r]{
      \begin{forest}
        uf [$f$ [$b$][$e$] [$d$ [$c$]]]
      \end{forest}
    }
    $$
    \caption{Diagrams of union-find table in Example \ref{ex:uf}}
    \label{fig:uf}
  \end{figure}

  This represents a single tree, and therefore a single equivalence class
  consisting of all the elements $\{b,c,d,e,f\}$.  However, note that $a$, $g$
  and $h$ have not been added to $\Lambda$, so they are in singleton classes of
  $\mathbf{R}^e$.
\end{example}

The simple description we have given so far is sufficient to implement a working
version of the algorithm, but has complexity that can be easily reduced.  The
height of a tree created by repeated applications of \textsc{Union} can be as
great as the size of the set $X$, which means that the worst-case time
complexity of both \textsc{Find} and \textsc{Union} is $O(|X|)$.
But we may consider
the following improvements to both find and union, which limit the height of
trees and thus lower complexity.

The \textsc{Find} operation descends all the way from a node to the root of its
tree, but does not do anything with the final value that is found.  Hence, if
\textsc{Find} is later called on the same element, all the work is likely to be
repeated.  One possible improvement is to change the element's $\tau$-entry to
be equal to the result of \textsc{Find}, before returning.  This way, a future
call to \textsc{Find} will reach the root of the tree in a single step.
Furthermore, it is possible to change every node in the tree along the way,
essentially flattening the entire path each time \textsc{Find} is called.  This
improvement is known as \textit{path compression} \cite{hopcroft_1973}.
Alternatives have been proposed which do less up-front work, for example
\textit{path splitting} which points each node to its grandparent, or even
\textit{path halving} which points alternate nodes to their grandparents
\cite{leeuwen_1977}.  These all improve complexity in a way which we will
describe shortly.

The \textsc{Union} operation combines two trees by making one root the parent of
the other.  In the method described above, we choose the root with lower $\Lambda$-index
to be the new parent, but we might choose the parent differently.  The
\textit{union by size} method keeps track of the size of each tree, and makes
the smaller tree point to the larger \cite{galil_1991}; the \textit{union by
  rank} method instead keeps track of the depth of each tree (the length of the
longest path from the root) and makes the shallower tree point to the deeper
\cite{tarjan_1984}.  Either of these methods curbs the height of trees in the
table, preventing any tree from growing to a height greater than
$\lceil\log_2 |X|\rceil$ \cite[Lemma 1.1.2]{galil_1991}.

These improvements are enough to give us the following statement about
complexity.

\begin{theorem}[Theorem 1.1.1 in \cite{galil_1991}]
  \label{thm:ackermann}
  Choose any \textsc{Find} method from path compression, path splitting or path
  halving.  Choose either union by size or union by rank as a \textsc{Union}
  method.  A sequence of $n-1$ calls to \textsc{Union} and $m$ calls to
  \textsc{Find} completes in $O(n + m\alpha(m+n, n))$ time, where $\alpha$ is
  a functional inverse of Ackermann's function.
\end{theorem}

Ackermann's function is a function which grows extremely quickly, and the
functional inverse used in Theorem \ref{thm:ackermann} (defined explicitly in
\cite{tarjan_1984}) therefore grows extremely slowly.  In fact, we have
$\alpha(m,n) \leq 3$ for any $n < 2^{16}$, so in practice it can be treated as
constant and the complexity stated in Theorem \ref{thm:ackermann} is close to
$O(n + m)$, meaning that over several calls, \textsc{Union} and \textsc{Find}
have close to constant time complexity.

\section{Introductions to-do}
\begin{itemize}
\item Zero
\item Ideals
\item Ideal generating sets \begin{definition}\label{def:ideal-generating-set}\end{definition}
\item Idempotent (with ordering and maybe primitive?)
\item Lattices of congruences (intersection, join, etc.)
  \begin{theorem}\label{thm:congruence-lattice}Congruences of a semigroup form a lattice\end{theorem}
  \begin{proposition}\label{prop:congruence-lattice-gens}The lattice of congruences is generated by the subset of principal congruences using $\vee$.  Any congs generate an upper semilattice.\end{proposition}
\item Green's relations
\item L is a right cong, R is a left cong
\item Eggbox diagrams \begin{figure}[h]Eggbox\caption{Eggbox diagram}\label{fig:eggbox-diagram}\end{figure}
\item Digraph, digraph-with-edge-labels
\item Right regular representation
\item Regular elements, J-classes, semigroups
  \begin{theorem}\label{thm:regular-min-ideal}finite semigroup has regular min ideal\end{theorem}
\item Rectangular bands
  \begin{definition}\label{def:rectangular-band}Rectangular band\end{definition}
\item Inverse semigroups - idempotents etc.
\item Rees congruences (right now it's just in ch-converting)
\end{itemize}
