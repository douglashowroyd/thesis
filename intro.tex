\chapter{Introduction}
\label{chap:intro}

Things to define/explain:

\begin{itemize}
\item $\mathbf{R}^\sharp$
\item $\mathbf{R}^e$, $\mathbf{R}^c$, $\mathbf{R}^r$, $\mathbf{R}^l$, and the
  fact that they are the smallest compatible rels (see congs2.tex)
\item Lattices of congruences (intersection, join, etc.)
\item Green's relations
\item Digraph, digraph-with-edge-labels
\item Right regular representation
\end{itemize}

\section{Basic definitions}
\label{sec:intro-basic}

\begin{definition}
  \label{def:semigroup}
  A \textbf{semigroup} is a non-empty set $S$ together with
  a binary operation $*: S \times S \to S$ such that
  $$(x * y) * z = x * (y * z)$$
  for all $x, y, z \in S$.
\end{definition}
The operation symbol $*$ is often omitted where there is no risk of ambiguity.
% TODO: talk about the empty semigroup

\begin{definition}
  \label{def:monoid}
  A \textbf{monoid} is a semigroup $M$ containing a distinguished element $e$
  such that
  $$ex = xe = x$$
  for all $x \in M$.  The element $e$ is called the \textit{identity} of $M$.
\end{definition}

\begin{definition}
  \label{def:cayley-graph}
  Let $S$ be a semigroup, with a generating set $X$.  The \textbf{right Cayley
    graph} of $S$ with respect to $X$ is the digraph-with-edge-labels $\Gamma$,
  which is described as follows:
  \begin{itemize}
  \item The vertices of $\Gamma$ are the elements of $S$;
  \item For each pair $(s, x) \in S \times X$ there exists an edge from $s$ to
    $s \cdot x$ labelled by $x$:
    $$s \overset{x}{\longrightarrow} s \cdot x$$
  \end{itemize}
  The \textbf{left Cayley graph} of $S$ with respect to $X$ is defined
  analogously, replacing $s \cdot x$ with $x \cdot s$.
\end{definition}

\section{Homomorphisms}
\label{sec:homomorphisms}

\begin{definition}
  \label{def:homomorphism}
  Let $S$ and $T$ be semigroups.  A semigroup \textbf{homomorphism} is a
  function $\phi: S \to T$ such that
  $$(x)\phi \cdot (y)\phi = (xy)\phi,$$
  for all $x, y \in S$.
\end{definition}

\begin{definition}
  \label{def:monomorphism}
  A semigroup \textbf{monomorphism} is a semigroup homomorphism which is
  injective (one-to-one).  It is indicated on diagrams by a hooked arrow:
  $$S \hookrightarrow T$$
\end{definition}

\begin{definition}
  \label{def:epimorphism}
  A semigroup \textbf{epimorphism} is a semigroup homomorphism which is
  surjective (onto).  It is indicated on diagrams by a double-headed arrow:
  $$S \twoheadrightarrow T$$
\end{definition}

Monoid homomorphisms, monomorphisms and epimorphisms are defined analogously,
replacing the word ``semigroup'' with ``monoid''.  If not specified, it is
assumed that ``homomorphism'' refers to a semigroup homomorphism.
% TODO: monoid homos map id to id

\begin{definition}
  The \textbf{kernel} $\ker\phi$ of a homomorphism $\phi:S \to T$ is the
  equivalence relation on $S$ defined by the rule that $(a,b) \in \ker\phi$ if
  and only if
  $$(a)\phi = (b)\phi,$$
  for $a, b \in S$.
\end{definition}

\begin{definition}
  The \textbf{image} $\im\phi$ of a homomorphism $\phi:S \to T$ is the set of
  elements $t \in T$ such that
  $$(s)\phi = t$$
  for some $s \in S$.
\end{definition}

\section{Congruences}
\label{sec:intro-congs}

\begin{definition}
  \label{def:congruence}
  Let $S$ be a semigroup, and let $\rho$ be an equivalence relation on $S$.  The
  relation $\rho$ is:
  \begin{itemize}
  \item a \textbf{left congruence} if $(x, y) \in \rho$ implies that
    $(ax, ay) \in \rho$ for all $a \in S$;
  \item a \textbf{right congruence} if $(x, y) \in \rho$ implies that
    $(xa, ya) \in \rho$ for all $a \in S$;
  \item a \textbf{two-sided congruence} if it is both a left congruence and a
    right congruence.
  \end{itemize}
\end{definition}

When we talk about a \textit{congruence} without specifying that it is left or
right, it is understood to be a two-sided congruence.

\begin{proposition}
  \label{prop:cong-def}
  Let $\rho$ be a congruence on a semigroup $S$.  If $(x, y), (s, t) \in \rho$,
  then $(xs, yt) \in \rho$.
  \begin{proof}
    Since $\rho$ is a left congruence, $xs ~\rho~ xt$, and since it is a right
    congruence, $xt ~\rho~ yt$.  Hence, by transitivity, $xs ~\rho~ yt$, as
    required.
  \end{proof}
\end{proposition}

Congruences have a property that allows new semigroups to be made from old
ones.  Consider the following definition of a quotient semigroup.

\begin{definition}
  \label{def:quotient}
  Let $S$ be a semigroup, and let $\rho$ be a congruence on $S$.  The
  \textbf{quotient semigroup} $S / \rho$ is the semigroup whose elements are the
  congruence classes of $\rho$, and whose operation $*$ is defined by
  $$[a]_\rho * [b]_\rho = [ab]_\rho,$$
  for $a, b \in S$.
\end{definition}

% TODO: is [a] notation well understood?

In order for quotient semigroups to be well-defined, the product $[ab]_\rho$ of
the two classes $[a]_\rho$ and $[b]_\rho$ must not depend on which specific
elements $a$ and $b$ are chosen to represent the two classes.  Hence consider
arbitrary elements $a' \in [a]_\rho$ and $b' \in [b]_\rho$.  We must have
$[a]_\rho * [b]_\rho = [a']_\rho * [b']_\rho$, so we must have
$[ab]_\rho = [a'b']_\rho$.  Since $a ~\rho~ a'$ and $b ~\rho~ b'$, we have
$ab ~\rho~ a'b'$ by Proposition \ref{prop:cong-def}, and so
$[ab]_\rho = [a'b']_\rho$ as required.  So a quotient semigroup is well-defined.
However, note that such a condition does not generally hold for left and right
congruences, which do not generally satisfy the condition stated in Proposition
\ref{prop:cong-def}.  Hence a quotient semigroup as described in Definition
\ref{def:quotient} can only be taken using a two-sided congruence.

\begin{definition}
  \label{def:natural-homomorphism}
  Let $S$ be a semigroup, and let $\rho$ be a congruence on $S$.  The
  \textbf{natural homomorphism} $\pi_\rho: S \to S / \rho$ is the map which
  takes an element of $S$ to its $\rho$-class:
  $$\pi_\rho: x \mapsto [x]_\rho.$$
  It is denoted simply by $\pi$ where there is no risk of ambiguity.
\end{definition}

Congruences have long been an important area of study in semigroup theory.
Perhaps the most important feature of two-sided congruences is that they
determine the homomorphic images of a semigroup, and therefore describe an
important part of a semigroup's structure.  Consider the following theorem.

\begin{theorem}[First isomorphism theorem]
  \label{thm:first-isomorphism}
  Let $S$ and $T$ be semigroups, and let $\phi$ be a homomorphism from $S$ to
  $T$.  Then the kernel of $\phi$ is a congruence on $S$, and the image of
  $\phi$ is isomorphic to the quotient semigroup $S / \ker{\phi}$.
\end{theorem}
%TODO: define phi-bar, and maybe explain how these diagrams work?

\begin{figure}[h]
  \centering
  $
  \begin{tikzcd}
    S \ar[d, two heads, "\pi"'] \ar[r, "\phi"] & T \\
    S / \ker{\phi} \ar[ur, dashed, hook, "\bar\phi"']
  \end{tikzcd}
  $
  \caption{Illustration of Theorem \ref{thm:first-isomorphism}}
  \label{fig:first-isomorphism-theorem}
\end{figure}
These ideas fit closely with the concept of semigroup \textit{presentations},
which we can describe after the concept of \textit{free semigroups}.

\section{Generating pairs}
\label{sec:intro-gen-pairs}

We now describe a concept key to Chapter \ref{chap:pairs} as well as to
semigroup presentations, that of \textit{generating pairs}.

\begin{definition}
  \label{def:gen-pairs}
  Let $S$ be a semigroup and let $\mathbf{R}$ be a subset of $S \times S$.
  \begin{itemize}
  \item The \textbf{equivalence generated by} $\mathbf{R}$ is the least
    equivalence relation (with respect to containment) which contains
    $\mathbf{R}$ as a subset.
  \item The \textbf{left congruence generated by} $\mathbf{R}$ is the least left
    congruence (with respect to containment) which contains $\mathbf{R}$ as a
    subset.
  \item The \textbf{right congruence generated by} $\mathbf{R}$ is the least
    right congruence (with respect to containment) which contains $\mathbf{R}$
    as a subset.
  \item The \textbf{congruence generated by} $\mathbf{R}$ is the least
    congruence (with respect to containment) which contains $\mathbf{R}$ as a
    subset.  It is denoted by $R^\sharp$.
  \end{itemize}
\end{definition}

Chapter \ref{chap:pairs} deals in detail with congruences specified by
generating pairs.  We now present some theory relating to generating pairs, in
order to inform discussions later.

We first need to establish a few definitions.  Let $S$ be a semigroup, and let
$\mathbf{R}$ be a relation on $S$.  We define
$$\mathbf{R}^{-1} = \{(x,y) \in S \times S ~|~ (y,x) \in \mathbf{R}\},$$
so $\mathbf{R}^{-1}$ is a copy of $\mathbf{R}$ but with the entries in each pair
swapped.

Next, let $\circ$ be the operation of concatenation, so that for two relations
$\mathbf{R}_1$ and $\mathbf{R}_2$ on $S$,
$$\mathbf{R}_1 \circ \mathbf{R}_2 = \left\{(x,y) \in S \times S ~\middle|~
  \exists z \in S: (x,z) \in \mathbf{R}_1, (z,y) \in \mathbf{R}_2\right\},$$
and for $n \in \mathbb{N}$ let
$$\mathbf{R}^n = \underbrace{\mathbf{R} \circ \dots \circ \mathbf{R}}_{n~\text{times}}.$$

\begin{definition}
  \label{def:transitiveclosure}
  The \textbf{transitive closure} $\mathbf{R}^\infty$ of a relation $\mathbf{R}$
  is the relation given by
  $$\mathbf{R}^\infty=\bigcup_{n \in \mathbb{N}}\mathbf{R}^n$$
\end{definition}

The transitive closure $\mathbf{R}^\infty$ of $\mathbf{R}$ is the least
transitive relation on $S$ containing $\mathbf{R}$ \cite[Lemma
2.3]{mtorpey_msc}.  This allows us to give a useful description of the
equivalence relation generated by $\mathbf{R}$.

\begin{definition}
  \label{def:re}
  For a relation $\mathbf{R}$ on a semigroup $S$, we define $\mathbf{R}^e$ as
  the relation $\left(\mathbf{R} \cup \mathbf{R}^{-1} \cup \Delta_S\right)^\infty$.
\end{definition}

\begin{lemma}
  \label{lem:re}
  $\mathbf{R}^e$ is the smallest equivalence relation on $S$ that contains
  $\mathbf{R}$ as a subset.
  \begin{proof}
    Clearly $\mathbf{R} \subseteq \mathbf{R}^e$.

    We will first prove that $\mathbf{R}^e$ is an equivalence relation, and then
    go on to prove that there is no smaller equivalence relation containing
    $\mathbf{R}$.

    Let $\mathbf{Q} = \mathbf{R} \cup \mathbf{R}^{-1} \cup \Delta_S$, so that
    $\mathbf{R}^e = \mathbf{Q}^\infty$.  Since $\Delta_S$ contains all the
    pairs necessary for reflexivity, we know that $\mathbf{Q}$ is reflexive,
    and therefore $\mathbf{Q}^\infty$ is reflexive and transitive.

    To show symmetry, observe that $(x,y) \in \mathbf{R}$ if and only if $(y,x)
    \in \mathbf{R}^{-1}$, and that $(x,y) \in \Delta_S$ if and only if $x=y$.
    $\mathbf{Q}$ is therefore certainly symmetric, and
    $$\mathbf{Q}^n = (\mathbf{Q}^{-1})^n = (\mathbf{Q}^n)^{-1},$$
    and so $\mathbf{Q}^n$ is symmetric.

    Now let $(x,y) \in \mathbf{R}^e$.  For some $n \in \mathbb{N}$, we have
    $(x,y) \in \mathbf{Q}^n$.  By the symmetry of $\mathbf{Q}^n$,
    $$(y,x) \in \mathbf{Q}^n \subseteq \mathbf{Q}^\infty = \mathbf{R}^e,$$
    and so $\mathbf{R}^e$ is symmetric.  Hence $\mathbf{R}^e$ is an equivalence.

    Now to show that $\mathbf{R}^e$ is the \textit{least} such equivalence,
    consider any equivalence $\mathbf{E}$ on $S$ such that $\mathbf{R} \subseteq
    \mathbf{E}$.  Since $\mathbf{E}$ is reflexive, we know that $\Delta_S
    \subseteq \mathbf{E}$, and since $\mathbf{E}$ is symmetric and contains
    $\mathbf{R}$, we know that $\mathbf{R}^{-1} \subseteq \mathbf{E}$.  Hence
    $$\mathbf{Q}=\mathbf{R}\cup\mathbf{R}^{-1}\cup\Delta_S \subseteq \mathbf{E}.$$
    Finally, since $\mathbf{E}$ is transitive and contains $\mathbf{Q}$, we know
    that $\mathbf{Q}^\infty \subseteq \mathbf{E}$.  Hence $\mathbf{R}^e$ is
    contained in $\mathbf{E}$, and so is no larger than any equivalence on $S$.
  \end{proof}
\end{lemma}

\begin{definition}
  \label{def:rc}
  For a relation $\mathbf{R}$ on a semigroup $S$, we define three relations:
  \begin{itemize}
  \item
    $\mathbf{R}^c = \left\{(xay, xby) ~\middle|~ (a,b) \in \mathbf{R},~x,y \in
      S^1\right\}$.
  \item
    $\mathbf{R}^l = \left\{(xa, xb) ~\middle|~ (a,b) \in \mathbf{R},~x \in
      S^1\right\}$.
  \item
    $\mathbf{R}^r = \left\{(ay, by) ~\middle|~ (a,b) \in \mathbf{R},~y \in
      S^1\right\}$.
  \end{itemize}
\end{definition}

\begin{lemma}
  \label{lem:rc}
  Let $S$ be a semigroup and let $\mathbf{R} \subseteq S \times S$.
  \begin{itemize}
  \item $\mathbf{R}^c$ is the smallest compatible relation on $S$ containing
    $\mathbf{R}$.
  \item $\mathbf{R}^l$ is the smallest left-compatible relation on $S$
    containing $\mathbf{R}$.
  \item $\mathbf{R}^c$ is the smallest right-compatible relation on $S$
    containing $\mathbf{R}$.
  \end{itemize}
  \begin{proof}
    We prove the statement for $\mathbf{R}^c$, and note that the proofs for
    $\mathbf{R}^l$ and $\mathbf{R}^r$ are very similar.

    $\mathbf{R}^c$ certainly contains $\mathbf{R}$ -- all the elements of
    $\mathbf{R}$ are encountered in the case that $x=y=1$.

    Let us show first that $\mathbf{R}^c$ is compatible.
    Let $(u,v) \in \mathbf{R}^c$ and let $w \in S$.  Now there must exist $a,b
    \in S$ and $x,y \in S^1$ such that $u=xay$, $v=xby$, and $(a,b) \in
    \mathbf{R}$.  Hence
    $wu = wx \cdot a \cdot y$ and
    $wv = wx \cdot b \cdot y$, and $wx \in S^1$,
    so $(wu,wv) \in \mathbf{R}^c$ and $\mathbf{R}^c$ is left-compatible.
    Similarly,
    $uw = x \cdot a \cdot yw$ and
    $vw = x \cdot b \cdot yw$, and $yw \in S^1$,
    so $(uw,vw) \in \mathbf{R}^c$ and $\mathbf{R}^c$ is right-compatible.

    Finally we need to show that there is no compatible relation smaller than
    $\mathbf{R}^c$ which contains $\mathbf{R}$.  For this purpose, let
    $\mathbf{C}$ be a compatible relation on $S$ such that $\mathbf{R} \subseteq
    \mathbf{C}$.  Now for any $(a,b) \in \mathbf{R}$ and $x,y \in S^1$, we must
    have $(xay,xby) \in \mathbf{C}$ by the definition of compatibility.  Every
    element of $\mathbf{R}^c$ has this form, hence $\mathbf{R}^c \subseteq
    \mathbf{C}$. \cite[p.26]{howie}
  \end{proof}
\end{lemma}

These three relations $\mathbf{R}^c$, $\mathbf{R}^l$ and $\mathbf{R}^r$ have
some properties which will be useful later.  These properties make up the
following lemmas.

\begin{lemma}
  \label{lem:cinverse}
  For a relation $\mathbf{R}$ on a semigroup $S$,
  \begin{itemize}
  \item $(\mathbf{R}^{-1})^c = (\mathbf{R}^c)^{-1}$
  \item $(\mathbf{R}^{-1})^l = (\mathbf{R}^l)^{-1}$
  \item $(\mathbf{R}^{-1})^r = (\mathbf{R}^r)^{-1}$
  \end{itemize}
  \begin{proof}
    Let $\mathbf{R}$ be a relation on a semigroup $S$.
    $\mathbf{R}^{-1} = \{(a,b)~|~(b,a)\in\mathbf{R}\}$, so
    $$(\mathbf{R}^{-1})^c = \{(xay,xby)~|~x,y \in S^1, (b,a)\in\mathbf{R}\}.$$
    The inverse of this last expression is
    $$\{(xay,xby)~|~x,y \in S^1, (a,b)\in\mathbf{R}\},$$
    which is equal to $\mathbf{R}^c$.
    Now $\big((\mathbf{R}^{-1})^c\big)^{-1} = \mathbf{R}^c$, which is equivalent
    to what we wanted.

    A similar argument holds for both $\mathbf{R}^l$ and $\mathbf{R}^r$
  \end{proof}
\end{lemma}

\begin{lemma}
  \label{lem:csubset}
  Let $\mathbf{A}$ and $\mathbf{B}$ be relations on a semigroup $S$.  If
  $\mathbf{A} \subseteq \mathbf{B}$, then $\mathbf{A}^c \subseteq \mathbf{B}^c$,
  $\mathbf{A}^l \subseteq \mathbf{B}^l$, and
  $\mathbf{A}^r \subseteq \mathbf{B}^r$.
  \begin{proof}
    Let $\mathbf{A} \subseteq \mathbf{B}$, and let $(xay,xby)$ be an arbitrary
    element of $\mathbf{A}^c$ where $(a,b) \in \mathbf{A}$ and $x,y \in S^1$.
    Since $\mathbf{A} \subseteq \mathbf{B}$, we have that $(a,b) \in
    \mathbf{B}$, and hence also that $(xay,xby) \in \mathbf{B}^c$.

    A similar statement holds for $\mathbf{A}^l \subseteq \mathbf{B}^l$ and
    $\mathbf{A}^r \subseteq \mathbf{B}^r$.
  \end{proof}
\end{lemma}

\begin{lemma}
  \label{lem:compatn}
  If $\mathbf{R}$ is a (left/right) compatible relation, then $\mathbf{R}^n$ is
  also (left/right) compatible, for all $n \in \mathbb{N}$.
  \begin{proof}
    Let $\mathbf{R}$ be a compatible relation on a semigroup $S$,
    and let $n \in \mathbb{N}$.
    Now let $(a,b) \in \mathbf{R}^n$, and $x \in S$.  Hence there exist
    $c_1, c_2, \dots, c_n, c_{n+1} \in S$ such that $a=c_1$, $b=c_{n+1}$,
    and $$(c_1, c_2), (c_2, c_3), \dots, (c_n, c_{n+1}) \in \mathbf{R}.$$
    Since $\mathbf{R}$ is left-compatible,
    $$(xc_1, xc_2), (xc_2, xc_3), \dots, (xc_n, xc_{n+1}) \in \mathbf{R},$$
    and since $\mathbf{R}$ is right-compatible,
    $$(c_1x, c_2x), (c_2x, c_3x), \dots, (c_nx, c_{n+1}x) \in \mathbf{R},$$
    and therefore
    $(xa,xb) \in \mathbf{R}^n$ and $(ax, bx) \in \mathbf{R}^n,$
    so $\mathbf{R}^n$ is compatible. \cite[p.26]{howie}
    A similar argument holds for left and right compatibility.
  \end{proof}
\end{lemma}

Now that we have these ways of describing the smallest \textit{equivalence}
relation and the smallest \textit{compatible} relation containing a set of
pairs, we are ready to describe the smallest \textit{compatible equivalence}
relation, or congruence, containing those pairs:

\begin{theorem}
  \label{thm:rsharp}
  Let $\mathbf{R}$ be a relation on a semigroup $S$.
  Then
  \begin{itemize}
  \item $\mathbf{R}^\sharp$, the smallest congruence on $S$ which contains
    $\mathbf{R}$, is equal to $(\mathbf{R}^c)^e$;
  \item $\mathbf{R}^\mathbf{l}$, the smallest left congruence on $S$ which
    contains $\mathbf{R}$, is equal to $(\mathbf{R}^l)^e$;
  \item and $\mathbf{R}^\mathbf{r}$, the smallest right congruence on $S$ which
    contains $\mathbf{R}$, is equal to $(\mathbf{R}^r)^e$.
  \end{itemize}
  \begin{proof}
    Since $\mathbf{R}^c$ is a relation, it follows from Lemma \ref{lem:re} that
    $(\mathbf{R}^c)^e$ is an equivalence, and it certainly contains
    $\mathbf{R}$.  To show that it is a congruence, we now only need to show
    that it is compatible:

    By Definition \ref{def:re}, $(\mathbf{R}^c)^e = \mathbf{Q}^\infty$, where
    $$\mathbf{Q} =
    \mathbf{R}^c \cup
    (\mathbf{R}^c)^{-1} \cup
    \Delta_S.$$
    Lemma \ref{lem:cinverse} gives us that
    $(\mathbf{R}^c)^{-1} = (\mathbf{R}^{-1})^c$, and
    we know
    $\Delta_S = {\Delta_S}^c$, so
    $$\mathbf{Q} =
    \mathbf{R}^c \cup
    (\mathbf{R}^{-1})^c \cup
    {\Delta_S}^c,$$
    and finally applying Lemma \ref{lem:csubset} gives us
    $$\mathbf{Q} =
    (\mathbf{R} \cup \mathbf{R}^{-1} \cup \Delta_S)^c.$$
    Hence by Lemma \ref{lem:rc}, $\mathbf{Q}$ is a compatible relation.

    Let $a \in S$ and let $(x,y) \in (\mathbf{R}^c)^e = \mathbf{Q}^\infty$.
    By Definition \ref{def:transitiveclosure}, $(x,y) \in \mathbf{Q}^n$ for some
    $n \in \mathbb{N}$, and by Lemma \ref{lem:compatn} we know that
    $\mathbf{Q}^n$ is compatible.  Hence
    $$(ax,ay), (xa, ya) \in \mathbf{Q}^n \subseteq \mathbf{Q}^\infty =
    (\mathbf{R}^c)^e,$$ and so $(\mathbf{R}^c)^e$ is a congruence.

    All that remains is to show that there is no congruence containing
    $\mathbf{R}$ which is smaller than $(\mathbf{R}^c)^e$.
    Let $\rho$ be a congruence containing $\mathbf{R}$.  Since $\rho$ is
    compatible, $\rho^c = \rho$ by Lemma \ref{lem:rc}; and since $\mathbf{R}
    \subseteq \rho$, by Lemma \ref{lem:csubset}, $\mathbf{R}^c \subseteq
    \rho^c$.  So we have
    $$\mathbf{R}^c \subseteq \rho.$$
    Finally, since $\rho$ is an equivalence containing $\mathbf{R}^c$, we know
    from Lemma \ref{lem:re} that $(\mathbf{R}^c)^e \subseteq \rho$, so
    $(\mathbf{R}^c)^e$ is the smallest congruence on $S$ containing
    $\mathbf{R}$. \cite[p.26-27]{howie}

    A similar argument holds for $\mathbf{R}^\mathbf{l}$ and
    $\mathbf{R}^\mathbf{r}$.
  \end{proof}
\end{theorem}

\section{Presentations}
\label{sec:intro-presentations}

\begin{definition}
  \label{def:free}
  Let $X$ be a set.  The \textbf{free monoid} over $X$ is denoted by $X^*$, and
  consists of all finite sequences of elements in $X$, with the operation of
  concatenation.  The \textbf{free semigroup} $X^+$ is the subsemigroup of $X^*$
  consisting of sequences of length at least $1$.
\end{definition}

When we consider free semigroups and monoids, the set $X$ is usually referred to
as an \textit{alphabet}, its elements as \textit{letters}, and sequences of
letters as \textit{words}.

We can now define semigroup presentations, a useful method of describing a
semigroup which will be encountered many times in this thesis, particularly in
Chapter \ref{chap:pairs}.  We will give the definition, and then discuss how the
definition is used to describe a semigroup.

\begin{definition}
  \label{def:presentation}
  A \textbf{semigroup presentation} is a pair $\pres X R$ consisting of a set
  $X$ and a set of pairs $R \subseteq X^+ \times X^+$.  It is taken as a
  description of a semigroup: the semigroup it defines is $X^+ / R^\sharp$.
\end{definition}

\begin{definition}
  \label{def:finite-presentation}
  A semigroup presentation $\pres X R$ is \textbf{finite} if $X$ and $R$ both
  have finite size, and each word in each relation in $R$ has finite length.
\end{definition}

The exact wording used to talk about a semigroup $S$ defined by a presentation
$\pres X R$ varies.  Some sources view $\pres X R$ as a semigroup in its own
right: they might describe $S$ as ``isomorphic to'' $\pres X R$, or they might
even say $S$ ``equals'' $\pres X R$.  We will opt for the more careful language
which separates a semigroup from its presentation: $S$ is \textbf{defined by} or
\textbf{presented by} $\pres X R$, if and only if it is isomorphic to
$X^+ / R^\sharp$.

If $S$ is presented by $\pres X R$, there is an epimorphism from $X^+$ to $S$:
if $\pi$ is the natural homomorphism from $X^+$ to the quotient semigroup
$X^+ / R^\sharp$ (see Definition \ref{def:natural-homomorphism}) and $\iota$ is
an isomorphism from $X^+ / R^\sharp$ to $S$, then $\pi\iota$ is an epimorphism
from $X^+$ to $S$, which assigns each word in the generators to an element of
the semigroup $S$.  If $w \in X^+$ and $s \in S$ are such that
$(w)\pi\iota = s$, then we say that the word $w$ \textbf{represents} the element
$s$, or that the element $s$ can be \textbf{factorised} to the word $w$.

\begin{figure}[h]
  \centering
  $
  \begin{tikzcd}
    X^+ \ar[r, two heads, "\pi"] &
    \frac{X^+}{R^\sharp} \ar[r, two heads, hook, "\iota"] &
    S
  \end{tikzcd}
  $
  \caption{Each word from $X^+$ represents an element in $S$}
  \label{fig:word-represents-element}
\end{figure}

\section{Computation \& decidability}
\label{sec:computation-decidability}

Since this thesis will deal with many computational issues, it will be wise to
make precise some computational terms.  We start with a definition of
``algorithm'', a term which is generally well understood, but whose precise
definition is debatable.  In this thesis, we opt for a definition in line with
the Church--Turing thesis, which has been favoured by a variety of authors since
it was established \cite{gurevich_2000, minsky_1967}.

The Church--Turing thesis evolved from work by G\"{o}del, Church and Turing in
the 1930s, in which they established three different models of computation:
\textit{general recursive functions} \cite{godel}, \textit{$\lambda$-calculus}
\cite{church}, and \textit{Turing machines} \cite{turing}.  These three models
were soon shown to be equivalent, with any method computable on one being
computable on both the others.  This led to the Church--Turing thesis: the
opinion that the informal notion of an algorithm is accurately characterised by
each of these three models, and therefore that they should be used as a
definition of ``algorithm''.  This gives rise to our chosen definition.

\begin{definition}
  \label{def:algorithm}
  An \textbf{algorithm} is a computational method which can be simulated by a
  Turing machine.
\end{definition}

This definition encompasses every computation which can be run on today's
electronic computers, and is therefore certainly applicable to any practical
implementation of the algorithms described in this thesis.

\section{Union-find}
\label{sec:union-find}
This thesis deals with congruences, and a congruence is a particular type of
equivalence.  It will therefore be useful for us to discuss methods of computing
with equivalences, in order to help us describe other algorithms later,
particularly the pair orbit enumeration algorithm in Section \ref{sec:p}.  We
begin by recalling a few definitions, and stating a problem we wish to solve.

Let $X$ be a set.  Recall that an \textbf{equivalence} on $X$ is a relation (a
subset of $X \times X$) which is reflexive, symmetric, and transitive---that is,
a partition of $X$ into disjoint subsets.  Also recall from Definition
\ref{def:re} the relation $\mathbf{R}^e$, the least equivalence containing a
given relation $\mathbf{R}$.  It may be that, given a set of pairs $\mathbf{R}$,
we wish to compute the equivalence $\mathbf{R}^e$.  This is where the union-find
method can be useful.

A \textbf{union-find} table, also known as a disjoint-set data structure, is a
data structure that stores and modifies an equivalence relation by viewing it as
a partition and keeping track of which elements lie in which class by keeping
the elements in a set of trees, with one tree for each set.  This approach was
first described in 1964 in \cite{galler_1964}, and has since been improved upon
in various ways to improve its time complexity.  A few of these ways will be
described after the main description of the algorithm, but see \cite{galil_1991}
for a detailed survey of different improvements and their possible advantages
and drawbacks.

Assume we wish to compute $\mathbf{R}^e$ for a relation $\mathbf{R}$ on a set
$X$.  A union-find table, for us, is a pair $(\tau, \Lambda)$ consisting of a
table $\tau$ of integers and a list $\Lambda$ of elements from $X$, which will
both begin as arrays of size $0$.  The list $\Lambda$ will contain all elements
not in singletons, and the table $\tau$ will be used to keep track of which
elements of $\Lambda$ are in which class.  We will denote by $\tau_n$ the $n$th
entry of $\tau$, and we will denote by $\tau_x$ the entry in $\tau$ in the same
position as $x$ in $\Lambda$.  We will then update $\Lambda$ and $\tau$ using
three operations \textsc{AddElement}, \textsc{Union} and \textsc{Find} in the
following way.

At the beginning of the algorithm, every $\mathbf{R}^e$-class is assumed to be a
singleton, so $\Lambda$ and $\tau$ are both empty.  As the algorithm progresses,
at various times we will find that two elements (say $x$ and $y$) are
$\mathbf{R}^e$-related, and we will wish to record this.  If either of $x$ or
$y$ is not already in $\Lambda$, we call \textsc{AddElement} to start tracking
it in the union-find system; \textsc{AddElement} simply adds the element to the
end of $\Lambda$ and adds a new entry to the end of $\tau$ with value equal to
the new length of $\Lambda$---a value not yet used in the table.  See Algorithm
\ref{alg:uf-addelement} for pseudo-code.  Then we call \textsc{Union($x,y$)} to
combine the two classes as described below.

\begin{algorithm}
\caption{Adding an element to the union-find table}
\label{alg:uf-addelement}
  \begin{algorithmic}
    \Require{$x \notin \Lambda$}
    \Procedure{AddElement}{$x$}
      \State{Append $x$ to the end of $\Lambda$}
      \State{Append $|\Lambda|$ to the end of $\tau$}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

A simple way of tracking classes would be for each entry in $\tau$ to contain
the equivalence class number of the appropriate element: distinct elements $x$
and $y$ are in the same equivalence class if and only if $x,y \in \Lambda$ and
$\tau_x = \tau_y$.  By this method, \textsc{Union($x,y$)} would go through the
whole of $\tau$, finding every entry which equals $\tau_y$, and updating it to
point to $\tau_x$, thus making the two classes equal.  However, this operation
has high time complexity---in the worst case, $O(|X|)$---and would cause any
implementation of this algorithm to take a long time to complete.  Instead, the
union-find algorithm treats $\tau_x$ as a pointer to a parent element, as
follows.

Rather than treating $\tau$ as a simple table in which elements are
$\mathbf{R}^e$-related if and only if they have the same number in their $\tau$
entry, we treat an entry in the table as a pointer to another entry in the
table.  Only if an entry in the table points to itself do we treat it as the
actual number of the equivalence class.  Hence we have a function,
\textsc{Find}, which takes an integer between $1$ and $|\Lambda|$ (referring to
the position of an element $x \in \Lambda$) and returns the number of its
equivalence class, as shown in Algorithm \ref{alg:uf-find}.

\begin{algorithm}
\caption{Finding the class number of an element}
\label{alg:uf-find}
  \begin{algorithmic}
    \Procedure{Find}{$x$}
      \State $n := $ position of $x$ in $\Lambda$
      \Repeat
        \State $n \gets \tau_n$
      \Until{$n = \tau_n$}
      \State \Return $n$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Now we may view the operation of finding an element's class as traversing a tree
from its leaf up to its root, and we can view the entire connected tree as the
class itself.  In order to combine two classes, therefore, we have the function
\textsc{Union}, which simply finds the roots of the two trees and changes the
higher one to point to the lower.  Its pseudo-code is shown in Algorithm
\ref{alg:uf-union}.

\begin{algorithm}
\caption{Uniting two classes}
\label{alg:uf-union}
  \begin{algorithmic}
    \Procedure{Union}{$x,y$}
      \State $m \gets $ \Call{Find}{$x$}
      \State $n \gets $ \Call{Find}{$y$}
      \If{$m<n$}
        \State $\tau_n \gets m$
      \ElsIf{$n<m$}
        \State $\tau_m \gets n$
      \EndIf
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

These three functions allow us to use two simple arrays to describe any
equivalence relation on a semigroup.  Whenever a pair $(x,y)$ is found in
$\rho$, we call \textsc{AddElement($x$)} and \textsc{AddElement($y$)} if
necessary, and then call \textsc{Union($x,y$)}.

Note that this union-find method has automatically removed the problem of
transitivity, as well as those of reflexivity and symmetry: if we relate the
element $x$ to $y$, and then $y$ to $z$, we have combined all three elements
into a single class, and so we will see that
$\textsc{Find}(x) = \textsc{Find}(y)$, so we have added the pair $(x,z)$ with no
additional effort; similarly every element $x$ is related to itself from the
very beginning; and relating $x$ to $y$ is precisely the same as relating $y$ to
$x$.  In other words, if we perform \textsc{Union} on all the pairs of
$\mathbf{R}$ one by one, we produce $\Lambda$ and $\tau$ which describe the
equivalence $\mathbf{R}^e$.

Other descriptions of union-find do not always include \textsc{AddElement}.  The
union-find algorithm is generally used to calculate equivalences on finite sets,
but it is possible to use it with infinite sets as well.  The method described
here allows for $X$ to be an infinite set, and stores information only about
elements that are not in singletons, by calling \textsc{AddElement} on each
element only when it is found to be in the same class as another element.  If
there are infinitely many elements in non-singleton classes, or if there are
infinitely many pairs in $\mathbf{R}$, then of course $\mathbf{R}^e$ cannot be
computed with this method.

\begin{example}
  Let $X = \{a,b,c,d,e,f,g,h\}$ and let $R$ be the set of pairs
  $$\{(f,b), (d,c), (e,b), (b,d)\}.$$
  We can calculate the classes of $\mathbf{R}^e$ by applying \textsc{Union} to
  each pair in $\mathbf{R}$ in turn, calling \textsc{AddElement} on any
  appropriate elements to add them to $\Lambda$ first.

  First we call \textsc{AddElement}$(f)$ and \textsc{AddElement}$(b)$, then
  \textsc{Union}$(f,b)$.  After these operations we have $\Lambda = (f,b)$ and
  $\tau = (1,1)$, representing just one non-singleton class containing both
  elements.  Next we call \textsc{AddElement} on $d$ and $c$, and then
  \textsc{Union}$(d,c)$, after which we have $\Lambda = (f,b,d,c)$ and
  $\tau = (1,1,3,3)$, shown in tree form as follows.
  $$
  \begin{forest}
    uf [$f$ [$b$]]
  \end{forest}
  \begin{forest}
    uf [$d$ [$c$]]
  \end{forest}
  $$
  Next we \textsc{AddElement}$(e)$ and call \textsc{Union}$(e,b)$.
  \textsc{Union} follows $b$ to its parent $f$ in the tree, and sets
  $\tau_x \gets 1$, resulting in $\Lambda = (f,b,d,c,e)$ and
  $\tau = (1,1,3,3,1)$.  This is represented in tree form as follows.
  $$
  \begin{forest}
    uf [$f$ [$b$][$e$]]
  \end{forest}
  \begin{forest}
    uf [$d$ [$c$]]
  \end{forest}
  $$
  Finally we process the last pair by calling \textsc{Union}$(b,d)$.  This finds
  the root of $b$, which is $f$, and the root of $d$, which is $d$ itself, and
  unites the two roots.  $f$ has a lower position in $\Lambda$ than $d$ does, so
  $\tau_d$ is set equal to $f$.  At this final stage we have
  $\Lambda = \{f,b,d,c,e\}$ as before, and $\tau = (1,1,1,3,1)$, representing
  the following tree structure.
  $$
  \begin{forest}
    uf [$f$ [$b$][$e$] [$d$ [$c$]]]
  \end{forest}
  $$
  This represents a single tree, and therefore a single equivalence class
  consisting of all the elements $\{b,c,d,e,f\}$.  However, note that $a$, $g$
  and $h$ have not been added to $\Lambda$, so they are in singleton classes of
  $\mathbf{R}^e$.
\end{example}

The simple description we have given so far is sufficient to implement a working
version of the algorithm, but has complexity that can be easily reduced.  The
height of a tree created by repeated applications of \textsc{Union} can be as
great as the size of the set $X$, which means that the worst case time
complexity of both \textsc{Find} and \textsc{Union} is $O(|X|)$.  But consider
the following improvements to both find and union, which limit the height of
trees and thus lower complexity.

The \textsc{Find} operation descends all the way from a node to the root of its
tree, but does not do anything with the final value that is found.  Hence, if
\textsc{Find} is later called on the same element, all the work is likely to be
repeated.  One possible improvement is to change the element's $\tau$-entry to
be equal to the result of \textsc{Find}, before returning.  This way, a future
call to \textsc{Find} will reach the root of the tree in a single step.
Furthermore, it is possible to change every node in the tree along the way,
essentially flattening the entire path each time \textsc{Find} is called.  This
improvement is known as \textit{path compression} \cite{hopcroft_1973}.
Alternatives have been proposed which do less up-front work, for example
\textit{path splitting} which points each node to its grandparent, or even
\textit{path halving} which points alternate nodes to their grandparents
\cite{leeuwen_1977}.  These all improve complexity in a way which we will
describe shortly.

The \textsc{Union} operation combines two trees by making one root the parent of
the other.  In the method described above, we choose the root with lower index
to be the new parent, but we might choose the parent differently.  The
\textit{union by size} method keeps track of the size of each tree, and makes
the smaller tree point to the larger \cite{galil_1991}; the \textit{union by
  rank} method instead keeps track of the depth of each tree (the length of the
longest path from the root) and makes the shallower tree point to the deeper
\cite{tarjan_1984}.  Either of these methods curbs the height of trees in the
table, preventing any tree from growing to a height greater than
$\lceil\log_2 |X|\rceil$ \cite[Lemma 1.1.2]{galil_1991}.

These improvements are enough to give us the following statement about
complexity.

\begin{theorem}[Theorem 1.1.1 in \cite{galil_1991}]
  \label{thm:ackermann}
  Choose any \textsc{Find} method from path compression, path splitting or path
  halving.  Choose either union by size or union by rank as a \textsc{Union}
  method.  A sequence of $n-1$ calls to \textsc{Union} and $m$ calls to
  \textsc{Find} completes in $O(n + m\alpha(m+n, n))$ time, where $\alpha$ is
  a functional inverse of Ackermann's function.
\end{theorem}

Ackermann's function is a function which grows extremely quickly, and the
functional inverse used in Theorem \ref{thm:ackermann} (defined explicitly in
\cite{tarjan_1984}) therefore grows extremely slowly.  In fact, we have
$\alpha(m,n) \leq 3$ for any $n < 2^{16}$, so in practice it can be treated as
constant and the complexity stated in Theorem \ref{thm:ackermann} is close to
$O(n + m)$, meaning that over several calls, \textsc{Union} and \textsc{Find}
have close to constant time complexity.
